#!/bin/bash
# ############################################################################################
# File: ........: functions
# Language .....: bash 
# Author .......: Sacha Dubois, VMware
# Description ..: Tanzu Demo Hub - Cluster Installation
# ############################################################################################

alias cat='/usr/local/bin/bat -p'

waitCmd() {
  read
}

prtHead() {
  if [ "${tocindex}" == "" ]; then tocindex=1; else let tocindex=tocindex+1; fi
  printf "%2d.) %s\n" $tocindex "$1"
}

prtRead() {
  if [ "${tocindex}" == "" ]; then tocindex=1; else let tocindex=tocindex+1; fi
  printf "%2s   %s" "" "$1"; read x
}

prtText() {
  if [ "${tocindex}" == "" ]; then tocindex=1; else let tocindex=tocindex+1; fi
  printf "%2s   %s\n" "" "$1"
}

lineFed() {
  echo
}

slntCmd() {
  bold=$(tput bold)
  normal=$(tput sgr0)

  echo -e "     => ${bold}$1${normal}\c"; read x
  eval "$1" 2>&1 | sed -e '/^$/d' -e 's/^/     /g'
  if [ $? -ne 0 ]; then exit 1; fi
}

fakeCmd() {
  bold=$(tput bold)
  normal=$(tput sgr0)

  echo -e "     => ${bold}$1${normal}\c"; read x
  echo "     -------------------------------------------------------------------------------------------------------------------------------------------------------"
  cat /tmp/log | sed -e 's/^/     /g'
  echo "     -------------------------------------------------------------------------------------------------------------------------------------------------------"
  echo ""
}

execCat() {
  echo -e "     => cat $1\c"; read x
  echo "     -------------------------------------------------------------------------------------------------------------------------------------------------------"

  if [ -f /usr/local/bin/bat ]; then
    /usr/local/bin/bat -p --color always $1 | sed 's/^/     /g'
  else
    cat $1
  fi

  echo "     -------------------------------------------------------------------------------------------------------------------------------------------------------"
  echo ""
}

execCmd() {
  bold=$(tput bold)
  normal=$(tput sgr0)

  echo -e "     => ${bold}$1${normal}\c"; read x
  echo "     -------------------------------------------------------------------------------------------------------------------------------------------------------"
  eval "$1" 2>&1 | sed -e 's/^/     /g'
  #$1 | sed -e '/^$/d' -e 's/^/     /g'
  echo "     -------------------------------------------------------------------------------------------------------------------------------------------------------"
  echo ""
  if [ $? -ne 0 ]; then exit 1; fi
}

tmcCheckLogin() {
  tmc cluster list >/dev/null 2>&1

  if [ $? -ne 0 ]; then 
    echo "ERROR: Currently not logged in in TMC, please login: tmc login"
    exit 1
  fi
}

messageTitle() {
  _msg="$*"
  _dat=$(date "+%Y%m%d-%H:%M:%S")

  if [ "${PCF_DEPLOYMENT_DEBUG}" == "true" -o "${DEBUG}" == "1" ]; then
    echo "[${_dat}] ${_msg}"
  else
    echo "${_msg}"
  fi
}

pivnetAPIdownload() {
  REFRESH_TOKEN="$1"
  PRODUCT_SLUG="$2"
  PRODUCT_ID=$3
  PRODUCT_FILE=$4
  FILE_NAME="$5"

  token=$(curl -X POST https://network.pivotal.io/api/v2/authentication/access_tokens \
               -d "{\"refresh_token\":\"$REFRESH_TOKEN\"}" 2>/dev/null | jq -r '.access_token')
  if [ "${token}" == "" -o "${token}" == "null" ]; then return 1; fi

  # --- LOGIN INTO PIVNET ---
  wget -O /tmp/$FILE_NAME --header="Authorization: Bearer $token" \
  https://network.pivotal.io/api/v2/products/$PRODUCT_SLUG/releases/$PRODUCT_ID/product_files/$PRODUCT_FILE/download > /dev/null 2>&1
echo "FILE_NAME:$FILE_NAME"

  if [ ! -f /tmp/$FILE_NAME ]; then
    echo "ERROR: Error downloading file from PIVNET /tmp/$FILE_NAME"
    echo "       SLUG:$PRODUCT_SLUG, PRODUCT_ID:$PRODUCT_ID"
    exit
  fi

  return 0
}


messagePrint() {
  _msg="$1"
  _stt="$2"
  _cnt=$(echo "${_msg}" | wc -c | sed 's/ //g')
  _dat=$(date "+%Y%m%d-%H:%M:%S")

  _str=""
  while [ $_cnt -lt 58 ]; do
    _str="${_str}."
    let _cnt=_cnt+1
  done

  # --- INIZIALIZE VALUES IF EMPTY --- ---
  [ "$DEBUG" == "" ] && DEBUG=0

  if [ "${PCF_DEPLOYMENT_DEBUG}" == "true" -o "${DEBUG}" == "1" ]; then
    echo "[${_dat}] ${_msg} ${_str}: ${_stt}"
  else
    echo "${_msg} ${_str}: ${_stt}"
  fi
}

sshEnvironment() {
  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "vSphere" ]; then
    JUMP_HOST="${TDH_TKGMC_VCENTER_JUMPHOST}"

    SSH_USER=ubuntu
    SSH_HOME=/home/ubuntu
    SSH_PRIVATE_KEY=~/.tanzu-demo-hub/KeyPair-Azure.pem
    SSH_PRIVATE_KEY=$VSPHERE_SSH_PRIVATE_KEY_FILE
    SSH_HOST=$JUMP_HOST
    SSH_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=240 -o ClientAliveInterval=240"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=240"
    SCP_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_COMMAND="ssh -q $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
    SCP_COMMAND="scp -i ${SSH_PRIVATE_KEY} -r $SCP_OPTIONS"
    SSH_DISPLAY="ssh $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"

  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" -o "${TDH_TKGMC_INFRASTRUCTURE}" == "Azure" ]; then
    JUMP_HOST="jump-${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"

    pip=""
    while [ "$pip" == "" -o "$pip" == "null" ]; do
      pip=$(az network public-ip list -g Admin --query "[?contains(name, 'AdminPublicIP_$TDH_TKGMC_ENVNAME')]" | \
            jq -r '.[].ipAddress')
      sleep 10
    done

    SSH_USER=ubuntu
    SSH_HOME=/home/ubuntu
    SSH_PRIVATE_KEY=~/.tanzu-demo-hub/KeyPair-Azure.pem
    SSH_HOST=$pip
    SSH_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=240 -o ClientAliveInterval=240"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=240"
    SCP_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_COMMAND="ssh -q $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
    SCP_COMMAND="scp -i ${SSH_PRIVATE_KEY} -r $SCP_OPTIONS"
    SSH_DISPLAY="ssh $SSH_OPTIONS -i ${SSH_PRIVATE_KEY} ${SSH_USER}@${SSH_HOST}"
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
echo gaga1
    SSH_HOST=$(aws ec2 --region=$AWS_REGION describe-instances \
                     --filters Name=tag:sigs.k8s.io/cluster-api-provider-aws/role,Values="bastion" | \
             jq -r '.Reservations[].Instances[].PublicDnsName')

    SSH_USER=ubuntu
    SSH_HOME=/home/ubuntu
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=30"
    SSH_DISPLAY="-o StrictHostKeyChecking=no"
    SCP_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_COMMAND="ssh -q $SSH_OPTIONS -i $SSH_KEY_FILE ${SSH_USER}@${SSH_HOST}"
    SCP_COMMAND="scp -r $SCP_OPTIONS -i $SSH_KEY_FILE"
    SSH_DISPLAY="ssh -i $SSH_KEY_FILE ${SSH_USER}@${SSH_HOST}"
echo "SSH_DISPLAY:$SSH_DISPLAY"
echo gaga2
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "GCP" ]; then
    INSTANCE_NAME="$(echo $JUMP_HOST | awk -F'.' '{ print $1 }')"
    #ZONE=$(gcloud compute zones list | grep " $GCP_REGION " | head -1 | awk '{ print $1 }')
    ZONE=$(gcloud compute instances list --filter="name=( '$INSTANCE_NAME')" | egrep "^$INSTANCE_NAME" | awk '{ print $2 }')

    prj=$(gcloud projects list | sed '1d' | awk '{ print $1 }')
    prj=${GCP_PROJECT}

    SSH_USER=$(whoami)
    SSH_HOME=/home/$(whoami)
    SSH_HOST="${INSTANCE_NAME}.${ZONE}.${prj}"
    SSH_OPTIONS="-o StrictHostKeyChecking=no -o RequestTTY=yes -o ServerAliveInterval=30"
    SCP_OPTIONS="-o StrictHostKeyChecking=no"
    SSH_COMMAND="ssh -q $SSH_OPTIONS ${SSH_USER}@${SSH_HOST}"
    SCP_COMMAND="scp -r $SCP_OPTIONS"
    SSH_DISPLAY="ssh ${SSH_USER}@${SSH_HOST}"
  fi

  echo "--------------------------------- JUMP SERVER ACCESS ------------------------------------------------------"
  echo "$SSH_DISPLAY"
  echo "-----------------------------------------------------------------------------------------------------------"
}

checkCloudCLI() {
  if [ "${TDH_DEPLOYMENT_CLOUD}" == "GCP" ]; then
    # --- CHECK FOR AWS CLI ---
    GCLOUD=$(which gcloud)
    if [ "${GCLOUD}" != "" ]; then
      $GCLOUD --version >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        GCLOUD_VERSION=$($GCLOUD --version 2>/dev/null | grep "Google Cloud SDK" | awk '{ print $NF}')
        GCP_CLI_ENABLED=1
        messagePrint "checking for gcloud CLI utility" "installed - ${GCLOUD_VERSION}"
      else
        echo ""
        echo "ERROR: The gcloud utility does not seam to be correct"
        echo "       please install gcloud from https://cloud.google.com/sdk/install"; exit 1
      fi
    else
      messagePrint "checking for gcloud CLI utility" "not installed"
      echo ""
      echo "ERROR: gcloud utility is not installed. please install terraform from"
      echo "       https://cloud.google.com/sdk/install"; exit 1
    fi

    # --- CHECK FOR AWS CLI ---
    AWSCLI=$(which aws)
    if [ "${AWSCLI}" != "" ]; then
      $AWSCLI --version >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        AWS_VERSION=$($AWSCLI --version 2>/dev/null | awk -F'/' '{ print $2 }' | awk '{ print $1}')
        AWS_CLI_ENABLED=1
        messagePrint "checking for AWS CLI utility (needed for AWS Route53)" "installed - ${AWS_VERSION}"
      else
        echo ""
        echo "ERROR: the AWS cli utility does not seam to be correct, please reinstall it from:"
        echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
      fi
    else
      messagePrint "checking for AWS CLI utility" "not installed"
      echo "ERROR: the AWS cli utility is not installed, please install it from:"
      echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
    # --- CHECK FOR AWS CLI ---
    AWSCLI=$(which aws)
    if [ "${AWSCLI}" != "" ]; then
      $AWSCLI --version >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        AWS_VERSION=$($AWSCLI --version 2>/dev/null | awk -F'/' '{ print $2 }' | awk '{ print $1}')
        AWS_CLI_ENABLED=1
        messagePrint "checking for AWS CLI utility" "installed - ${AWS_VERSION}"
      else
        echo ""
        echo "ERROR: the AWS cli utility does not seam to be correct, please reinstall it from:"
        echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
      fi
    else
      messagePrint "checking for AWS CLI utility" "not installed"
      echo "ERROR: the AWS cli utility is not installed, please install it from:"
      echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" ]; then
    # --- CHECK FOR AWS CLI ---
    AZCLI=$(which az)
    if [ "${AZCLI}" != "" ]; then
      $AZCLI -v >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        AZ_VERSION=$($AZCLI -v 2>/dev/null | egrep "^azure-cli" | awk '{ print $2 }'); ret=$?
        AZURE_CLI_ENABLED=1
        messagePrint "checking for Azure CLI utility (az)" "installed - ${AZ_VERSION}"
      else
        echo ""
        echo "ERROR: the $cloud CLI $(which az) does not seam to be correct"
        echo "       please install it from https://docs.microsoft.com/en-us/cli/azure/install-azure-cli?view=azure-cli-latest"; exit 1
      fi
    else
      messagePrint "checking for $cloud CLI utility" "no installed"
    fi
    # --- CHECK FOR AWS CLI ---
    AWSCLI=$(which aws)
    if [ "${AWSCLI}" != "" ]; then
      $AWSCLI --version >/dev/null 2>&1; ret=$?
      if [ ${ret} -eq 0 ]; then
        AWS_VERSION=$($AWSCLI --version 2>/dev/null | awk -F'/' '{ print $2 }' | awk '{ print $1}')
        AWS_CLI_ENABLED=1
        messagePrint "checking for AWS CLI utility (needed for AWS Route53)" "installed - ${AWS_VERSION}"
      else
        echo ""
        echo "ERROR: the AWS cli utility does not seam to be correct, please reinstall it from:"
        echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
      fi
    else
      messagePrint "checking for AWS CLI utility" "not installed"
      echo "ERROR: the AWS cli utility is not installed, please install it from:"
      echo "       https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html"; exit 1
    fi
  fi

  if [ ! -x /usr/local/bin/kp ]; then 
    echo "ERROR: Please install the kp utility from https://network.pivotal.io/products/build-service"
    echo "       to /usr/local/bin/kp"
    echo "       => sudo mv kp-darwin-0.2.0 /usr/local/bin/kp"
    echo "       => sudo chmod a+x /usr/local/bin/kp"
    exit
  fi

  if [ ! -x /usr/local/bin/kbld ]; then 
    echo "ERROR: Please install the kp utility from https://network.pivotal.io/products/kbld" 
    echo "       to /usr/local/bin/kbld"
    echo "       => sudo mv kbld-darwin-amd64-0.29.0 /usr/local/bin/kbld"
    echo "       => sudo chmod a+x /usr/local/bin/kbld"
    exit
  fi
}

checkKubernetesServices() {
  service=$1
  missing_variables=0

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "minikube" ]; then
    if [ "${service}" == "registry_docker" ]; then
      if [ "${TDH_REGISTRY_DOCKER_NAME}" == "" -o "${TDH_REGISTRY_DOCKER_USER}" == "" -o "${TDH_REGISTRY_DOCKER_PASS}" == "" ]; then
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  --------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_REGISTRY_DOCKER_NAME}" == "" ]; then
          echo "  TDH_REGISTRY_DOCKER_NAME         (required) Docker Registry (hub.docker.com)"
        fi

        if [ "${TDH_REGISTRY_DOCKER_USER}" == "" ]; then
          echo "  TDH_REGISTRY_DOCKER_USER         (required) Docker User"
        fi

        if [ "${TDH_REGISTRY_DOCKER_PASS}" == "" ]; then
          echo "  TDH_REGISTRY_DOCKER_PASS         (required) Docker Password"
        fi
      fi
    fi

    if [ "${service}" == "github" ]; then
      if [ "${TDH_GITHUB_USER}" == "" -o "${TDH_GITHUB_PASS}" == "" ]; then
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  --------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_GITHUB_USER}" == "" ]; then
          echo "  TDH_GITHUB_USER                  (required) Github User"
        fi

        if [ "${TDH_GITHUB_PASS}" == "" ]; then
          echo "  TDH_GITHUB_PASS                  (required) VMware Password"
        fi
      fi
    fi

    if [ "${service}" == "registry_vmware" ]; then
      if [ "${TDH_REGISTRY_VMWARE_NAME}" == "" -o "${TDH_REGISTRY_VMWARE_USER}" == "" -o "${TDH_REGISTRY_VMWARE_PASS}" == "" ]; then
        missing_variables=1
        echo ""
        echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
        echo "  --------------------------------------------------------------------------------------------------------------"

        if [ "${TDH_REGISTRY_VMWARE_NAME}" == "" ]; then
          echo "  TDH_REGISTRY_VMWARE_NAME         (required) VMware Registry (registry.pivotal.io)"
        fi

        if [ "${TDH_REGISTRY_VMWARE_USER}" == "" ]; then
          echo "  TDH_REGISTRY_VMWARE_USER         (required) VMware User"
        fi

        if [ "${TDH_REGISTRY_VMWARE_PASS}" == "" ]; then
          echo "  TDH_REGISTRY_VMWARE_PASS         (required) VMware Password"
        fi
      fi
    fi

    if [ "${service}" == "harbor" ]; then
      if [ "${TDH_SERVICE_REGISTRY_HARBOR}" == "true" ]; then
        if [ "${TDH_HARBOR_ADMIN_PASSWORD}" == "" -o "${TDH_HARBOR_STAGING_TLS_CERT}" == "" ]; then
          missing_variables=1
          echo ""
          echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
          echo "  --------------------------------------------------------------------------------------------------------------"

          if [ "${TDH_HARBOR_ADMIN_PASSWORD}" == "" ]; then
            echo "  TDH_HARBOR_ADMIN_PASSWORD        (required) Harbor Admin Password"
          fi

          if [ "${TDH_HARBOR_STAGING_TLS_CERT}" == "" ]; then
            echo "  TDH_HARBOR_STAGING_TLS_CERT      (required) TLS Staging (true) or Productive (false)"
          fi
        fi
      fi
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  --------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the pcfconfig"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             ie. => export AZURE_PKS_TLS_CERTIFICATE=/home/demouser/certs/cert.pem"
    echo "  --------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkCloudAccess() {
  service=$1
  missing_variables=0

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "minikube" ]; then
    TDH_MINIKUBE_PROFILE="tdh-minikube-${TMC_USER}"
    if [ "${TDH_MINIKUBE_PROFILE}" == "" -o "${TMC_USER}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  --------------------------------------------------------------------------------------------------------------"

      if [ "${TMC_USER}" == "" ]; then
        echo "  TMC_USER                        (required) The name of the TMC UserID"
      fi
    else
      if [ -f ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json ]; then 
        cpu=$(cat ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json | jq -r '.CPUs')
        dsk=$(cat ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json | jq -r '.DiskSize')
        mem=$(cat ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json | jq -r '.Memory')
        drv=$(cat ~/.minikube/profiles/$TDH_MINIKUBE_PROFILE/config.json | jq -r '.Driver')
      else
        echo "ERROR: Minukube is not running, start it first"
        #echo "       => minikube -p $TDH_MINIKUBE_PROFILE start --driver=hyperkit --cpus $TDH_MINIKUBE_CONFIG_CPU --memory $TDH_MINIKUBE_CONFIG_MEMORY --disk-size $TDH_MINIKUBE_CONFIG_DISK --nodes=$TDH_MINIKUBE_NODES"; exit
        echo "       => minikube -p $TDH_MINIKUBE_PROFILE start --driver=hyperkit --cpus $TDH_MINIKUBE_CONFIG_CPU --memory $TDH_MINIKUBE_CONFIG_MEMORY --disk-size $TDH_MINIKUBE_CONFIG_DISK"; exit
      fi

      stt=$(minikube status -p $TDH_MINIKUBE_PROFILE| grep "^host:" | awk '{ print $NF }' | head -1)
      messageTitle "MiniKube Status and Configuration"
      messagePrint " - MiniKube Profile"                "$TDH_MINIKUBE_PROFILE"
      messagePrint " - MiniKube Version"                "$(minikube version | head -1 | awk '{ print $NF }')"
      messagePrint " - MiniKube Status"                 "$stt"
      messagePrint " - Minikube Disk"                  "$dsk"
      messagePrint " - Minikube Memory"                "$mem"
      messagePrint " - Minikube CPU's"                 "$cpu"
      messagePrint " - Minikube Driver"                "$drv"
      messageTitle " - Minikube Nodes"              
      echo "   ------------------------------------------------------------------------"
      minikube -p $TDH_MINIKUBE_PROFILE node list | sed 's/^/   /g'
      echo "   ------------------------------------------------------------------------"

      if [ "$stt" == "Stopped" -o "$stt" == "" ]; then 
        echo "ERROR: Minukube is not running, start it first"
        echo "       => minikube -p $TDH_MINIKUBE_PROFILE start --driver=hyperkit --cpus $TDH_MINIKUBE_CONFIG_CPU --memory $TDH_MINIKUBE_CONFIG_MEMORY --disk-size $TDH_MINIKUBE_CONFIG_DISK"; exit
        echo "       => cat ~/.minikube/profiles/tanzu-demo-hub/config.json"
      fi

      messagePrint " - MiniKube ServerIP"               "$(minikube ip -p $TDH_MINIKUBE_PROFILE)"

      cnt=$(tmc cluster list --name $TDH_MINIKUBE_PROFILE 2> /dev/null | egrep -c " $TDH_MINIKUBE_PROFILE ")
      if [ $cnt -eq 0 ]; then 
        messageTitle "Attach Minikube Cluster to TMC"
        messagePrint " - TMC Cluster Name"                "$TDH_MINIKUBE_PROFILE"
        messagePrint " - TMC Clustergroup"                "tanzu-demo-hub"
        echo "  --------------------------------------------------------------------------------------------------------------"
        tmc cluster attach -n $TDH_MINIKUBE_PROFILE -g tanzu-demo-hub -p minikube
        kubectl apply -f k8s-attach-manifest.yaml
        echo "  --------------------------------------------------------------------------------------------------------------"
      fi
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "vSphere" ]; then
    if [ "${VSPHERE_ADMIN}" == "" -o "${VSPHERE_PASSWORD}" == "" -o "${VSPHERE_SERVER}" == "" -o \
         "${VSPHERE_DATACENTER}" == "" -o "${VSPHERE_DATASTORE}" == "" -o "${VSPHERE_CLUSTER}" == "" -o \
         "${VSPHERE_SSH_PRIVATE_KEY_FILE}" == "" -o "${VSPHERE_SSH_PUBLIC_KEY_FILE}" == "" -o \
         "${VSPHERE_CONTROLPLANE_IP}" == "" -o \
         "${VSPHERE_MANAGEMENT_NETWORK}" == "" -o "${VSPHERE_WAN_NETWORK}" == "" -o "${VSPHERE_CLUSTER}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  --------------------------------------------------------------------------------------------------------------"

      if [ "${VSPHERE_SERVER}" == "" ]; then
        echo "  VSPHERE_SERVER                   (required) vSphere Server Name"
      fi

      if [ "${VSPHERE_CONTROLPLANE_IP}" == "" ]; then
        echo "  VSPHERE_CONTROLPLANE_IP          (required) TKG Controlplance IP"
      fi

      if [ "${VSPHERE_ADMIN}" == "" ]; then
        echo "  VSPHERE_ADMIN                    (required) vSphere Amdinistrator"
      fi

      if [ "${VSPHERE_PASSWORD}" == "" ]; then
        echo "  VSPHERE_PASSWORD                 (required) vSphere Admin Password"
      fi

      if [ "${VSPHERE_DATACENTER}" == "" ]; then
        echo "  VSPHERE_DATACENTER               (required) vSphere Datacenter"
      fi

      if [ "${VSPHERE_DATASTORE}" == "" ]; then
        echo "  VSPHERE_DATASTORE                (required) vSphere Datastore"
      fi

      if [ "${VSPHERE_CLUSTER}" == "" ]; then
        echo "  VSPHERE_CLUSTER                  (required) vSphere Cluster"
      fi

      if [ "${VSPHERE_MANAGEMENT_NETWORK}" == "" ]; then
        echo "  VSPHERE_MANAGEMENT_NETWORK       (required) vSphere Management Network"
      fi

      if [ "${VSPHERE_WAN_NETWORK}" == "" ]; then
        echo "  VSPHERE_WAN_NETWORK              (required) vSphere WAN Network"
      fi

      if [ "${VSPHERE_SSH_PRIVATE_KEY_FILE}" == "" ]; then
        echo "  VSPHERE_SSH_PRIVATE_KEY_FILE     (required) SSH Private Key File"
      fi
    
      if [ "${VSPHERE_SSH_PUBLIC_KEY_FILE}" == "" ]; then
        echo "  VSPHERE_SSH_PUBLIC_KEY_FILE      (required) SSH Public Key File"
      fi
    else
      echo "vSphere Access Credentials"
      messagePrint " - vSphere Server Name"                "$VSPHERE_SERVER"
      messagePrint " - vSphere Admin User"                 "$VSPHERE_ADMIN"
      messagePrint " - vSphere Admin Password"             $(maskPassword "$VSPHERE_PASSWORD")
      messagePrint " - vSphere Datacenter"                 "$VSPHERE_DATACENTER"
      messagePrint " - vSphere Datastore"                  "$VSPHERE_DATASTORE"
      messagePrint " - vSphere Cluster NAme"               "$VSPHERE_CLUSTER"
      messagePrint " - vSphere Management Network"         "$VSPHERE_MANAGEMENT_NETWORK"
      messagePrint " - vSphere WAN Network"                "$VSPHERE_WAN_NETWORK"
      messagePrint " - SSH Private Key"                    "$VSPHERE_SSH_PRIVATE_KEY_FILE"
      messagePrint " - SSH Public Key"                     "$VSPHERE_SSH_PUBLIC_KEY_FILE"
    fi

    # --- VERIFY OVA-IMAGES / OVFTOOL BUNDLE ---
    found=1; file_list=""
    for n in $TDH_TKGMC_TKG_IMAGES; do
      if [ ! -f $n ]; then found=0; file_list="$file_list $n"; fi
    done

    if [ $found -eq 0 ]; then 
      txt="ERROR: Missing OVA Images for TKG:"
      for n in $file_list; do 
        echo "$txt $n"; txt="                                  "
      done

      echo "Please download them from the VMware Download page:"
      echo "=> https://my.vmware.com/group/vmware/downloads/details?downloadGroup=TKG-121&productId=988"
      exit
    fi

    if [ ! -f $TDH_TKGMC_OVFTOOL_BUNDLE ]; then 
      echo "ERROR: Missing OVFTOOL Bundle, please download it from the VMware Download page:"
      echo "=> https://my.vmware.com/group/vmware/downloads/details?downloadGroup=OVFTOOL441&productId=734"
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "GCP" ]; then
    #if [ "${GCP_SERVICE_ACCOUNT}" == "" -o "${GCP_REGION}" == "" -o "${GCP_PROJECT}" == "" ]; then
    if [ "${GCP_REGION}" == "" -o "${GCP_PROJECT}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  4MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  --------------------------------------------------------------------------------------------------------------"

      #if [ "${GCP_SERVICE_ACCOUNT}" == "" ]; then
      #  echo "  GCP_SERVICE_ACCOUNT           (required) GCP Service Account"
      #fi

      if [ "${GCP_PROJECT}" == "" ]; then
        echo "  GCP_PROJECT                   (required) GCP Project"
        echo "                                  => gcloud projects list | sed '1d' | awk '{ print \$1 }'"
      fi

      if [ "${GCP_REGION}" == "" ]; then
        echo "  GCP_REGION                    (required) Choose the GCP Region where your installation should run"
        echo "                                  => gcloud compute zones list | sed '1d' | awk '{ print $2 }' | sort -u"
        echo "                                     europe-west1, us-east1, europe-west4  etc."
      fi
    else
      echo "GCP Access Credentials"
      #messagePrint " - GCP Service Account"          "$GCP_SERVICE_ACCOUNT"
      messagePrint " - GCP Project"                  "$GCP_PROJECT"
      messagePrint " - GCP Region"                   "$GCP_REGION"
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "AWS" ]; then
    if [ "${AWS_ACCESS_KEY}" == "" -o "${AWS_SECRET_KEY}" == "" -o \
         "${AWS_REGION}" == "" ]; then

      missing_variables=1
      echo ""
      echo "  5MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  --------------------------------------------------------------------------------------------------------------"

      if [ "${AWS_ACCESS_KEY}" == "" ]; then
        echo "  AWS_ACCESS_KEY                  (required) AWS Acess Key"
      fi

      if [ "${AWS_SECRET_KEY}" == "" ]; then
        echo "  AWS_SECRET_KEY                  (required) AWS Secret Key"
      fi

      if [ "${AWS_REGION}" == "" ]; then
        echo "  AWS_REGION                    (required) Choose the AWS Region where your installation should run"
        echo "                                  => aws ec2 --region=eu-west-3 describe-regions --output text | awk '{ print $NF }'"
        echo "                                     eu-north-1, eu-west-3, eu-central-1 etc."
      fi
    else
      messageTitle "Azure Access Credentials"
      messagePrint " - AWS AccwssKey"                $(maskPassword "$AWS_ACCESS_KEY")
      messagePrint " - AWS SecretKey"                $(maskPassword "$AWS_SECRET_KEY")
      messagePrint " - AWS Region"                   "$AWS_REGION"
    fi
  fi

  if [ "${TDH_DEPLOYMENT_ENV_NAME}" == "Azure" ]; then
    if [ "${AZURE_SUBSCRIPTION_ID}" == "" -o "${AZURE_TENANT_ID}" == "" -o "${AZURE_LOCATION}" == "" ]; then

      missing_variables=1
      echo ""
      echo "  1MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  --------------------------------------------------------------------------------------------------------------"

      if [ "${AZURE_SUBSCRIPTION_ID}" == "" ]; then
        echo "  AZURE_SUBSCRIPTION_ID           (required) has the format <xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx>"
      fi

      if [ "${AZURE_TENANT_ID}" == "" ]; then
        echo "  AZURE_TENANT_ID                 (required) has the format <xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx>"
      fi

      #if [ "${AZURE_CLIENT_ID}" == "" ]; then
      #  echo "  AZURE_CLIENT_ID                 (required) has the format <xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx>"
      #  echo "                                  => az ad sp create-for-rbac --name \"TanzuDemoHub\" ## appId"
      #fi

      #if [ "${AZURE_CLIENT_SECRET}" == "" ]; then
      #  echo "  AZURE_CLIENT_SECRET             (required) has the format <xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx>"
      #  echo "                                  => az ad sp create-for-rbac --name \"TanzuDemoHub\" ## password"
      #fi

      if [ "${AZURE_LOCATION}" == "" ]; then
        echo "  AZURE_LOCATION                    (required) Choose the Azure Region where your installation should run"
        echo "                                  => az account list-locations -o table | awk '{ print $NF }'"
        echo "                                     westeurope, northeurope, switzerlandnorth etc."
      fi
    else
      messageTitle "Azure Access Credentials"
      messagePrint " - Azure SubscriptionId"         $(maskPassword "$AZURE_SUBSCRIPTION_ID")
      messagePrint " - Azure TennantId"              $(maskPassword "$AZURE_TENANT_ID")
      #messagePrint " - Azure ClientId"               $(maskPassword "$AZURE_CLIENT_ID")
      #messagePrint " - Azure Client Secret"          $(maskPassword "$AZURE_CLIENT_SECRET")
      messagePrint " - Azure Region"                 "$AZURE_LOCATION"
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  --------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the pcfconfig"
    echo "             configuration file ~/.tanzu-demo-hub.cfg and set all variables with the 'export' notation"
    echo "             ie. => export AZURE_PKS_TLS_CERTIFICATE=/home/demouser/certs/cert.pem"
    echo "  --------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkTMCAccess() {
  missing_variables=0

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
    if [ "${TMC_MANAGEMENT_CLUSTER}" == "" -o "${TMC_PROVISONER_NAME}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  2MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  --------------------------------------------------------------------------------------------------------------"
  
      if [ "${TMC_MANAGEMENT_CLUSTER}" == "" ]; then
        echo "  TMC_MANAGEMENT_CLUSTER          (required) A Management Cluster deployed on AWS"
      fi
  
      if [ "${TMC_PROVISONER_NAME}" == "" ]; then
        echo "  TMC_PROVISONER_NAME             (required) The TMC Provisionert Name for AWS"
      fi

      if [ "${AWS_HOSTED_DNS_DOMAIN}" == "" ]; then
        echo "  AWS_HOSTED_DNS_DOMAIN           (required) A DNS Domain managed by AWS Route 53 ie. yourdomain.com with a"
        echo "                                  valid DNS Zone. => https://console.aws.amazon.com/route53"
      fi
    else
      messageTitle "Supporting services access (Pivotal Network, AWS Route53)"
      messagePrint " - Pivotal Network Token"           "$PCF_PIVNET_TOKEN"
      messagePrint " - AWS Route53 Hosted DNS Domain"   "$AWS_HOSTED_DNS_DOMAIN"

      # --- CHECK IF AWS CLI IS CONFIGURED ---
      if [ ! -d ~/.aws -o -d ~/.aws/credentials ]; then
        echo "ERROR: AWS CLI is not configured yet, please run aws configure"
        echo "       => aws configure"
        exit 1
      fi
    fi
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  --------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the pcfconfig"
    echo "             configuration file ~/.pcfconfig and set all variables with the 'export' notation"
    echo "             ie. => export AZURE_PKS_TLS_CERTIFICATE=/home/demouser/certs/cert.pem"
    echo "  --------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkTDHAccess() {
  missing_variables=0

  if [ "${PCF_PIVNET_TOKEN}" == "" -o "${AWS_HOSTED_DNS_DOMAIN}" == "" ]; then
    missing_variables=1
    echo ""
    echo "  2MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
    echo "  --------------------------------------------------------------------------------------------------------------"

    if [ "${PCF_PIVNET_TOKEN}" == "" ]; then
      echo "  PCF_PIVNET_TOKEN                (required) PIVNET Access Token to download software. Get a UAA API TOKEN from"
      echo "                                  => https://network.pivotal.io/users/dashboard/edit-profile"
    fi

    if [ "${AWS_ACCESS_KEY}" == "" ]; then
      echo "  AWS_ACCESS_KEY                  (required) A DNS Domain managed by AWS Route 53 ie. yourdomain.com with a"
      echo "                                  valid DNS Zone. => https://console.aws.amazon.com/route53"
    fi

    if [ "${AWS_SECRET_KEY}" == "" ]; then
      echo "  AWS_SECRET_KEY                  (required) A DNS Domain managed by AWS Route 53 ie. yourdomain.com with a"
      echo "                                  valid DNS Zone. => https://console.aws.amazon.com/route53"
    fi

    if [ "${AWS_HOSTED_DNS_DOMAIN}" == "" ]; then
      echo "  AWS_HOSTED_DNS_DOMAIN           (required) A DNS Domain managed by AWS Route 53 ie. yourdomain.com with a"
      echo "                                  valid DNS Zone. => https://console.aws.amazon.com/route53"
    fi

    #if [ "${AWS_HOSTED_ZONE_ID}" == "" ]; then
    #  echo "  AWS_HOSTED_ZONE_ID              (required) An AWS Route 53 Token is required to add/update DNS records"
    #  echo "                                  => https://console.aws.amazon.com/route53 -> hosted Zones -> Zone ID"
    #fi
  else
    messageTitle "Supporting services access (Pivotal Network, AWS Route53)"
    messagePrint " - Pivotal Network Token"            "$PCF_PIVNET_TOKEN"
    messagePrint " - AWS Route53 Hosted DNS Domain"    "$AWS_HOSTED_DNS_DOMAIN"
    messagePrint " - AWS Route53 Hosted DNS SubDomain" "$TDH_ENVNAME"

    # --- VERIFY PIVNET ACCESS ---
    pivnetAPI $PCF_PIVNET_TOKEN
    if [ $? -ne 0 ]; then
      write_line
      echo "ERROR: Pivnet Token: $PCF_PIVNET_TOKEN does not seam to be valit. Please greate a new one by"
      echo "       login to http://network.pivotal.io => Your login Name => Edit Profile => UAA API TOKEN"
      exit 1
    fi

    # --- CHECK IF AWS CLI IS CONFIGURED ---
    if [ ! -d ~/.aws -o -d ~/.aws/credentials ]; then
      echo "ERROR: AWS CLI is not configured yet, please run aws configure"
      echo "       => aws configure"
      exit 1
    fi

    export AWS_HOSTED_ZONE_ID=$(aws route53 list-hosted-zones --query "HostedZones[?starts_with(to_string(Name), '${AWS_HOSTED_DNS_DOMAIN}.')]" | \
                        jq -r '.[].Id' | awk -F '/' '{ print $NF }')
    if [ $? -ne 0 -o "${AWS_HOSTED_ZONE_ID}" == "" ]; then
      echo "ERROR: failed to get domain information for ($AWS_HOSTED_DNS_DOMAIN)"
      echo "       => aws route53 list-hosted-zones --query \"HostedZones[?starts_with(to_string(Name), '${AWS_HOSTED_DNS_DOMAIN}.')]\""
      exit 1
    else
      messagePrint " - AWS Route53 ZoneID"              "$AWS_HOSTED_ZONE_ID"
    fi

    pslug="pivotal-container-service"; pver="1.5"
    pslug="elastic-runtime"; pver="2.6.6"
    pslug="elastic-runtime"; pver="467928"
  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  --------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the pcfconfig"
    echo "             configuration file ~/.pcfconfig and set all variables with the 'export' notation"
    echo "             ie. => export AZURE_PKS_TLS_CERTIFICATE=/home/demouser/certs/cert.pem"
    echo "  --------------------------------------------------------------------------------------------------------------"
    exit 1
  fi

} 

pivnetAPI() {
  REFRESH_TOKEN="$1"
  token=$(curl -X POST https://network.pivotal.io/api/v2/authentication/access_tokens \
               -d "{\"refresh_token\":\"$REFRESH_TOKEN\"}" 2>/dev/null | jq -r '.access_token')
  if [ "${token}" == "" -o "${token}" == "null" ]; then return 1; fi

  # --- LOGIN INTO PIVNET ---
  curl -X GET https://network.pivotal.io/api/v2/authentication -H "Authorization: Bearer $token"

  # --- PIVNET ACCESS TEST ---
  if [ "${2}" == "" ]; then
    return 0
  fi

  curl -H "Authorization: Bearer $token" -X $2 https://network.pivotal.io/api/v2/$3

  return 0
}

write_line() {
  messageTitle "----------------------------------------------------------------------------------------------------------------"
}

createENVfile() {
  dep="$1"
  envFile="$2"

  rm -f $env
  . ${TANZU_DEMO_HUB}/deployments/$dep

  cat ${TANZU_DEMO_HUB}/deployments/$dep | sed -e '/^$/d' -e '/^#/d' -e 's/#.*$//g' > $envFile

  echo "PCF_DEPLOYMENT_DEBUG=$PCF_DEPLOYMENT_DEBUG"                              >> $envFile
  echo "AWS_HOSTED_ZONE_ID=$AWS_HOSTED_ZONE_ID"                                  >> $envFile
  echo "AWS_HOSTED_DNS_DOMAIN=$AWS_HOSTED_DNS_DOMAIN"                            >> $envFile
  echo "PCF_PIVNET_TOKEN=$PCF_PIVNET_TOKEN"                                      >> $envFile

  echo "PCF_TILE_PKS_ADMIN_USER=$PCF_TILE_PKS_ADMIN_USER"                        >> $envFile
  echo "PCF_TILE_PKS_ADMIN_PASS=$PCF_TILE_PKS_ADMIN_PASS"                        >> $envFile
  echo "PCF_TILE_PKS_ADMIN_EMAIL=$PCF_TILE_PKS_ADMIN_EMAIL"                      >> $envFile
  echo "PCF_TILE_PAS_ADMIN_USER=$PCF_TILE_PAS_ADMIN_USER"                        >> $envFile
  echo "PCF_TILE_PAS_ADMIN_PASS=$PCF_TILE_PAS_ADMIN_PASS"                        >> $envFile
  echo "PCF_TILE_PAS_ADMIN_EMAIL=$PCF_TILE_PAS_ADMIN_EMAIL"                      >> $envFile

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "GCP" ]; then
    #echo "GCP_SERVICE_ACCOUNT=/tmp/$(basename $GCP_SERVICE_ACCOUNT)"             >> $envFile
    echo "GCP_REGION=\"$GCP_REGION\""                                            >> $envFile
    echo "GCP_PROJECT=\"$GCP_PROJECT\""                                          >> $envFile

    echo "PCF_TERRAFORMS_TEMPLATE_BUNDLE=\"$PCF_TERRAFORMS_TEMPLATE_BUNDLE\""    >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_NAME=\"$PCF_TERRAFORMS_TEMPLATE_NAME\""        >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_VERSION=\"$PCF_TERRAFORMS_TEMPLATE_VERSION\""  >> $envFile

    if [ "${PCF_TILE_PKS_DEPLOY}" == "true" ]; then
      if [ "${GCP_PKS_TLS_CERTIFICATE}" != "" -a "${GCP_PKS_TLS_FULLCHAIN}" != "" -a \
           "${GCP_PKS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$GCP_PKS_TLS_CERTIFICATE\""              >> $envFile
        echo "TLS_FULLCHAIN=\"$GCP_PKS_TLS_FULLCHAIN\""                  >> $envFile
        echo "TLS_PRIVATE_KEY=\"$GCP_PKS_TLS_PRIVATE_KEY\""              >> $envFile
      fi
    fi

    if [ "${PCF_TILE_PAS_DEPLOY}" == "true" ]; then
      if [ "${GCP_PAS_TLS_CERTIFICATE}" != "" -a "${GCP_PAS_TLS_FULLCHAIN}" != "" -a \
           "${GCP_PAS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$GCP_PAS_TLS_CERTIFICATE\""              >> $envFile
        echo "TLS_FULLCHAIN=\"$GCP_PAS_TLS_FULLCHAIN\""                  >> $envFile
        echo "TLS_PRIVATE_KEY=\"$GCP_PAS_TLS_PRIVATE_KEY\""              >> $envFile
      fi
    fi

    if [ "${PCF_TILE_HARBOR_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_HARBOR_ADMIN_PASS=\"$PCF_TILE_HARBOR_ADMIN_PASS\"" >> $envFile
    fi
    if [ "${PCF_TILE_PBS_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_PBS_DEPLOY=\"$PCF_TILE_PBS_DEPLOY\""               >> $envFile
      echo "PCF_TILE_PBS_NAME=\"$PCF_TILE_PBS_NAME\""                   >> $envFile
      echo "PCF_TILE_PBS_SLUG=\"$PCF_TILE_PBS_SLUG\""                   >> $envFile
      echo "PCF_TILE_PBS_VERSION=\"$PCF_TILE_PBS_VERSION\""             >> $envFile
      echo "PCF_TILE_PBS_ADMIN_USER=\"$PCF_TILE_PBS_ADMIN_USER\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_PASS=\"$PCF_TILE_PBS_ADMIN_PASS\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_EMAIL=\"$PCF_TILE_PBS_ADMIN_EMAIL\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_REPO=\"$PCF_TILE_PBS_DOCKER_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_USER=\"$PCF_TILE_PBS_DOCKER_USER\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_PASS=\"$PCF_TILE_PBS_DOCKER_PASS\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_REPO=\"$PCF_TILE_PBS_GITHUB_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_USER=\"$PCF_TILE_PBS_GITHUB_USER\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_PASS=\"$PCF_TILE_PBS_GITHUB_PASS\""     >> $envFile
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
    echo "AWS_ACCESS_KEY=\"$AWS_ACCESS_KEY\""                                    >> $envFile
    echo "AWS_SECRET_KEY=\"$AWS_SECRET_KEY\""                                    >> $envFile
    echo "AWS_REGION=\"$AWS_REGION\""                                            >> $envFile

    echo "PCF_TERRAFORMS_TEMPLATE_BUNDLE=\"$PCF_TERRAFORMS_TEMPLATE_BUNDLE\""    >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_NAME=\"$PCF_TERRAFORMS_TEMPLATE_NAME\""        >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_VERSION=\"$PCF_TERRAFORMS_TEMPLATE_VERSION\""  >> $envFile

    if [ "${PCF_TILE_PKS_DEPLOY}" == "true" ]; then
      if [ "${AWS_PKS_TLS_CERTIFICATE}" != "" -a "${AWS_PKS_TLS_FULLCHAIN}" != "" -a \
           "${AWS_PKS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$AWS_PKS_TLS_CERTIFICATE\""              >> $envFile
        echo "TLS_FULLCHAIN=\"$AWS_PKS_TLS_FULLCHAIN\""                  >> $envFile
        echo "TLS_PRIVATE_KEY=\"$AWS_PKS_TLS_PRIVATE_KEY\""              >> $envFile
      fi
    fi

    if [ "${PCF_TILE_PAS_DEPLOY}" == "true" ]; then
      if [ "${AWS_PAS_TLS_CERTIFICATE}" != "" -a "${AWS_PAS_TLS_FULLCHAIN}" -o \
           "${AWS_PAS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$AWS_PAS_TLS_CERTIFICATE\""              >> $envFile
        echo "TLS_FULLCHAIN=\"$AWS_PAS_TLS_FULLCHAIN\""                  >> $envFile
        echo "TLS_PRIVATE_KEY=\"$AWS_PAS_TLS_PRIVATE_KEY\""              >> $envFile
      fi
    fi

    if [ "${PCF_TILE_HARBOR_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_HARBOR_ADMIN_PASS=\"$PCF_TILE_HARBOR_ADMIN_PASS\"" >> $envFile
    fi

    if [ "${PCF_TILE_PBS_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_PBS_DEPLOY=\"$PCF_TILE_PBS_DEPLOY\""               >> $envFile
      echo "PCF_TILE_PBS_NAME=\"$PCF_TILE_PBS_NAME\""                   >> $envFile
      echo "PCF_TILE_PBS_SLUG=\"$PCF_TILE_PBS_SLUG\""                   >> $envFile
      echo "PCF_TILE_PBS_VERSION=\"$PCF_TILE_PBS_VERSION\""             >> $envFile
      echo "PCF_TILE_PBS_ADMIN_USER=\"$PCF_TILE_PBS_ADMIN_USER\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_PASS=\"$PCF_TILE_PBS_ADMIN_PASS\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_EMAIL=\"$PCF_TILE_PBS_ADMIN_EMAIL\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_REPO=\"$PCF_TILE_PBS_DOCKER_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_USER=\"$PCF_TILE_PBS_DOCKER_USER\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_PASS=\"$PCF_TILE_PBS_DOCKER_PASS\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_REPO=\"$PCF_TILE_PBS_GITHUB_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_USER=\"$PCF_TILE_PBS_GITHUB_USER\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_PASS=\"$PCF_TILE_PBS_GITHUB_PASS\""     >> $envFile
    fi
  fi

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" ]; then
    echo "AZURE_SUBSCRIPTION_ID=\"$AZURE_SUBSCRIPTION_ID\""                      >> $envFile
    echo "AZURE_TENANT_ID=\"$AZURE_TENANT_ID\""                                  >> $envFile
    echo "AZURE_CLIENT_ID=\"$AZURE_CLIENT_ID\""                                  >> $envFile
    echo "AZURE_CLIENT_SECRET=\"$AZURE_CLIENT_SECRET\""                          >> $envFile
    echo "AZURE_LOCATION=\"$AZURE_LOCATION\""                                    >> $envFile

    echo "PCF_TERRAFORMS_TEMPLATE_BUNDLE=\"$PCF_TERRAFORMS_TEMPLATE_BUNDLE\""    >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_NAME=\"$PCF_TERRAFORMS_TEMPLATE_NAME\""        >> $envFile
    echo "PCF_TERRAFORMS_TEMPLATE_VERSION=\"$PCF_TERRAFORMS_TEMPLATE_VERSION\""  >> $envFile

    if [ "${PCF_TILE_PKS_DEPLOY}" == "true" ]; then
      if [ "${AZURE_PKS_TLS_CERTIFICATE}" != "" -a "${AZURE_PKS_TLS_FULLCHAIN}" -o \
           "${AZURE_PKS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$AZURE_PKS_TLS_CERTIFICATE\""           >> $envFile
        echo "TLS_FULLCHAIN=\"$AZURE_PKS_TLS_FULLCHAIN\""               >> $envFile
        echo "TLS_PRIVATE_KEY=\"$AZURE_PKS_TLS_PRIVATE_KEY\""           >> $envFile
      fi
    fi

    if [ "${PCF_TILE_PAS_DEPLOY}" == "true" ]; then
      if [ "${AZURE_PAS_TLS_CERTIFICATE}" != "" -a "${AZURE_PAS_TLS_FULLCHAIN}" -o \
           "${AZURE_PAS_TLS_PRIVATE_KEY}" != "" ]; then

        echo "TLS_CERTIFICATE=\"$AZURE_PAS_TLS_CERTIFICATE\""           >> $envFile
        echo "TLS_FULLCHAIN=\"$AZURE_PAS_TLS_FULLCHAIN\""               >> $envFile
        echo "TLS_PRIVATE_KEY=\"$AZURE_PAS_TLS_PRIVATE_KEY\""           >> $envFile
      fi
    fi

    if [ "${PCF_TILE_HARBOR_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_HARBOR_ADMIN_PASS=\"$PCF_TILE_HARBOR_ADMIN_PASS\"" >> $envFile
    fi

    if [ "${PCF_TILE_PBS_DEPLOY}" == "true" ]; then
      echo "PCF_TILE_PBS_DEPLOY=\"$PCF_TILE_PBS_DEPLOY\""               >> $envFile
      echo "PCF_TILE_PBS_NAME=\"$PCF_TILE_PBS_NAME\""                   >> $envFile
      echo "PCF_TILE_PBS_SLUG=\"$PCF_TILE_PBS_SLUG\""                   >> $envFile
      echo "PCF_TILE_PBS_VERSION=\"$PCF_TILE_PBS_VERSION\""             >> $envFile
      echo "PCF_TILE_PBS_ADMIN_USER=\"$PCF_TILE_PBS_ADMIN_USER\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_PASS=\"$PCF_TILE_PBS_ADMIN_PASS\""       >> $envFile
      echo "PCF_TILE_PBS_ADMIN_EMAIL=\"$PCF_TILE_PBS_ADMIN_EMAIL\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_REPO=\"$PCF_TILE_PBS_DOCKER_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_USER=\"$PCF_TILE_PBS_DOCKER_USER\""     >> $envFile
      echo "PCF_TILE_PBS_DOCKER_PASS=\"$PCF_TILE_PBS_DOCKER_PASS\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_REPO=\"$PCF_TILE_PBS_GITHUB_REPO\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_USER=\"$PCF_TILE_PBS_GITHUB_USER\""     >> $envFile
      echo "PCF_TILE_PBS_GITHUB_PASS=\"$PCF_TILE_PBS_GITHUB_PASS\""     >> $envFile
    fi
  fi
}

configureJumpHost() {
  messagePrint "Configure Jump Server" "$SSH_HOST"
echo "$SSH_COMMAND -n id "

  # --- WAIT UNTIL SSH DEAMON IS READY NO JUMPHOST ----
  ret=1
  while [ $ret -ne 0 ]; do
    $SSH_COMMAND -n id > /dev/null 2>&1; ret=$?
    [ $ret -ne 0 ] && sleep 10
  done

  GITTDH="https://github.com/pivotal-sadubois/tanzu-demo-hub.git"
  TDHHOME="${SSH_HOME}/tanzu-demo-hub"

  messagePrint " - Verify SSH Access" "success"

  $SSH_COMMAND -n "[ ! -d /home/ubuntu/tanzu-demo-hub ] && git clone $GITTDH > /dev/null 2>&1"
  $SSH_COMMAND -n "[ -d /home/ubuntu/tanzu-demo-hub ] && cd $SSH_HOME/tanzu-demo-hub; git pull > /dev/null 2>&1"

  # --- HOSTNAME ---
#  if [ "${TDH_DEPLOYMENT_CLOUD}" == "Azure" -o "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
#    $SSH_COMMAND -n "sudo hostname $JUMP_HOST > /dev/null 2>&1"
#  fi

  # --- TEST FOR RUNNING REMOTE SCRIPT ---
  stt=$($SSH_COMMAND -n "[ -f /tmp/tanzu-demo-hub.pid ] && pgrep -F /tmp/tanzu-demo-hub.pid")
  stt=$($SSH_COMMAND -n "[ -f /jump_software_installed ] && echo true")
  if [ "${stt}" == "" ]; then
    messagePrint " - Install CLI Utilities"   "aws,az,gcp,bosh,pivnet,cf,om,jq"
    $SSH_COMMAND -n "[ -d $SSH_HOME/tanzu-demo-hub ] && chmod a+x ${TDHHOME}/scripts/InstallUtilities.sh && \
        sudo ${TDHHOME}/scripts/InstallUtilities.sh $PCF_PIVNET_TOKEN > /dev/null 2>&1"

    # --- COPY THE AWS DIRECTORY ---
    $SCP_COMMAND -r ~/.aws ${SSH_USER}@${SSH_HOST}:$SSH_HOME > /dev/null 2>&1
    $SCP_COMMAND -r ~/.azure ${SSH_USER}@${SSH_HOST}:$SSH_HOME > /dev/null 2>&1
    $SCP_COMMAND -r ~/.tanzu-demo-hub ${SSH_USER}@${SSH_HOST}:$SSH_HOME > /dev/null 2>&1
    $SCP_COMMAND -r ~/.tanzu-demo-hub.cfg ${SSH_USER}@${SSH_HOST}:$SSH_HOME > /dev/null 2>&1
  fi

  # --- COPY SOFTWARE ---
  stt=$($SSH_COMMAND -n "[ -d /home/ubuntu/tanzu-demo-hub/software ] && echo true")
  if [ "${stt}" == "" ]; then
    # --- COPY THE AWS DIRECTORY ---
    $SSH_COMMAND -n "[ ! -d $SSH_HOME/tanzu-demo-hub/software ] && mkdir -p $SSH_HOME/tanzu-demo-hub/software"
    if [ "$TDH_TKGMC_INFRASTRUCTURE" == "Azure" ]; then
      $SCP_COMMAND -r ${TDHPATH}/software/tkg-* ${SSH_USER}@${SSH_HOST}:${SSH_HOME}/tanzu-demo-hub/software > /dev/null 2>&1
    fi

    if [ "$TDH_TKGMC_INFRASTRUCTURE" == "vSphere" ]; then
      $SCP_COMMAND -r ${TDHPATH}/software/tkg-* ${SSH_USER}@${SSH_HOST}:${SSH_HOME}/tanzu-demo-hub/software > /dev/null 2>&1
      $SCP_COMMAND -r ${TDHPATH}/software/pho* ${SSH_USER}@${SSH_HOST}:${SSH_HOME}/tanzu-demo-hub/ > /dev/null 2>&1
    fi
  fi

  # --- INSTALL TKG UTILITES ---
  stt=$($SSH_COMMAND -n "[ -f /tkg_software_installed ] && echo true")
  if [ "${stt}" == "" ]; then
    messagePrint " - Install TKG Utilities"   "tkg, ytt, kapp, kbld, kubectl, kind"
    $SSH_COMMAND -n "[ -d $SSH_HOME/tanzu-demo-hub ] && chmod a+x ${TDHHOME}/scripts/InstallTKGutilities.sh && \
        sudo ${TDHHOME}/scripts/InstallTKGutilities.sh \"$SSH_HOME/tanzu-demo-hub\" > /dev/null 2>&1"
    #$SSH_COMMAND -n "[ -d $SSH_HOME/tanzu-demo-hub ] && chmod a+x ${TDHHOME}/scripts/InstallTKGutilities.sh && \
    #    sudo ${TDHHOME}/scripts/InstallTKGutilities.sh \"$SSH_HOME/tanzu-demo-hub\"" 

    # --- WAIT UNTIL SSH DEAMON IS READY NO JUMPHOST ----
    ret=1
    while [ $ret -ne 0 ]; do
      $SSH_COMMAND -n id > /dev/null 2>&1; ret=$?
      [ $ret -ne 0 ] && sleep 10
    done
  fi
}

verifyHostedZone() {
  zone=$1

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${zone} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$zone.\")).Id")
  if [ "${ZONE_ID}" == "" ]; then
    messagePrint "Create DNS Hosted Zone" "$zone"
    route53createHostedZone $zone
  else
    messagePrint "Verify DNS Hosted Zone" "$zone"
  fi

}

route53createHostedZone() {
  zone=$1

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${zone} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$zone.\")).Id")
  if [ "${ZONE_ID}" != "" ]; then
    route53deleteHostedZone $ZONE_ID
  fi

  aws route53 create-hosted-zone --name $zone --caller-reference "$(date)" \
       --hosted-zone-config Comment="Managed by pcfconfig" > /dev/null 2>&1
  if [ $? -ne 0 ]; then
      echo "ERROR: failed to create AWS Route53 zone $ZONE_ID"
      echo "       => aws route53 create-hosted-zone --name $zone --caller-reference \"$(date)\" \\"
      echo "          --hosted-zone-config Comment=\"command-line version\""
      exit 1
  fi

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${zone} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$zone.\")).Id")
  NAME_SERVERS=$(aws route53 get-hosted-zone --id $ZONE_ID | jq -r '.DelegationSet.NameServers[]')

  # --- CREATE ZONE ---
  route53setNSrecord $zone "$AWS_HOSTED_DNS_DOMAIN" "$NAME_SERVERS"
}

route53setNSrecord () {
  hnm=$1; dom=$2; dns="$3"

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${dom} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$dom.\")).Id")
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')
  if [ "${ZONE_ID}" == "" ]; then
    echo "ERROR: ZoneID for domain $dom can not be optained, please veriofy manually"
    echo "       aws route53 list-hosted-zones-by-name --dns-name ${dom}"
    exit 1
  fi

  DNS1=$(echo $dns | awk '{ print $1 }')
  DNS2=$(echo $dns | awk '{ print $2 }')
  DNS3=$(echo $dns | awk '{ print $3 }')
  DNS4=$(echo $dns | awk '{ print $4 }')

  TMPROUTE53=/tmp/$$_tmp_route53.json
  echo "{"                                                   >  $TMPROUTE53
  echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","  >> $TMPROUTE53
  echo "  \"Changes\": [{"                                   >> $TMPROUTE53
  echo "  \"Action\": \"UPSERT\","                           >> $TMPROUTE53
  echo "  \"ResourceRecordSet\": {"                          >> $TMPROUTE53
  echo "    \"Name\": \"${hnm}\","                           >> $TMPROUTE53
  echo "    \"Type\": \"NS\","                               >> $TMPROUTE53
  echo "    \"TTL\": 300,"                                   >> $TMPROUTE53
  echo "    \"ResourceRecords\": [ "                         >> $TMPROUTE53
  echo "      { \"Value\": \"${DNS1}\" },"                   >> $TMPROUTE53
  echo "      { \"Value\": \"${DNS2}\" },"                   >> $TMPROUTE53
  echo "      { \"Value\": \"${DNS3}\" },"                   >> $TMPROUTE53
  echo "      { \"Value\": \"${DNS4}\" }"                    >> $TMPROUTE53
  echo "    ]"                                               >> $TMPROUTE53
  echo "}}]"                                                 >> $TMPROUTE53
  echo "}"                                                   >> $TMPROUTE53

  aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
      --change-batch file://${TMPROUTE53} > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "4ERROR: failed to set DNS for $hnm"
    echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
    echo "              --change-batch file://${TMPROUTE53}"
    cat /tmp/$$_zone_record
    exit 1
  fi
}

route53deleteHostedZone() {
  ZONE_ID=$1
  aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID | jq -c '.ResourceRecordSets[]' | \
  while read -r resourcerecordset; do
    read -r name type <<<$(echo $(jq -r '.Name,.Type' <<<"$resourcerecordset"))

     if [ $type != "NS" -a $type != "SOA" ]; then
        aws route53 change-resource-record-sets \
          --hosted-zone-id $ZONE_ID \
          --change-batch '{"Changes":[{"Action":"DELETE","ResourceRecordSet":
          '"$resourcerecordset"'
        }]}' \
        --output text --query 'ChangeInfo.Id' >/dev/null 2>&1
     fi
  done

  aws route53 delete-hosted-zone --id $ZONE_ID >/dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "ERROR: failed to delete AWS Route53 zone $ZONE_ID"
    echo "       => aws route53 delete-hosted-zone --id $ZONE_ID"
    #exit 1
  fi
}

checkKeyPairs_old() {
  SSH_KEY_NAME=vmware-cloud-tmc
  SSH_KEY_FILE=~/.tanzu-demo-hub/KeyPair-${SSH_KEY_NAME}-${AWS_REGION}.pem

  #"KeyFingerprint": "a8:c4:01:2b:12:7e:0d:8f:56:8c:38:80:cf:8b:6d:53:13:9c:28:cd",
  #aws ec2 --region=eu-central-1 create-key-pair --dry-run --key-name sacha
  #/tmp/key.pem

  if [ ! -d ~/.tanzu-demo-hub ] ; then mkdir ~/.tanzu-demo-hub; fi

  # --- VERIFY KEY-PAIR ---
  key=$(aws ec2 --region=$AWS_REGION describe-key-pairs | \
        jq -r --arg key "$SSH_KEY_NAME" '.KeyPairs[] | select(.KeyName == $key).KeyFingerprint')

  # --- CREATE ONE IF IT DOES NOT EXIST ---
  if [ "${key}" == "" ]; then
    aws ec2 --region=$AWS_REGION create-key-pair --key-name vmware-cloud-tmc | \
       jq -r '.KeyMaterial' > $SSH_KEY_FILE
    chmod 600 $SSH_KEY_FILE
  fi

  if [ -f "${SSH_KEY_FILE}" ]; then
    kfp=$(openssl pkcs8 -in $SSH_KEY_FILE -inform PEM -outform DER -topk8 -nocrypt | openssl sha1 -c)

    if [ "${key}" != "${kfp}" ]; then
      echo "ERROR: Fingerprint of AWS SSH Key-pair ($SSH_KEY_NAME) and the local PEM file: "
      echo "       $SSH_KEY_FILE are not the same"
      exit
    fi
  fi
}

maskPassword() {
  echo "$1" | sed 's/[^-]/X/g'
}

route53getIPaddress() {
  env=$1
  dom=$2
  hst="jump-${env}.${dom}."
  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${dom} | jq -r ".HostedZones[] | select(.Name | \
            scan(\"^$dom.\")).Id")
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')

  aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID_STR --query "ResourceRecordSets[?Name == '${hst}']" | \
  jq -r '.[].ResourceRecords[].Value'
}

#route53setDNSrecord "$pip" "$JUMP_HOST" "$AWS_HOSTED_DNS_DOMAIN"
route53setDNSrecord() {
  ipa=$1; hnm=$2; dom=$3

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${dom} | jq -r '.HostedZones[0].Id')
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')
  if [ "${ZONE_ID}" == "" ]; then
    echo "ERROR: ZoneID for domain $dom can not be optained, please veriofy manually"
    echo "       aws route53 list-hosted-zones-by-name --dns-name ${dom}"
    exit 1
  fi

cat << EOF | sed -e "s/FQHN/$hnm/g" -e "s/IPADDR/$ipa/g" > /tmp/$$_zone_record
{
            "Comment": "CREATE/DELETE/UPSERT a record ",
            "Changes": [{
            "Action": "UPSERT",
                        "ResourceRecordSet": {
                                    "Name": "FQHN",
                                    "Type": "A",
                                    "TTL": 300,
                                 "ResourceRecords": [{ "Value": "IPADDR"}]
}}]
}
EOF

   aws route53 change-resource-record-sets --hosted-zone-id "${ZONE_ID}" --change-batch file:///tmp/$$_zone_record > /dev/null 2>&1
   if [ $? -ne 0 ]; then
     echo "5ERROR: failed to set DNS for $hnm"
     echo "       => aws route53 change-resource-record-sets --hosted-zone-id "${ZONE_ID}" --change-batch file:///tmp/$$_zone_record"
     cat /tmp/$$_zone_record
     exit 1
   fi
}

createCluster() {
  unset TKG_CONFIG
  TKG_TEMPLATE=${HOME}/.tanzu-demo-hub/${TDH_TKGMC_CONFIG}
  TKG_KUBECONFIG=${HOME}/.tanzu-demo-hub/${TDH_TKGMC_NAME}.kubeconfig
  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "Azure" ]; then
    MDOE=azure
    messageTitle "Creating TKG Managment Cluster"
    messagePrint " - Cluster Name"                "$TDH_TKGMC_NAME"
    messagePrint " - Configuration File"          "$TKG_TEMPLATE"
    messagePrint " - Control Plane Machine Type"  "$TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE"
    messagePrint " - Worker Node Machine Type"    "$TDH_TKGMC_MACHINE_TYPE"
    messagePrint " - Cluster CIDR"                "$TDH_TKGMC_CLUSTER_CIDR"
    messagePrint " - Service CIDR"                "$TDH_TKGMC_SERVICE_CIDR"
    messagePrint " - Health Check Enabled"        "$TDH_TKGMC_MACHINE_HEALTH_CHECK_ENABLED"

    [ -f ${HOME}/.tkg/config.yaml ] && rm -f ${HOME}/.tkg/config.yaml
    tkg get mc > /dev/null 2>&1
    cp ${HOME}/.tkg/config.yaml $TKG_TEMPLATE

    AZURE_SSH_PUBLIC_KEY_B64=$(cat ~/.tanzu-demo-hub/KeyPair-Azure.pub | base64 -w 10000)
    AZURE_CLIENT_ID=$(az ad app list --display-name "TanzuDemoHub" | jq -r '.[].appId')
    AZURE_CLIENT_SECRET="tanzu-demo-hub"

    echo "AZURE_SSH_PUBLIC_KEY_B64: $AZURE_SSH_PUBLIC_KEY_B64"                        >> $TKG_TEMPLATE
    echo "AZURE_NODE_MACHINE_TYPE: $TDH_TKGMC_MACHINE_TYPE"                           >> $TKG_TEMPLATE
    echo "AZURE_TENANT_ID: $AZURE_TENANT_ID"                                          >> $TKG_TEMPLATE
    echo "AZURE_CLIENT_ID: $AZURE_CLIENT_ID"                                          >> $TKG_TEMPLATE
    echo "AZURE_CLIENT_SECRET: $AZURE_CLIENT_SECRET"                                  >> $TKG_TEMPLATE
    echo "AZURE_CONTROL_PLANE_MACHINE_TYPE: $TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE"    >> $TKG_TEMPLATE
    echo "SERVICE_CIDR: $TDH_TKGMC_SERVICE_CIDR"                                      >> $TKG_TEMPLATE
    echo "CLUSTER_CIDR: $TDH_TKGMC_CLUSTER_CIDR"                                      >> $TKG_TEMPLATE
    echo "AZURE_LOCATION: $AZURE_LOCATION"                                            >> $TKG_TEMPLATE
    echo "AZURE_SUBSCRIPTION_ID: $AZURE_SUBSCRIPTION_ID"                              >> $TKG_TEMPLATE

    MPDE=azure

    stt=$(tkg get mc --name $TDH_TKGMC_NAME -o json | jq -r '.[].status')
    if [ "${stt}" != "Success" ]; then
      echo "----------------------------------------------------------------------------------------------------------------"
      echo "tkg init -q -i $MPDE -p dev --ceip-participation true --cni $TDH_TKGMC_CNI -v 0 \\"
      echo "         --name $TDH_TKGMC_NAME --config $TKG_TEMPLATE --kubeconfig=$TKG_KUBECONFIG"
      echo "----------------------------------------------------------------------------------------------------------------"
      time tkg init -i $MPDE -p dev --ceip-participation true --cni $TDH_TKGMC_CNI -v 0 --name $TDH_TKGMC_NAME --config $TKG_TEMPLATE --kubeconfig=$TKG_KUBECONFIG
      echo "----------------------------------------------------------------------------------------------------------------"
    fi
  fi

  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "vSphere" ]; then
    MODE=vsphere
    MC_CLUSTER_STATUS=1  ### MC-CLUSTER IS ACTIVE

    # --- ADJUST CONFIG FILE ---
    echo y | tkg get mc > /dev/null 2>&1

    if [ -f $TKG_TEMPLATE ]; then 
      stt=$(tkg get mc --name $TDH_TKGMC_NAME -o json --config $TKG_TEMPLATE | jq -r '.[].status')
      if [ "${stt}" != "Success" ]; then
        rm -f $TKG_TEMPLATE
        MC_CLUSTER_STATUS=0
      fi
    else
      MC_CLUSTER_STATUS=0
    fi

    if [ $MC_CLUSTER_STATUS -eq 0 ]; then 
      messageTitle "Creating TKG Managment Cluster"
      messagePrint " - Cluster Name"                "$TDH_TKGMC_NAME"
      messagePrint " - Configuration File"          "$TKG_TEMPLATE"
      messagePrint " - Control Plane Machine Type"  "$TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE"
      messagePrint " - Worker Node Machine Type"    "$TDH_TKGMC_MACHINE_TYPE"
      messagePrint " - Cluster CIDR"                "$TDH_TKGMC_CLUSTER_CIDR"
      messagePrint " - Service CIDR"                "$TDH_TKGMC_SERVICE_CIDR"
      messagePrint " - Health Check Enabled"        "$TDH_TKGMC_MACHINE_HEALTH_CHECK_ENABLED"

      export GOVC_INSECURE=1
      export GOVC_URL=https://${VSPHERE_SERVER}/sdk
      export GOVC_USERNAME=$VSPHERE_ADMIN
      export GOVC_PASSWORD=$VSPHERE_PASSWORD
      export GOVC_DATASTORE=$VSPHERE_DATASTORE
      export GOVC_NETWORK="$VSPHERE_MANAGEMENT_NETWORK"
      export GOVC_RESOURCE_POOL=/${VSPHERE_DATACENTER}/host/${VSPHERE_CLUSTER}/Resources

      nodes=$(govc find -name "$TDH_TKGMC_NAME*")
      if [ $? != 0 ]; then 
        echo "ERROR: failed to login to vSphere"
        echo "       => govc find -name \"$TDH_TKGMC_NAME*\""; exit
      fi

      for n in $nodes; do
        messagePrint " - Cleanup Old Cluster Nodes"  "$n"
        govc vm.destroy $n > /dev/null 2>&1
        if [ $? != 0 ]; then 
          echo "ERROR: failed to login to vSphere"
          echo "       => govc vm.destroy $n"; exit 
        fi
      done

      for n in $(kind get clusters); do
        messagePrint " - Cleanup local kind Clusters:"  "$n"
        kind delete clusters $n > /dev/null 2>&1
      done

      [ -f ${HOME}/.tkg/config.yaml ] && rm -f ${HOME}/.tkg/config.yaml
      tkg get mc > /dev/null 2>&1
      cp ${HOME}/.tkg/config.yaml $TKG_TEMPLATE

      echo "VSPHERE_WORKER_MEM_MIB: 2048"                                                 >> $TKG_TEMPLATE
      echo "_VSPHERE_CONTROL_PLANE_ENDPOINT_IP: 10.1.1.30"                                >> $TKG_TEMPLATE
      echo "VSPHERE_CONTROL_PLANE_DISK_GIB: 20"                                           >> $TKG_TEMPLATE
      echo "VSPHERE_PASSWORD: <encoded:$(eval printf \"$VSPHERE_PASSWORD\" | base64)>"    >> $TKG_TEMPLATE
      echo "VSPHERE_NETWORK: Management"                                                  >> $TKG_TEMPLATE
      echo "VSPHERE_WORKER_NUM_CPUS: \"2\""                                               >> $TKG_TEMPLATE
      echo "VSPHERE_USERNAME: $VSPHERE_ADMIN"                                             >> $TKG_TEMPLATE
      echo "VSPHERE_FOLDER: /CoreDC/vm/Templates"                                         >> $TKG_TEMPLATE
      echo "SERVICE_CIDR: $TDH_TKGMC_SERVICE_CIDR"                                        >> $TKG_TEMPLATE
      echo "CLUSTER_CIDR: $TDH_TKGMC_CLUSTER_CIDR"                                        >> $TKG_TEMPLATE
      echo "VSPHERE_CONTROL_PLANE_NUM_CPUS: \"2\""                                        >> $TKG_TEMPLATE
      echo "VSPHERE_WORKER_DISK_GIB: \"20\""                                              >> $TKG_TEMPLATE
      echo "VSPHERE_SSH_AUTHORIZED_KEY: |-"                                               >> $TKG_TEMPLATE
      cat $VSPHERE_SSH_PUBLIC_KEY_FILE | sed 's/^/    /g'                                 >> $TKG_TEMPLATE
      echo "VSPHERE_SERVER: $TDH_TKGMC_VCENTER_SERVER"                                    >> $TKG_TEMPLATE
      echo "VSPHERE_DATASTORE: /$VSPHERE_DATACENTER/datastore/$VSPHERE_DATASTORE"         >> $TKG_TEMPLATE
      echo "VSPHERE_RESOURCE_POOL: /$VSPHERE_DATACENTER/host/$VSPHERE_CLUSTER/Resources"  >> $TKG_TEMPLATE
      echo "VSPHERE_CONTROL_PLANE_MEM_MIB: \"2048\""                                      >> $TKG_TEMPLATE
      echo "MACHINE_HEALTH_CHECK_ENABLED: \"true\""                                       >> $TKG_TEMPLATE
      echo "VSPHERE_DATACENTER: /$VSPHERE_DATACENTER"                                     >> $TKG_TEMPLATE

      [ -f $HOME/.kube/config ] && mv $HOME/.kube/config $HOME/.kube/config.old

      echo "----------------------------------------------------------------------------------------------------------------"
      echo "tkg init -q -i $MODE -p dev --vsphere-controlplane-endpoint-ip $VSPHERE_CONTROLPLANE_IP \\"
      echo "         --ceip-participation true --cni $TDH_TKGMC_CNI -v 0 \\"
      echo "         --name $TDH_TKGMC_NAME --config $TKG_TEMPLATE --kubeconfig $TKG_KUBECONFIG"
      echo "----------------------------------------------------------------------------------------------------------------"
      time tkg init -i $MODE -p dev --vsphere-controlplane-endpoint-ip $VSPHERE_CONTROLPLANE_IP --ceip-participation true \
           --cni $TDH_TKGMC_CNI -v 0 --name $TDH_TKGMC_NAME --config $TKG_TEMPLATE --kubeconfig $TKG_KUBECONFIG
      echo "----------------------------------------------------------------------------------------------------------------"

      cp $HOME/.kube/config $HOME/.tanzu-demo-hub/${TDH_TKGMC_NAME}.kubeconfig
      [ -f $HOME/.kube/config.old ] && mv $HOME/.kube/config.old $HOME/.kube/config

      # --- COPY TKG CONFIG ---
      $SCP_COMMAND $HOME/.tanzu-demo-hub/${TDH_TKGMC_CONFIG} $SSH_USER@$SSH_HOST:${SSH_HOME}/.tanzu-demo-hub > /dev/null 2>&1
      $SCP_COMMAND $HOME/.tanzu-demo-hub/${TDH_TKGMC_NAME}.kubeconfig $SSH_USER@$SSH_HOST:${SSH_HOME}/.tanzu-demo-hub > /dev/null 2>&1
      $SSH_COMMAND -n "echo \"y\" | tkg get mc --config \${HOME}/.tanzu-demo-hub/$TDH_TKGMC_CONFIG" > /dev/null 2>&1
      $SSH_COMMAND -n "sed -i 's+file: /Users/sdu/.kube-tkg/config+file: /home/ubuntu/.kube-tkg/config+g' ~/.tanzu-demo-hub/tkgmc-dev-vsphere-macbook.yaml"

      $SSH_COMMAND -n "[ ! -d \$HOME/.kube-tkg ] && mkdir -p \$HOME/.kube-tkg"
      $SCP_COMMAND $HOME/.kube-tkg/config $SSH_USER@$SSH_HOST:${SSH_HOME}/.kube-tkg/  > /dev/null 2>&1
      #tkg get credentials tdh-1 > /dev/null 2>&1
    else
      messageTitle "Verify TKG Managment Cluster"
      messagePrint " - Cluster Name"                "$TDH_TKGMC_NAME"
      messagePrint " - Configuration File"          "$TKG_TEMPLATE"
      messagePrint " - Control Plane Machine Type"  "$TDH_TKGMC_CONTROL_PLANE_MACHINE_TYPE"
      messagePrint " - Worker Node Machine Type"    "$TDH_TKGMC_MACHINE_TYPE"
      messagePrint " - Cluster CIDR"                "$TDH_TKGMC_CLUSTER_CIDR"
      messagePrint " - Service CIDR"                "$TDH_TKGMC_SERVICE_CIDR"
      messagePrint " - Health Check Enabled"        "$TDH_TKGMC_MACHINE_HEALTH_CHECK_ENABLED"
    fi
  fi
}

setTKGclusterDNS() {
  TKG_CLUSTER="$1"
  TKG_IPADRESS="$2"
  TKG_INGRESS="$3"
  DNS_PREFIX="$TDH_TKGMC_ENVNAME"
  DNS_SUFFIX="$AWS_HOSTED_DNS_DOMAIN"

  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "AWS" ]; then
    AWS_ID=$(echo $DNSLB | awk -F '-' '{ print $1 }')
    AWS_LB="k8s-master-$TKG_CLUSTER"
    AWS_SG=$(aws elb --region $AWS_REGION describe-load-balancers --load-balancer-names $AWS_ID | \
             jq -r '.LoadBalancerDescriptions[].SecurityGroups[]')
    AWS_VP=$(aws elb --region $AWS_REGION describe-load-balancers --load-balancer-names $AWS_ID | \
             jq -r '.LoadBalancerDescriptions[].VPCId')
  
    cnt=$(echo "${AWS_SG}" | egrep -c "$AWS_SG_NEW")
    if [ $cnt -eq 0 ]; then
      a=1
    fi
  fi
  
  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "GCP" ]; then
    DNS_NAME="*.${TKG_INGRESS}-${TKG_CLUSTER}"
    messagePrint "- Create DNS Entry for:" "*.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"
  
    cnt=$(gcloud dns record-sets list -z ${DNS_PREFIX}-zone --name "${DNS_NAME}.${DNS_PREFIX}.${DNS_SUFFIX}." \
            --type=A 2> /dev/null | grep -v "^NAME" | wc -l | sed 's/ //g')
    if [ ${cnt} -eq 0 ]; then
      messagePrint " - Creating DNS Entry in (${DNS_PREFIX}-zone)" "${DNS_NAME}"
      gcloud dns record-sets transaction abort -z ${DNS_PREFIX}-zone > /dev/null 2>&1
      gcloud dns record-sets transaction start -z ${DNS_PREFIX}-zone > /dev/null 2>&1
      gcloud dns record-sets transaction add "$DNSLB" --name "${DNS_NAME}.${DNS_PREFIX}.${DNS_SUFFIX}." \
         --type A -z ${DNS_PREFIX}-zone --ttl=300 > /dev/null 2>&1
      gcloud dns record-sets transaction execute -z ${DNS_PREFIX}-zone > /dev/null 2>&1; ret=$?
      if [ ${ret} -ne 0 ]; then
        echo "ERROR: Creating DNS record-sets for zone (${DNS_PREFIX}-zone)"
        echo "       => gcloud dns record-sets transaction execute -z ${DNS_PREFIX}-zone"
        exit 1
      fi
    fi
  fi
  
  if [ "${TDH_TKGMC_INFRASTRUCTURE}" == "Azure" -o "${TDH_TKGMC_INFRASTRUCTURE}" == "vSphere" ]; then
    DNS_NAME="*.${TKG_INGRESS}-${TKG_CLUSTER}"
    messagePrint "- LoadBalancer PublicIP:" "$TKG_IPADRESS"
    messagePrint "- Create DNS Entry for:" "*.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"

    ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${DNS_PREFIX}.${DNS_SUFFIX} | jq -r '.HostedZones[0].Id')
    ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')

    if [ "${ZONE_ID}" != "" ]; then
      ZONE="*.${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"
      ZONE="*.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"

      TMPROUTE53=/tmp/$$_tmp_route53.json
      echo "{"                                                            >  $TMPROUTE53
      echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","           >> $TMPROUTE53
      echo "  \"Changes\": [{"                                            >> $TMPROUTE53
      echo "    \"Action\": \"UPSERT\","                                  >> $TMPROUTE53
      echo "    \"ResourceRecordSet\": {"                                 >> $TMPROUTE53
      echo "      \"Name\": \"${ZONE}\","                                 >> $TMPROUTE53
      echo "      \"Type\": \"A\","                                       >> $TMPROUTE53
      echo "      \"TTL\": 300,"                                          >> $TMPROUTE53
      echo "      \"ResourceRecords\": [{ \"Value\": \"$TKG_IPADRESS\"}]" >> $TMPROUTE53
      echo "    }"                                                        >> $TMPROUTE53
      echo "  }]"                                                         >> $TMPROUTE53
      echo "}"                                                            >> $TMPROUTE53

      aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
          --change-batch file://${TMPROUTE53} > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "1ERROR: failed to set DNS for $hnm"
        echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
        echo "              --change-batch file://${TMPROUTE53}"
        cat $TMPROUTE53
        exit 1
      fi

      rm -f $TMPROUTE53
    fi
  fi

  if [ "${AWS_HOSTED_ZONE_ID}" != "" -a "${TDH_TKGMC_INFRASTRUCTURE}" == "AWS" ]; then
    ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')
    DNSLB=$(aws elb --region $AWS_REGION describe-load-balancers --load-balancer-names "$AWS_ID" | \
        jq -r '.LoadBalancerDescriptions[0].DNSName')
    DNSLB_ZONEID=$(aws elb --region $AWS_REGION describe-load-balancers --load-balancer-names "$AWS_ID" | \
        jq -r '.LoadBalancerDescriptions[0].CanonicalHostedZoneNameID')
    ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${DNS_PREFIX}.${DNS_SUFFIX} | jq -r '.HostedZones[0].Id')
  
echo "TDH_TKGMC_INFRASTRUCTURE:$TDH_TKGMC_INFRASTRUCTURE"
#jjjjjjjjjjjjjjjjjjj
    ALIAS=$(aws route53 list-resource-record-sets --hosted-zone-id $ZONE_ID \
            --query "ResourceRecordSets[?contains(Name, '${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}')].AliasTarget.DNSName" | \
            jq -r '.[]' | sed -e 's/dualstack\.//g' -e 's/\.$//g' )
  
    if [ "${ALIAS}" != "$DNSLB" ]; then
      if [ "${ZONE_ID}" != "" ]; then
        echo "Create DNS Entry for *.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}:"
        ZONE="*.${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"
        ZONE="*.${TKG_INGRESS}-${TKG_CLUSTER}.${DNS_PREFIX}.${DNS_SUFFIX}"
  
        TMPROUTE53=/tmp/$$_tmp_route53.json
        echo "{"                                                   >  $TMPROUTE53
        echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","  >> $TMPROUTE53
        echo "  \"Changes\": [{"                                   >> $TMPROUTE53
        echo "  \"Action\": \"UPSERT\","                           >> $TMPROUTE53
        echo "  \"ResourceRecordSet\": {"                          >> $TMPROUTE53
        echo "    \"Name\": \"${ZONE}\","                          >> $TMPROUTE53
        echo "    \"Type\": \"A\","                                >> $TMPROUTE53
        echo "    \"AliasTarget\": {"                              >> $TMPROUTE53
        echo "      \"HostedZoneId\": \"${DNSLB_ZONEID}\","        >> $TMPROUTE53
        echo "      \"DNSName\": \"dualstack.${DNSLB}.\","         >> $TMPROUTE53
        echo "      \"EvaluateTargetHealth\": true"                >> $TMPROUTE53
        echo "    }"                                               >> $TMPROUTE53
        echo "}}]"                                                 >> $TMPROUTE53
        echo "}"                                                   >> $TMPROUTE53
  
        aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
            --change-batch file://${TMPROUTE53} > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "2ERROR: failed to set DNS for $hnm"
          echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
          echo "              --change-batch file://${TMPROUTE53}"
          cat $TMPROUTE53
          exit 1
        fi
  
        rm -f $TMPROUTE53
      fi
    fi
  fi
}

verifyTLScertificate() {
  TLS_CERTIFICATE=$1
  TLS_PRIVATE_KEY=$2

  dif=$((openssl x509 -in $TLS_CERTIFICATE -noout -modulus; openssl rsa -in $TLS_PRIVATE_KEY -noout -modulus) | \
      uniq -c | awk '{ print $1 }')
  if [ "$dif" == "" ]; then dif=0; fi
  if [ $dif -ne 2 ]; then
    echo "ERROR: Certificate modulus of $TLS_CERTIFICATE does not match with the private_key $TLS_PRIVATE_KEY"
    echo "       => openssl x509 -in $TLS_CERTIFICATE -noout -modulus"
    echo "       => openssl rsa -in $TLS_PRIVATE_KEY -noout -modulus"
    exit 1
  fi

  if [ ! -f ${TDHPATH}/certificates/ca.pem ]; then
    # --- TRY TO FIND THE ROOT_CA ---
    if [ "${ROOT_CA_NAME}" == "DST Root CA X3" ]; then
      ROOT_CA_URL="https://letsencrypt.org/certs/trustid-x3-root.pem.txt"
      curl --output /tmp/trustid-x3-root.pem $ROOT_CA_URL > /dev/null 2>&1
      TLS_ROOT_CA=/tmp/trustid-x3-root.pem
      #eval export ${CLOUD}_${MODE}_TLS_ROOT_CA=/tmp/trustid-x3-root.pem
    fi

    # --- GENERATE CA.PEM WITH ROOT_CA AND FULLCHAIN ---
    cat $TLS_ROOT_CA $TLS_CERTIFICATE > ${TDHPATH}/certificates/ca.pem

    if [ "${TLS_ROOT_CA}" == "" ]; then
      echo "ERROR: ROOT_CA for $ROOT_CA_NAME could not be found, please download the file manually"
      echo "       and set the variable in the ~/.pcfconfig"
      echo "       => export ${CLOUD}_${MODE}_TLS_ROOT_CA=<ca.crt>"
      exit 1
    fi
  fi

}

verifyCertificate () {
  CLOUD="$1"
  TLS_CERTIFICATE="$2"
  TLS_FULLCHAIN="$3"
  TLS_PRIVATE_KEY="$4"
  TLS_CHAIN="$5"
  TLS_ROOT_CA="$6"


echo "CLOUD:$CLOUD"
echo "CLOUD:$CLOUD"
echo "CLOUD:$CLOUD"
echo "CLOUD:$CLOUD"

exit

  if [ "${TLS_CERTIFICATE}" == "" -o "${TLS_PRIVATE_KEY}" == "" -o \
       "${TLS_FULLCHAIN}" == "" ]; then

    echo ""
    echo "  3MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
    echo "  --------------------------------------------------------------------------------------------------------------"
    echo "  To allow TLS encrypted httpd traffic a Certificate and key needs to be created for your DNS Domain. Free"
    echo "  certificates can be optained through https://letsencrypt.org."
    echo ""

    if [ "${TLS_CERTIFICATE}" == "" ]; then
      echo "  ${CLOUD}_${MODE}_TLS_CERTIFICATE       (optional)  TLS Certificate (type PEM Certificate)"
    fi

    if [ "${TLS_FULLCHAIN}" == "" ]; then
      echo "  ${CLOUD}_${MODE}_TLS_FULLCHAIN         (optional)  TLS Fullchain (type PEM Certificate)"
    fi

    if [ "${TLS_PRIVATE_KEY}" == "" ]; then
      echo "  ${CLOUD}_${MODE}_TLS_PRIVATE_KEY       (optional)  TLS Private Key"
    fi

    if [ "${ROOT_CA}" == "" ]; then
      echo "  ${CLOUD}_${MODE}_TLS_ROOT_CA           (automatic) TLS Root CA"
    fi

    dom="$PCF_DEPLOYMENT_ENV_NAME.$AWS_HOSTED_DNS_DOMAIN"
    echo "                                  "
    echo "                                  $PCF_DEPLOYMENT_ENV_NAME.$AWS_HOSTED_DNS_DOMAIN"
    echo "                                    |      |_________ represented by the PCF_ENVIRONMENT_NAME variable"
    echo "                                    |________________ represented by the AWS_HOSTED_DNS_DOMAIN variable"
    echo ""
    echo "                                  The certificate Should include the following domains:"
    echo "                                  - PKS Services (API, OpsMan, Harbor) .: *.${dom}"
    echo "                                  - PKS Cluster and Applications .......: *.apps.cl1.${dom}"
    echo "                                                                          *.apps.cl2.${dom}"
    echo "                                                                          *.apps.cl3.${dom}"
    echo ""
  else
    messageTitle "TLS Encryption for domain (${PCF_DEPLOYMENT_ENV_NAME}.${AWS_HOSTED_DNS_DOMAIN})"

    for n in $TLS_CERTIFICATE $TLS_FULLCHAIN; do
      cnt=$(grep -c "BEGIN CERTIFICATE" $n)
      if [ $cnt -eq 0 ]; then
        echo "ERROR: The file ($n) is not in (Base64) format"
        echo "       It should begin with '-----BEGIN CERTIFICATE-----' and end with '-----END CERTIFICATE-----'"
        exit 1
      fi
    done

    for n in $TLS_PRIVATE_KEY; do
      cnt=$(egrep -c "BEGIN RSA PRIVATE|BEGIN PRIVATE" $n)
      if [ $cnt -eq 0 ]; then
        echo "ERROR: The file ($n) is not (private-key Base64) format"
        echo "       It should begin with '-----BEGIN RSA PRIVATE' and end with '-----END RSA PRIVATE'"
        exit 1
      fi
    done

    # --- VERIFY THE CERTIFICATE ---
    cnm="${PCF_DEPLOYMENT_ENV_NAME}.${AWS_HOSTED_DNS_DOMAIN}"
    cnt=$(openssl crl2pkcs7 -nocrl -certfile $TLS_FULLCHAIN | openssl pkcs7 -print_certs | egrep "^subject|^issuer" | \
          sed -e 's/CN = /CN=/g' -e 's/^subject=.*CN=//g' -e 's/^issuer=.*CN=//g' | egrep -c "^\*.${cnm}")
    if [ $cnt -eq 0 ]; then
      echo "ERROR: The file ($(basename $TLS_FULLCHAIN)) does not contain the CN=*.${cnm}"
      exit 1
    fi

    ISSUER="*.${cnm}"; item_found=1
    while [ ${item_found} -eq 1 ]; do
      item_found=0
      for item in $(openssl crl2pkcs7 -nocrl -certfile $TLS_FULLCHAIN | openssl pkcs7 -print_certs | \
                    egrep "^subject|^issuer" | sed -e 's/CN = /CN=/g' -e 's/^subject=.*CN=//g' \
                    -e 's/^issuer=.*CN=//g' -e 's/ /~1~/g' | paste -d ':' - -); do

        sub=$(echo "${item}" | awk -F: '{ print $1 }')
        iss=$(echo "${item}" | awk -F: '{ print $2 }')

        if [ "${sub}" == "${ISSUER}" ]; then ISSUER="${iss}"; SUBJECT="${sub}"; item_found=1; fi
      done
    done

    ROOT_CA=$ISSUER
    ROOT_CA_NAME=$(echo $ISSUER | sed 's/~1~/ /g')

    if [ "${TLS_ROOT_CA}" == "" ]; then
      # --- TRY TO FIND THE ROOT_CA ---
      if [ "${ROOT_CA_NAME}" == "DST Root CA X3" ]; then
        ROOT_CA_URL="https://letsencrypt.org/certs/trustid-x3-root.pem.txt"
        curl --output /tmp/trustid-x3-root.pem $ROOT_CA_URL > /dev/null 2>&1
        TLS_ROOT_CA=/tmp/trustid-x3-root.pem
        eval export ${CLOUD}_${MODE}_TLS_ROOT_CA=/tmp/trustid-x3-root.pem
      fi

      # --- GENERATE CA.PEM WITH ROOT_CA AND FULLCHAIN ---
      cat $TLS_ROOT_CA $TLS_FULLCHAIN > $HOME/pcfconfig/certificates/ca.pem

      if [ "${TLS_ROOT_CA}" == "" ]; then
        echo "ERROR: ROOT_CA for $ROOT_CA_NAME could not be found, please download the file manually"
        echo "       and set the variable in the ~/.pcfconfig"
        echo "       => export ${CLOUD}_${MODE}_TLS_ROOT_CA=<ca.crt>"
        exit 1
      fi
    fi

    # --- VERIFY TO ROOT_CA ---
    for n in $TLS_ROOT_CA; do
      if [ ! -f $n ]; then
        echo "ERROR: the file $n does not exist"; exit 1
      fi

      cnt=$(grep -c "BEGIN CERTIFICATE" $n)
      if [ $cnt -eq 0 ]; then
        echo "ERROR: The file ($(basename $n)) is not in (Base64) format"
        echo "       It should begin with '-----BEGIN CERTIFICATE-----' and end with '-----END CERTIFICATE-----'"
        exit 1
      fi
    done

    # --- VERIFY THE CERTIFICATE ISSUER AND SUBJECT ---
    tmp=$(openssl crl2pkcs7 -nocrl -certfile $TLS_ROOT_CA | openssl pkcs7 -print_certs | \
          egrep "^subject|^issuer" | sed -e 's/CN = /CN=/g' -e 's/^subject=.*CN=//g' -e 's/^issuer=.*CN=//g' -e 's/ /~1~/g' | \
          paste -d ':' - -)
    sub=$(echo "${tmp}" | awk -F: '{ print $1 }' | sed 's/~1~/ /g')
    iss=$(echo "${tmp}" | awk -F: '{ print $2 }' | sed 's/~1~/ /g')
    if [ "${iss}" != "${sub}" ]; then
      echo "ERROR: $TLS_ROOT_CA is not a Root CA. Issuer and Subject should be the same"
      echo "       => File: $TLS_ROOT_CA Issuer=$sub / Subject=$iss"
      exit 1
    fi

    if [ "${iss}" != "${ROOT_CA_NAME}" ]; then
      echo "ERROR: $TLS_ROOT_CA is not Signed by $ROOT_CA_NAME."
      echo "       => File: $TLS_ROOT_CA Subject=$iss"
      exit 1
    fi

    # --- VERIFY CERTITICATE AND FULLCHAAIN SIGNED BY ROOT_CA ---
    cat $TLS_ROOT_CA $TLS_FULLCHAIN > /tmp/ca.crt
    openssl verify -CAfile /tmp/ca.crt $TLS_CERTIFICATE > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Certificate ($TLS_CERTIFICATE) was not signed by the Roor CA ($ROOT_CA_NAME)"
      exit 1
    fi

    openssl verify -CAfile /tmp/ca.crt $TLS_FULLCHAIN > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Certificate ($TLS_FULLCHAIN) was not signed by the Roor CA ($ROOT_CA_NAME)"
      exit 1
    fi

   # --- VERIFY IF PRIVATE KEY MATCH THE CERTIFICATE ---
    pk=$(openssl rsa  -noout -modulus -in $TLS_PRIVATE_KEY | openssl md5)
    cr=$(openssl x509 -noout -modulus -in $TLS_CERTIFICATE | openssl md5)
    fc=$(openssl x509 -noout -modulus -in $TLS_FULLCHAIN | openssl md5)

    if [ "${pk}" != "${cr}" ]; then
      echo "ERROR: Cert ($(basename $TLS_CERTIFICATE)) does not match the private key ($(basename $TLS_PRIVATE_KEY))"
      echo "       => openssl rsa -noout -modulus -in $TLS_PRIVATE_KEY | openssl md5"
      echo "       => openssl x509 -noout -modulus -in $TLS_CERTIFICATE | openssl md5"
      exit 1
    fi

    if [ "${pk}" != "${fc}" ]; then
      echo "ERROR: Cert ($(basename $TLS_FULLCHAIN)) does not match the private key ($(basename $TLS_PRIVATE_KEY))"
      echo "       => openssl rsa -noout -modulus -in $TLS_PRIVATE_KEY | openssl md5"
      echo "       => openssl x509 -noout -modulus -in $TLS_FULLCHAIN | openssl md5"
      exit 1
    fi

    messagePrint " - TLS Certificate"                  "$TLS_CERTIFICATE"
    messagePrint " - TLS fullchain"                    "$TLS_FULLCHAIN"
    messagePrint " - TLS Private Key"                  "$TLS_PRIVATE_KEY"
    messagePrint " - TLS Root CA ($ROOT_CA_NAME)"      "$TLS_ROOT_CA"
  fi
}

setDNSalias() {
  TKG_IPADRESS="$1"
  DNS_PREFIX="$2"
  DNS_SUFFIX="$3"
  HOSTNAME="$4"
  TYPE="$5"

  AWS_LB_NAME=$(aws elb --region $AWS_REGION describe-load-balancers | \
                jq -r --arg n $TKG_IPADRESS '.LoadBalancerDescriptions[] | select(.DNSName == $n).LoadBalancerName')
  AWS_LB_ZONE=$(aws elb --region $AWS_REGION describe-load-balancers | \
                jq -r --arg n $TKG_IPADRESS '.LoadBalancerDescriptions[] | select(.DNSName == $n).CanonicalHostedZoneNameID')

  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${DNS_PREFIX}.${DNS_SUFFIX} | jq -r '.HostedZones[0].Id')
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')

  if [ "${ZONE_ID}" != "" ]; then
    ZONE="${HOSTNAME}.${DNS_PREFIX}.${DNS_SUFFIX}"

    TMPROUTE53=/tmp/$$_tmp_route53.json
    echo "{"                                                   >  $TMPROUTE53
    echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","  >> $TMPROUTE53
    echo "  \"Changes\": [{"                                   >> $TMPROUTE53
    echo "  \"Action\": \"UPSERT\","                           >> $TMPROUTE53
    echo "  \"ResourceRecordSet\": {"                          >> $TMPROUTE53
    echo "    \"Name\": \"${ZONE}\","                          >> $TMPROUTE53
    echo "    \"Type\": \"A\","                                >> $TMPROUTE53
    echo "    \"AliasTarget\": {"                              >> $TMPROUTE53
    echo "      \"HostedZoneId\": \"${AWS_LB_ZONE}\","         >> $TMPROUTE53
    echo "      \"DNSName\": \"dualstack.${TKG_IPADRESS}.\","  >> $TMPROUTE53
    echo "      \"EvaluateTargetHealth\": true"                >> $TMPROUTE53
    echo "    }"                                               >> $TMPROUTE53
    echo "}}]"                                                 >> $TMPROUTE53
    echo "}"                                                   >> $TMPROUTE53

    aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
        --change-batch file://${TMPROUTE53} > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "2ERROR: failed to set DNS for $hnm"
      echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
      echo "              --change-batch file://${TMPROUTE53}"
      cat $TMPROUTE53
      exit 1
    fi

    rm -f $TMPROUTE53
  fi
}



setDNSrecord() {
  TKG_IPADRESS="$1"
  DNS_PREFIX="$2"
  DNS_SUFFIX="$3"
  HOSTNAME="$4"
  TYPE="$5"

  if [ "$TYPE" == "" ]; then TYPE=A; fi
  
  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${DNS_PREFIX}.${DNS_SUFFIX} | jq -r '.HostedZones[0].Id')
  ZONE_ID_STR=$(echo "${ZONE_ID}" | awk -F'/' '{ print $NF }')

  if [ "${ZONE_ID}" != "" ]; then
    ZONE="${HOSTNAME}.${DNS_PREFIX}.${DNS_SUFFIX}"

    TMPROUTE53=/tmp/$$_tmp_route53.json
    echo "{"                                                            >  $TMPROUTE53
    echo "  \"Comment\": \"CREATE/DELETE/UPSERT a record \","           >> $TMPROUTE53
    echo "  \"Changes\": [{"                                            >> $TMPROUTE53
    echo "    \"Action\": \"UPSERT\","                                  >> $TMPROUTE53
    echo "    \"ResourceRecordSet\": {"                                 >> $TMPROUTE53
    echo "      \"Name\": \"${ZONE}\","                                 >> $TMPROUTE53
    echo "      \"Type\": \"$TYPE\","                                   >> $TMPROUTE53
    echo "      \"TTL\": 300,"                                          >> $TMPROUTE53
    echo "      \"ResourceRecords\": [{ \"Value\": \"$TKG_IPADRESS\"}]" >> $TMPROUTE53
    echo "    }"                                                        >> $TMPROUTE53
    echo "  }]"                                                         >> $TMPROUTE53
    echo "}"                                                            >> $TMPROUTE53

    aws route53 change-resource-record-sets --hosted-zone-id $ZONE_ID \
        --change-batch file://${TMPROUTE53} > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "3ERROR: failed to set DNS for $hnm"
      echo "       => aws route53 change-resource-record-sets --hosted-zone-id \"${ZONE_ID}\" \\"
      echo "              --change-batch file://${TMPROUTE53}"
      cat $TMPROUTE53
      exit 1
    fi

    rm -f $TMPROUTE53
  fi
}

listDeployments() {
  printf "%-32s %-8s  %-15s %-20s %-5s %s\n" "DEPLOYMENT" "PLATFORM" "PROFILE" "MGMT-CLUSTER" "PLAN" "CONFIGURATION"
  echo "----------------------------------------------------------------------------------------------------------------"
  for deployment in $(ls -1 ${TDHPATH}/deployments/minikube*.cfg) ; do
    mcr=$(egrep "^TDH_MINIKUBE_PROFILE=" $deployment | awk -F= '{ print $NF }') 

    dep=$(basename $deployment)

    printf "%-32s %-8s  %-15s %-20s %-5s %s\n" $dep minikube $mcr "n/a" "n/a" \
           "$TDH_TKGMC_CONFIG"
  done

  echo 
  printf "%-32s %-8s  %-15s %-20s %-5s %s\n" "DEPLOYMENT" "CLOUD" "REGION" "MGMT-CLUSTER" "PLAN" "CONFIGURATION"
  echo "----------------------------------------------------------------------------------------------------------------"
  for deployment in $(ls -1 ${TDHPATH}/deployments/tkgmc*.cfg) ; do

    mci=$(egrep "^TDH_TKGMC_INFRASTRUCTURE=" $deployment | awk -F= '{ print $NF }') 
    reg=$(egrep "^TDH_TKGMC_REGION=" $deployment | awk -F= '{ print $NF }') 
    mcn=$(egrep "^TDH_TKGMC_NAME=" $deployment | awk -F= '{ print $NF }') 
    mcp=$(egrep "^TDH_TKGMC_PLAN=" $deployment | awk -F= '{ print $NF }') 
    mcc=$(egrep "^TDH_TKGMC_CONFIG=" $deployment | awk -F= '{ print $NF }') 

    dep=$(basename $deployment)

    printf "%-32s %-8s  %-15s %-20s %-5s %s\n" $dep $mci $reg $mcn \
           $mcp "$mcc"

  done

  echo "----------------------------------------------------------------------------------------------------------------"
}


VerifyDemoEnvironment() {
  export TDH_PLATFORM=unknown
  if [ "${TKG_DEPLOYMENT}" != "" ]; then
    if [ -f ${TDHPATH}/deployments/$TKG_DEPLOYMENT ]; then
      . ${TDHPATH}/deployments/$TKG_DEPLOYMENT
    
      DOMAIN="apps-${TDH_TKGWC_NAME}.${TDH_TKGMC_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
    else
      echo "ERROR: can not find ${TDHPATH}/deployments/$TKG_DEPLOYMENT"; exit
    fi

    if [ "$TDH_MINIKUBE_PROFILE" != "" ]; then 
      cpu=$(cat ~/.minikube/profiles/tanzu-demo-hub/config.json | jq -r '.CPUs')
      mem=$(cat ~/.minikube/profiles/tanzu-demo-hub/config.json | jq -r '.Memory')
      drv=$(cat ~/.minikube/profiles/tanzu-demo-hub/config.json | jq -r '.Driver')
      export TDH_PLATFORM=minikube
      messageTitle "Demo Environment"
      messagePrint " - Deployment Config:"         "$TKG_DEPLOYMENT"
      messagePrint " - Kubernetes Infrastucture"   "minikube"
      messagePrint " - Minikube Profile"           "$TDH_MINIKUBE_PROFILE"
      messagePrint " - Minikube Domain"            "$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
      messagePrint " - Minikube Memory"            "$mem"
      messagePrint " - Minikube CPU's"             "$cpu"
      messagePrint " - Minikube Driver"            "$drv"
    else
      export TDH_PLATFORM=tkgm
      export WORKLOAD_CLUSTER=tkg-tanzu-demo-hub.cfg
      export TDH_TKGWC_NAME=tdh-1
      . ${TDHPATH}/deployments/$WORKLOAD_CLUSTER

      messageTitle "TKG Demo Environment"
      messagePrint " - Deployment Config:"         "$TKG_DEPLOYMENT"
      messagePrint " - TKG Infrastructure"         "$TDH_TKGMC_INFRASTRUCTURE"
      messagePrint " - TKG Management Cluster"     "$TDH_TKGMC_NAME"
      messagePrint " - TKG Workload Cluster"       "$TDH_TKGWC_NAME"
      messagePrint " - DNS Domain"                 "$TDH_TKGMC_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
      messagePrint " - Kubernetes Context:"        "$TDH_TKGWC_NAME-admin@$TDH_TKGWC_NAME"
      echo ""

      if [ "${TKG_CONFIG}" == "" ]; then
        echo "ERROR: environment variable TKG_CONFIG has not been set"; exit
      fi

      if [ "${KUBECONFIG}" == "" ]; then
        echo "ERROR: environment variable KUBECONFIG has not been set"; exit
      fi
    fi

    echo -n "Is the environment setup correct ? (y/n): "; read x
    answer_provided="n"
    while [ "${answer_provided}" == "n" ]; do
      if [ "${x}" == "y" -o "${x}" == "Y" ]; then break; fi
      if [ "${x}" == "n" -o "${x}" == "N" ]; then 
        listDeployments
        echo ""
        echo "Please choose another deployment option:"
        echo "       => export TKG_DEPLOYMENT=<DEPLOYMENT>"; exit
      fi
      echo -n "Is the environment setup correct ? (y/n): "; read x
    done
  else
    listDeployments
    echo ""
    echo "ERROR: environment variable TKG_DEPLOYMENT not set. Pleae choose one from the list"
    echo "       => export TKG_DEPLOYMENT=<DEPLOYMENT>"; exit
  fi
}

VerifyTKGDemoPlatform() {
echo tkgm
}

VerifyK8SDemoPlatform() {
  if [ "${TDH_INFRASTRUCTURE}" == "minikube" ]; then 
    messageTitle "MiniKube Verify Addons"

    minikube profile $TDH_MINIKUBE_PROFILE > /dev/null 2>&1
    minikube status -p $TDH_MINIKUBE_PROFILE -o json > /dev/null 2>&1
    if [ "$?" -ne 0 ]; then
      echo "ERROR: minikube is not running, please start it by:"
      echo "       => minikube start -p $TDH_MINIKUBE_PROFILE"; exit
    fi

    ctx=$(kubectl config current-context)
    if [ "$ctx" != "$TDH_MINIKUBE_PROFILE" ]; then
      echo "ERROR: Kubernetes context is currently set to $crx. Please set it by:"
      echo "       => kubectl config use-context $TDH_MINIKUBE_PROFILE"; exit
    fi

    # --- CHECK FOR ADDONS ---
    for tmp in default-storageclass:default-storageclass metrics-server:metrics-server metallb-system:metallb; do
      val=$(echo $tmp | awk -F: '{ print $1 }') 
      key=$(echo $tmp | awk -F: '{ print $2 }') 
      stt=$(minikube addons list -o json | jq -r ".\"$key\".Status")
      
      if [ "$stt" != "enabled" ]; then
        messagePrint " - Minikube Addon: $val" "$stt, enabling now"
        minikube addons enable $key -p $TDH_MINIKUBE_PROFILE > /dev/null 2>&1
        if [ $? -ne 0 ]; then 
           echo "ERROR: enabling Minikube Addob: $val"
           echo "       => minikube addons enable $key -p $TDH_MINIKUBE_PROFILE "
           exit
        fi
      else
        messagePrint " - Minikube Addon: $val" "$stt"
      fi
    done
  fi
}



VerifyServiceDependancies() {
  if [ "$TDH_DEPENDANCY_CERT_MANAGER" == "true" ]; then 
    INGRESS_NAMESPACE=cert-manager
    HELM_CHART=jetstack/cert-manager

    helm repo add bitnami https://charts.bitnami.com/bitnami > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to add helm registry https://charts.bitnami.com/bitnami"; exit
    fi

    messagePrint "- Install Cert Manager" "bitnami/nginx-ingress-controller"
    kubectl get namespace $INGRESS_NAMESPACE > /dev/null 2>&1
    if [ $? -ne 0 ]; then kubectl create namespace $INGRESS_NAMESPACE > /dev/null 2>&1; fi

    stt=$(helm list -n $INGRESS_NAMESPACE -o json | jq -r --arg key "cert-manager-" '.[] | select(.chart | contains($key)).status')
    if [ "$stt" != "deployed" ]; then
      helm install $HELM_CHART -n $INGRESS_NAMESPACE > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: Failed to install $HELM_CHART"
        echo "       => helm install stable $HELM_CHART -n $INGRESS_NAMESPACE"
      fi
    fi

  fi

  if [ "$TDH_DEPENDANCY_NGINX" == "true" ]; then 
    INGRESS_NAMESPACE=ingress-nginx
    HELM_CHART=bitnami/nginx-ingress-controller

    helm repo add bitnami https://charts.bitnami.com/bitnami > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to add helm registry https://charts.bitnami.com/bitnami"; exit
    fi

    messagePrint "- Install Ingress Controller" "$HELM_CHART"
    kubectl get namespace $INGRESS_NAMESPACE > /dev/null 2>&1
    if [ $? -ne 0 ]; then kubectl create namespace $INGRESS_NAMESPACE > /dev/null 2>&1; fi

    stt=$(helm list -n nginx-ingress -o json | jq -r --arg key "ngress-controller" '.[] | select(.chart | contains($key)).status')
    if [ "$stt" != "deployed" ]; then
      helm install bitnami bitnami/nginx-ingress-controller -n $INGRESS_NAMESPACE > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: Failed to install $HELM_CHART"
        echo "       => helm install stable $HELM_CHART"
      fi
    fi

    # --- SET CLUSTER DNS ---
    ipa=$(kubectl get svc bitnami-nginx-ingress-controller -n nginx-ingress -o json 2>/dev/null | \
          jq -r '.status.loadBalancer.ingress[].ip' 2>/dev/null)
    while [ "${ipa}" == "" ]; do
      sleep 10
      ipa=$(kubectl get svc bitnami-nginx-ingress-controller -n nginx-ingress -o json 2>/dev/null | \
            jq -r '.status.loadBalancer.ingress[].ip' 2>/dev/null)
    done
  
    if [ "${ipa}" != "" ]; then
      setTKGclusterDNS "$TDH_TKGWC_NAME" "${ipa}" "nginx"
    else
      echo "ERROR: Unable to get LoadBalancer IP-Adress of the Ingress"
      echo "       => kubectl get svc envoy -n tanzu-system-ingress"
      exit 1
    fi
  fi

  if [ "$TDH_DEPENDANCY_CONTOUR" == "true" ]; then 
    INGRESS_NAMESPACE=ingress-contour
    HELM_CHART=bitnami/contour

    stt=$(kubectl get ns -o json | jq -r --arg key "$INGRESS_NAMESPACE" '.items[] | select(.metadata.name == $key).status.phase')
    if [ "${stt}" == "" ]; then
      kubectl create ns $INGRESS_NAMESPACE > /dev/null 2>&1
    fi

    helm repo add bitnami https://charts.bitnami.com/bitnami > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to add helm registry https://charts.bitnami.com/bitnami"; exit
    fi

    stt=$(helm list -n $INGRESS_NAMESPACE -o json | jq -r --arg key "contour-3" '.[] | select(.chart | contains($key)).status')
    if [ "$stt" != "deployed" ]; then
      messageTitle "Install Contour Ingress Controller" "$HELM_CHART"
      helm install bitnami $HELM_CHART -n $INGRESS_NAMESPACE > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: Failed to install $HELM_CHART"
        echo "       => helm install stable $HELM_CHART"
      fi
    fi
  fi
}

verifyKubernetesContext() {
  echo "----------------------------------------------------------------------------------------------------------------"
  kubectl config get-contexts
  echo "----------------------------------------------------------------------------------------------------------------"

  ret=""
  while [ "$ret" != "y" -a "$ret" != "n" ]; do
    echo -e "Is the Kubernetes Context correct ? <y/n>: \c"; read ret
  done
  echo ""

  if [ "$ret" == "n" ]; then exit; fi
}

uodateConfigMap() {
  CONFIG_MAP="$1"
  KEY="$2"
  VALUE="$3"

  [ "$VALUE" == "" ] && VALUE="<empty>"
  [ "$VALUE" == "True" ] && VALUE=true
  [ "$VALUE" == "False" ] && VALUE=false
  
  kubectl get cm tanzu-demo-hub > /dev/null 2>&1
  [ $? -ne 0 ] && kubectl create cm tanzu-demo-hub > /dev/null 2>&1

  kubectl get cm tanzu-demo-hub -o json | jq -r '.data' | egrep ": " | sed -e 's/^ *"//g' -e 's/: /=/g' -e 's/"=/=/g' -e 's/\\\"//g' -e 's/,$//g' -e 's/"//g' | \
          egrep -v "^$KEY=" | sort -u > /tmp/$CONFIG_MAP
  echo "$KEY=$VALUE" >> /tmp/$CONFIG_MAP

  kubectl create configmap $CONFIG_MAP --from-env-file=/tmp/$CONFIG_MAP --dry-run=client -o yaml | kubectl apply -f - > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "ERROR: updating config map $CONFIG_MAP"
    echo "       => kubectl create configmap $CONFIG_MAP --from-env-file=/tmp/$CONFIG_MAP --dry-run=client -o yaml | kubectl apply -f -"
    exit
  fi  
}

getConfigMap() {
  CONFIG_MAP=$1
  KEY=$2

  kubectl get configmap tanzu-demo-hub -o json | jq -r ".data.$KEY" | sed 's/"//g'
}

createConfigMap() {
  kubectl delete configmap tanzu-demo-hub > /dev/null 2>&1
  kubectl create configmap tanzu-demo-hub \
       --from-literal=TDH_ENVNAME=$TDH_ENVNAME \
       --from-literal=TDH_DOMAIN=$AWS_HOSTED_DNS_DOMAIN \
       --from-literal=TDH_LB_CONTOUR=apps-contour \
       --from-literal=TDH_LB_NGINX=apps-nginx 
}

verifyRequiredServices() {
  flag=$1
  desc="$2"

  stt=$(getConfigMap tanzu-demo-hub $flag)
  if [ "$stt" != "true" ]; then 
    echo "ERROR: Service $desc requires to be enabled and installed"
    echo "       Please set $flag=true in the deployment file"
    exit
  fi
}

checkTKGdownloads() {
  kpl=$(ls -1 software | egrep -c "^kp-linux|photon")
  ptl=$(ls -1 software | egrep -c "^photon")
  tkg=$(ls -1 software | egrep -c "^tkg-linux")
  if [ $tkg -eq 0 ]; then 
    echo "ERROR: Please download TKG Utilies from https://www.vmware.com/go/get-tkg"
    echo "       => $TANZU_DEMO_HUB/software/tkg-linux-amd64-vx.x.x-vmware.x.tar.gz"
    exit
  fi
}

checkCLIcommands() {
  cat="$1"
  if [ "$cat" == "TKG" ]; then
    if [ ! -x /usr/local/bin/tkg ]; then
      echo "ERROR: /usr/local/bin/tkg not installed, please download package from https://www.vmware.com/go/get-tkg"
      echo "       => search for VMware Tanzu Kubernetes Grid CLI for Mac"
      exit
    fi

    if [ ! -x /usr/local/bin/kp ]; then
      echo "ERROR: /usr/local/bin/kp not installed, please download package from https://network.pivotal.io/products/build-service/"
      echo "       => search for kp-darwin for Mac"
      exit
    fi
  fi

  if [ "$cat" == "TOOLS" ]; then
    CLI_COMMANDS="/usr/local/bin/jq:brew:jq /usr/local/bin/wget:brew:wget /usr/local/bin/mc:brew:minio/stable/mc /usr/local/bin/helm:brew:helm"

    for arg in $CLI_COMMANDS; do
      cmd=$(echo $arg | awk -F: '{ print $1 }') 
      sys=$(echo $arg | awk -F: '{ print $2 }') 
      pkg=$(echo $arg | awk -F: '{ print $3 }') 
  
      if [ $sys == "brew" ]; then
        if [ ! -x $cmd ]; then
          echo "ERROR: $cmd not installed, please install with brew"
          echo "       => brew install $pkg"
          exit
        fi
      fi
  
    done
  fi
}

checkTMCAccess() {
  missing_variables=0

  if [ "${TDH_DEPLOYMENT_CLOUD}" == "AWS" ]; then
    TMC_ACCOUNT_NAME=$TMC_ACCOUNT_NAME_AWS

    if [ "${TMC_MANAGEMENT_CLUSTER}" == "" -o "${TMC_PROVISONER_NAME}" == "" -o "${TMC_ACCOUNT_NAME_AWS}" == "" -o "${TMC_CLUSTER_GROUP}" == "" -o "${TMC_USER}" == "" -o "${TMC_KUBERNETES_VERSION}" == "" ]; then
      missing_variables=1
      echo ""
      echo "  2MISSING ENVIRONMENT-VARIABES    DESCRIPTION        "
      echo "  --------------------------------------------------------------------------------------------------------------"

      if [ "${TMC_KUBERNETES_VERSION}" == "" ]; then
        echo "  TMC_KUBERNETES_VERSION          (required) The Kubernetes version for the cluster e.g. 1.19.1-3-amazon2"
      fi

      if [ "${TMC_USER}" == "" ]; then
        echo "  TMC_USER                        (required) The name of the TMC UserID"
      fi

      if [ "${TMC_CLUSTER_GROUP}" == "" ]; then
        echo "  TMC_CLUSTER_GROUP               (required) The name of the TMC Cluster Group"
      fi

      if [ "${TMC_ACCOUNT_NAME_AWS}" == "" ]; then
        echo "  TMC_ACCOUNT_NAME_AWS            (required) The name of the TMC Account Name"
      fi

      if [ "${TMC_MANAGEMENT_CLUSTER}" == "" ]; then
        echo "  TMC_MANAGEMENT_CLUSTER          (required) The name of the TKGm Management Cluster"
      fi

      if [ "${TMC_PROVISONER_NAME}" == "" ]; then
        echo "  TMC_PROVISONER_NAME             (required) The TMC Provisioner name"
      fi

      if [ "${TMC_SSH_KEY_NAME_AWS}" == "" ]; then
        echo "  TMC_SSH_KEY_NAME_AWS            (required) AWS SSH Key name for Region"
      fi
      TMC_CLUSTER_NAME="tdh-aws-${TMC_USER}"
    else
      TMC_CLUSTER_NAME="tdh-aws-${TMC_USER}"

      messageTitle "Tanzu Mission Control (TMC) - Config" 
      messagePrint " - TMC User"                        "$TMC_USER"
      messagePrint " - TMC Cluster Group"               "$TMC_CLUSTER_GROUP"
      messagePrint " - TMC Cluster Name"                "$TMC_CLUSTER_NAME"
      messagePrint " - TMC Context Name"                "$TMC_CONTEXT_NAME"
      messagePrint " - TMC AWS Account Name"            "$TMC_ACCOUNT_NAME_AWS"
      messagePrint " - TMC Management Cluster"          "$TMC_MANAGEMENT_CLUSTER"
      messagePrint " - TMC Provisioner Name"            "$TMC_PROVISONER_NAME"

      # --- CHECK IF AWS CLI IS CONFIGURED ---
      if [ ! -d ~/.aws -o -d ~/.aws/credentials ]; then
        echo "ERROR: AWS CLI is not configured yet, please run aws configure"
        echo "       => aws configure"
        exit 1
      fi
    fi


   tmc clustergroup create -n tanzu-demo-hub -d "Tanzu Demo Hub by Sacha Dubois" > /dev/null 2>&1

  fi

  if [ ${missing_variables} -eq 1 ]; then
    echo "  --------------------------------------------------------------------------------------------------------------"
    echo "  IMPORTANT: Please set the missing environment variables either in your shell or in the pcfconfig"
    echo "             configuration file ~/.pcfconfig and set all variables with the 'export' notation"
    echo "             ie. => export AZURE_PKS_TLS_CERTIFICATE=/home/demouser/certs/cert.pem"
    echo "  --------------------------------------------------------------------------------------------------------------"
    exit 1
  fi
}

checkTMCcontext() {
  tmc_context=$(tmc system context current -o json 2>/dev/null | jq -r '.full_name.name')
  if [ "$tmc_context" == "" -o "$tmc_context" != "$TMC_CONTEXT_NAME" ]; then 
    tmc system context use $TMC_CONTEXT_NAME > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Can not set TMC Context to $TMC_CONTEXT_NAME"
      echo "       => tmc system context use $TMC_CONTEXT_NAME"
      exit 1
    fi 
  fi 
}

tmcCreateCluster() {
  # steve: missing provisioner, returned sacha's cluster
  stt=$(tmc cluster list -m $TMC_MANAGEMENT_CLUSTER -p $TMC_PROVISIONER_NAME --name $TMC_CLUSTER_NAME -o json 2>/dev/null | jq -r '.clusters[].status.phase' 2>/dev/null) 

  if [ "$stt" == "DELETING" ]; then                                         
     echo "ERROR: Cluster is currently on deleting, please try again later"; exit 1
  fi

  if [ "$stt" == "" -o "$stt" != "READY" ]; then
    messageTitle "TMC Create Cluster"     
    messagePrint " - TMC Cluster Name"       "$TMC_CLUSTER_NAME"
    messagePrint " - TMC Cluster Group"      "$TMC_CLUSTER_GROUP"
    messagePrint " - TMC Template"           "$TMC_TKGWC_TEMPLATE"
    messagePrint " - Provisioner Name"       "$TMC_PROVISONER_NAME"
    messagePrint " - TKG Management Cluster" "$TMC_MANAGEMENT_CLUSTER"
    messagePrint " - Account Name"           "$TMC_ACCOUNT_NAME"
    messagePrint " - Kubernetes Version"     "$TMC_KUBERNETES_VERSION"

    tmc cluster create -m $TMC_MANAGEMENT_CLUSTER -p $TMC_PROVISONER_NAME --ssh-key-name $TMC_SSH_KEY_NAME_AWS \
                       -c $TMC_ACCOUNT_NAME -r $AWS_REGION -n $TMC_CLUSTER_NAME -q $TMC_TKGWC_WORKERNODES \
                       -g $TMC_CLUSTER_GROUP \
                       -t $TMC_TKGWC_TEMPLATE \
                       --version $TMC_KUBERNETES_VERSION \
                       --availability-zone ${AWS_REGION}a

    messagePrint " - Cluster Status"         "creating ...."
    stt=""
    while [ "$stt" != "READY" ]; do
      stt=$(tmc cluster list -m $TMC_MANAGEMENT_CLUSTER --name $TMC_CLUSTER_NAME -o json 2>/dev/null | \
            jq -r '.clusters[].status.phase' 2>/dev/null | head -1)

      sleep 10
    done
  fi

  messageTitle "TMC Verify Cluster"     
  messagePrint " - Cluster Name"           "$TMC_CLUSTER_NAME"
  messagePrint " - TKG Management Cluster" "$TMC_MANAGEMENT_CLUSTER"
  messagePrint " - Cluster Status"         "$stt"
  messagePrint " - Provisioner Name"       "$TMC_PROVISONER_NAME"
  messagePrint " - Account Name"           "$TMC_ACCOUNT_NAME"
  echo "----------------------------------------------------------------------------------------"
  echo " TO-DELETE-THE-CLUSTER"
  echo " => tmc cluster delete -m $TMC_MANAGEMENT_CLUSTER -p $TMC_PROVISONER_NAME $TMC_CLUSTER_NAME"
  echo "----------------------------------------------------------------------------------------"
}

alignStr() {
  printf "%-25s" "$1"
}

getTDHClusterCredentials() {
  messageTitle "TMC Get TKG Cluster Credentials"
  messagePrint " - Cluster Name"           "$TMC_CLUSTER_NAME"
  messagePrint " - Kubeconfig File"        "/tmp/${TMC_CLUSTER_NAME}.kubeconfig"
  messagePrint " - Kubernetes Context"     "$TMC_CLUSTER_NAME"

  ret=1; cnt=0
  while [ $ret -ne 0 -a $cnt -lt 10 ]; do
    tmc cluster auth kubeconfig get $TMC_CLUSTER_NAME -p $TMC_PROVISONER_NAME -m $TMC_MANAGEMENT_CLUSTER > /dev/null 2>&1; ret=$?
    sleep 60
    let cnt=cnt+1
  done

  if [ $ret -ne 0 ]; then 
    echo "ERROR: Failled to retrieve cluster credentials, aborting"
    echo "       => tmc cluster auth kubeconfig get $TMC_CLUSTER_NAME -p $TMC_PROVISONER_NAME -m $TMC_MANAGEMENT_CLUSTER"
    exit
  fi

  tmc cluster auth kubeconfig get $TMC_CLUSTER_NAME -p $TMC_PROVISONER_NAME -m $TMC_MANAGEMENT_CLUSTER > /tmp/${TMC_CLUSTER_NAME}.kubeconfig
  export KUBECONFIG=/tmp/${TMC_CLUSTER_NAME}.kubeconfig
  chmod 600 /tmp/${TMC_CLUSTER_NAME}.kubeconfig

  kubectl config use-context $TMC_CLUSTER_NAME 
  echo "----------------------------------------------------------------------------------------"
  echo "export KUBECONFIG=/tmp/${TMC_CLUSTER_NAME}.kubeconfig"
  echo "kubectl config get-contexts"
  echo "----------------------------------------------------------------------------------------"
}

verifyDockerRegistry() {
a=1
}

verifyVMwareRegistry() {
a=1
}

installSpringCloudGateway() {
  NAMESPACE=spring-cloud-gateway

  if [ "$TDH_SERVICE_SPRING_CLOUD_GATEWAY" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_SPRING_CLOUD_GATEWAY        "false"
    return
  fi

  # --- DOWNLOAD BUILD_SERVICE BINARIES ---
  downloadFromPivnet spring-cloud-gateway-for-kubernetes $TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION \
      "$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION" "Spring Cloud Gateway for Kubernetes Installer"

  TBS_FILE="/tmp/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION.tgz"
  [ -d /tmp/spring-cloud-gateway ] && rm -rf /tmp/spring-cloud-gateway
  mkdir -p /tmp/spring-cloud-gateway
  tar xfz $TBS_FILE -C /tmp/spring-cloud-gateway

  messagePrint "Verify VMware Registry Access" "$TDH_REGISTRY_VMWARE_NAME"
  docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "ERROR: Docker login does not work"
    echo "       => docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS"; exit
  fi

  #########################################################################################################################
  ################################# TANZU BUILD SERVICE (TBS) - ON HARBOR REGISTRY ########################################
  #########################################################################################################################

  if [ "$TDH_SERVICE_REGISTRY_HARBOR" == "true" ]; then
    DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
    TDH_HARBOR_REGISTRY_DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
    TDH_HARBOR_REGISTRY_ADMIN_PASSWORD=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD)
    messagePrint "Verify Harbor Registry Access" "$TDH_HARBOR_REGISTRY_DNS_HARBOR"
    docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Docker login does not work 1"
      echo "       => docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"; exit
    fi

    kubectl get ns $NAMESPACE > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      # --- INSTALL CONTOUR INGRESS ---
      messageTitle "Installing Spring Cloud Gateway"

      messagePrint "- Relocate the images" "$TDH_REGISTRY_VMWARE_NAME/library/build-service"
      cd /tmp/spring-cloud-gateway/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION
      scripts/relocate-images.sh $DNS_HARBOR/library/spring-cloud-gateway > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: Image Relocation failed"
        echo "       => cd /tmp/spring-cloud-gateway/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION"
        echo "       => scripts/relocate-images.sh $DNS_HARBOR/library/spring-cloud-gateway" 
      fi

      messagePrint "- Deploy Spring Cloud Gateway" "$NAMESPACE"
      echo "-----------------------------------------------------------------------------------------------------------"
      cd /tmp/spring-cloud-gateway/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION
      ./scripts/install-spring-cloud-gateway.sh
      echo "-----------------------------------------------------------------------------------------------------------"
      if [ $? -ne 0 ]; then
        echo "ERROR: Image Relocation failed"
        echo "       => cd /tmp/spring-cloud-gateway/spring-cloud-gateway-k8s-$TDH_SERVICE_SPRING_CLOUD_GATEWAY_VERSION"
        echo "       => ./scripts/install-spring-cloud-gateway.sh"
      fi
    else
      messageTitle "Verify Spring Cloud Gateway"
    fi
  fi
} 
#hhhhhhhhhhhhhhhh

installMinio() {
  if [ "$TDH_SERVICE_MINIO" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO        "false"
    helm uninstall minio > /dev/null 2>&1
    return
  fi

  TDH_DOMAIN=$(getConfigMap tanzu-demo-hub TDH_DOMAIN)
  TDH_ENVNAME=$(getConfigMap tanzu-demo-hub TDH_ENVNAME)
  DOMAIN=$(getConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN)

  cnt=$(helm list -q | egrep -c "^tdh-minio")
  if [ $cnt -eq 0 ]; then
    messageTitle "Installing Minio"
    HELM_VALUES=/tmp/helm_values.yaml
    echo ""                                                                                                                     >  $HELM_VALUES
    echo "ingress:"                                                                                                             >> $HELM_VALUES
    echo "  enabled: true"                                                                                                      >> $HELM_VALUES
    echo "  certManager: false"                                                                                                 >> $HELM_VALUES
    echo "  annotations:"                                                                                                       >> $HELM_VALUES
    echo "    kubernetes.io/ingress.class: contour"                                                                             >> $HELM_VALUES
    echo "    ingress.kubernetes.io/force-ssl-redirect: \"true\""                                                               >> $HELM_VALUES
    echo "  path: /"                                                                                                            >> $HELM_VALUES
    echo "  hostname: minio.$DOMAIN"                                                                                            >> $HELM_VALUES
    echo "  tls: true"                                                                                                          >> $HELM_VALUES
    echo "  servicePort: minio"                                                                                                 >> $HELM_VALUES
    echo "  extraTls:"                                                                                                          >> $HELM_VALUES
    echo "  - hosts:"                                                                                                           >> $HELM_VALUES
    echo "      - minio.$DOMAIN"                                                                                                >> $HELM_VALUES
    echo "    secretName: tanzu-demo-hub-tls"                                                                                   >> $HELM_VALUES
    echo "  extraHosts:"                                                                                                        >> $HELM_VALUES
    echo "    - name: tdh-postgres-backup.minio.$DOMAIN"                                                                        >> $HELM_VALUES
    echo "      path: /"                                                                                                        >> $HELM_VALUES

    messagePrint " - Deploy the Postgres operator" "postgres-operator"
    helm install tdh-minio bitnami/minio -f ${HELM_VALUES} > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Failed to deploy Minio Helm Chart"
      echo "       => helm install tdh-minio bitnami/minio -f ${HELM_VALUES}"
      exit
    fi
  fi

  ver=$(helm ls -o json | jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).app_version')
  crt=$(helm ls -o json | jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).chart')
  stt=$(helm ls -o json | jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).status')
  dat=$(helm ls -o json | jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).updated')
  ACCESS_KEY=$(kubectl get secret --namespace default tdh-minio -o jsonpath="{.data.access-key}" | base64 --decode)
  SECRET_KEY=$(kubectl get secret --namespace default tdh-minio -o jsonpath="{.data.secret-key}" | base64 --decode)

  messageTitle "Verify Minio S3"
  messagePrint " - Helm Chart"                     "$crt"
  messagePrint " - Helm Chart Name"                "minio"
  messagePrint " - Helm Chart Version"             "$ver"
  messagePrint " - Helm Chart Status"              "$stt"
  messagePrint " - Helm Chart Installed/Updated"   "$dat"
  messagePrint " - Minio Internal Name"            "tdh-minio.default.svc.cluster.local"
  messagePrint " - Minio Management Portal"        "https://minio.$DOMAIN"
  messagePrint " - Minio Access Key"               "$ACCESS_KEY"
  messagePrint " - Minio Secret Key"               "$SECRET_KEY"
  echo "-----------------------------------------------------------------------------------------------------------"
  echo " Test Minio S3 Access: (https://docs.min.io/docs/minio-client-complete-guide)"
  echo " => brew install minio/stable/mc"
  echo " => mc alias set minio https://minio.$DOMAIN $ACCESS_KEY $SECRET_KEY"
  echo " => mc ls minio"
  echo "-----------------------------------------------------------------------------------------------------------"
  
  # --- WAIT FOR HARBOR TO COME ONLINE ---
  ret=1; cnt=0
  while [ $ret -ne 0 -a $cnt -le 5 ]; do
    mc alias set minio https://minio.$DOMAIN $ACCESS_KEY $SECRET_KEY > /dev/null 2>&1; ret=$?
    sleep 60
    let cnt=cnt+1
  done

  if [ $ret -ne 0 ]; then
    echo "ERROR: Unable to configure Minio Client (mc)"
    echo "       => mc alias set minio https://minio.$DOMAIN $ACCESS_KEY $SECRET_KEY"
    exit
  fi

  #mc mb minio/tdh-postgres-backup > /dev/null 2>&1

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO                "true"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO_INTERNAL_NAME  "tdh-minio.default.svc.cluster.local:9000"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO_ACCESS_KEY     "$ACCESS_KEY"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_MINIO_SECRET_KEY     "$SECRET_KEY"
}

installTanzuDataPostgres() {
  NAMESPACE=postgres-operator

  if [ "$TDH_SERVICE_TANZU_DATA_POSTGRES" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_POSTGRES        "false"
    kubectl delete -n $NAMESPACE > /dev/null 2>&1
    helm uninstall tdh-pgadmin > /dev/null 2>&1
    helm uninstall postgres-operator > /dev/null 2>&1
    return
  fi

  # --- DOWNLOAD BUILD_SERVICE BINARIES ---
  downloadFromPivnet tanzu-sql-postgres $TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION "$TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION" "Postgres for VMware Tanzu v$TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION"

  TBS_FILE="/tmp/postgres-for-kubernetes-v$TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION.tar.gz"
  [ -d /tmp/postgres-for-kubernetes ] && rm -rf /tmp/postgres-for-kubernetes
  mkdir -p /tmp/postgres-for-kubernetes
  tar xfz $TBS_FILE -C /tmp/postgres-for-kubernetes

  if [ "$TDH_SERVICE_REGISTRY_HARBOR" == "true" ]; then
    DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
    TDH_HARBOR_REGISTRY_DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
    TDH_HARBOR_REGISTRY_ADMIN_PASSWORD=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD)
    messagePrint "Verify Harbor Registry Access" "$TDH_HARBOR_REGISTRY_DNS_HARBOR"
    docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Docker login does not work 1"
      echo "       => docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"; exit
    fi

    cnt=$(helm list -q | egrep -c "^postgres-operator") 
    if [ $cnt -eq 0 ]; then
      messageTitle "Installing Postgres Operator"

      # --- CREATE NAMESPACE ---
      #kubectl create namespace $NAMESPACE > /dev/null 2>&1

      # --- INSTALL CONTOUR INGRESS ---
      POSTGRES_INSTALL_PATH="/tmp/postgres-for-kubernetes/postgres-for-kubernetes-v$TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION"
      REGISTRY="harbor.apps-contour.local.pcfsdu.com/library"

      docker load -i ${POSTGRES_INSTALL_PATH}/images/postgres-instance  > /dev/null 2>&1
      docker load -i ${POSTGRES_INSTALL_PATH}/images/postgres-operator  > /dev/null 2>&1

      messagePrint " - Reocate Images (postgres-instance) to Harbor" "$REGISTRY/postgres-instance"
      INSTANCE_IMAGE_NAME="${REGISTRY}/postgres-instance:$(cat $POSTGRES_INSTALL_PATH/images/postgres-instance-tag)"
      docker tag $(cat $POSTGRES_INSTALL_PATH/images/postgres-instance-id) ${INSTANCE_IMAGE_NAME}  > /dev/null 2>&1
      docker push ${INSTANCE_IMAGE_NAME}  > /dev/null 2>&1

      messagePrint " - Reocate Images (postgres-operator) to Harbor" "$REGISTRY/postgres-operator"
      OPERATOR_IMAGE_NAME="${REGISTRY}/postgres-operator:$(cat $POSTGRES_INSTALL_PATH/images/postgres-operator-tag)"
      docker tag $(cat $POSTGRES_INSTALL_PATH/images/postgres-operator-id) ${OPERATOR_IMAGE_NAME}  > /dev/null 2>&1
      docker push ${OPERATOR_IMAGE_NAME}  > /dev/null 2>&1

      # --- CLEANUP DOCKER IMAGES ---
      #for n in $(docker images | egrep "postgres-(operator|instance) " | awk '{ print $3 }'); do
      #  docker image rm $n -f > /dev/null 2>&1
      #done

      messagePrint "Customize Postgres Helm Chart Values" "/tmp/values-overrides.yaml"
      sed -e "s+operatorImageRepository: postgres-operator+operatorImageRepository: $REGISTRY/postgres-operator+g" \
          -e "s+postgresImageRepository: postgres-instance+postgresImageRepository: $REGISTRY/postgres-instance+g" \
          ${POSTGRES_INSTALL_PATH}/operator/values.yaml > /tmp/values-overrides.yaml
      if [ "${DEBUG}" == "1" ]; then
        echo "-----------------------------------------------------------------------------------------------------------"
        cat /tmp/values-overrides.yaml
        echo "-----------------------------------------------------------------------------------------------------------"
      fi

      messagePrint " - Deploy the Postgres operator" "postgres-operator"
      #helm install postgres-operator -n $NAMESPACE -f /tmp/values-overrides.yaml ${POSTGRES_INSTALL_PATH}/operator/ > /dev/null 2>&1
      helm install postgres-operator -f /tmp/values-overrides.yaml ${POSTGRES_INSTALL_PATH}/operator/ > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: Failed to deploy Postgres Helm Chart"
        echo "       => helm install postgres-operator -n $NAMESPACE -f /tmp/values-overrides.yaml ${POSTGRES_INSTALL_PATH}/operator/"
        exit
      fi
    fi

    ver=$(helm ls -o json | jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).app_version')
    crt=$(helm ls -o json | jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).chart')
    stt=$(helm ls -o json | jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).status')
    dat=$(helm ls -o json | jq -r --arg chart "postgres-operator" '.[] | select(.name == $chart).updated')
    
    messageTitle "Verify Postgres Operator"  
    messagePrint " - Helm Chart"                     "$crt"
    messagePrint " - Helm Chart Name"                "postgres-operator"
    messagePrint " - Helm Chart Version"             "$ver"
    messagePrint " - Helm Chart Status"              "$stt"
    messagePrint " - Helm Chart Installed/Updated"   "$dat"

    ver=$(helm ls -o json | jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).app_version')
    crt=$(helm ls -o json | jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).chart')
    stt=$(helm ls -o json | jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).status')
    dat=$(helm ls -o json | jq -r --arg chart "tdh-pgadmin" '.[] | select(.name == $chart).updated')

    messageTitle "Verify pgAdmin" 
    messagePrint " - Helm Chart"                     "$crt"
    messagePrint " - Helm Chart Name"                "tdh-pgadmin"
    messagePrint " - Helm Chart Version"             "$ver"
    messagePrint " - Helm Chart Status"              "$stt"
    messagePrint " - Helm Chart Installed/Updated"   "$dat"
  fi

  uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_POSTGRES          "true"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION  $TDH_SERVICE_TANZU_DATA_POSTGRES_VERSION
} 

installBuildService() {
  if [ "$TDH_SERVICE_BUILD_SERVICE" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE        "false"
    return
  fi

  if [ "$TDH_SERVICE_BUILD_SERVICE_VERSION" == "" ]; then
    echo "ERROR: Envuronment Variable (TDH_SERVICE_BUILD_SERVICE_VERSION) not found in"
    echo "       in the deployment file: ${TDHPATH}/deployments/${K8S_DEPLOYMENT}"
    exit
  fi

  # --- DOWNLOAD BUILD_SERVICE BINARIES ---
  downloadFromPivnet build-service $TDH_SERVICE_BUILD_SERVICE_VERSION "$TDH_SERVICE_BUILD_SERVICE_VERSION" "build-service-$TDH_SERVICE_BUILD_SERVICE_VERSION.tar"

  TBS_FILE="/tmp/build-service-$TDH_SERVICE_BUILD_SERVICE_VERSION.tar"
  [ -d /tmp/build-service ] && rm -rf /tmp/build-service
  mkdir /tmp/build-service
  tar xf $TBS_FILE -C /tmp/build-service

  # --- VERIFY SERVICE CONFIGURATION ---
  checkKubernetesServices registry_vmware
  checkKubernetesServices registry_docker
  checkKubernetesServices github

  messagePrint "Verify VMware Registry Access" "$TDH_REGISTRY_VMWARE_NAME"
  docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    echo "ERROR: Docker login does not work"
    echo "       => docker login $TDH_REGISTRY_VMWARE_NAME -u $TDH_REGISTRY_VMWARE_USER -p $TDH_REGISTRY_VMWARE_PASS"; exit
  fi

  #########################################################################################################################
  ################################# TANZU BUILD SERVICE (TBS) - ON DOCKER REGISTRY ########################################
  #########################################################################################################################

  if [ "$TDH_SERVICE_REGISTRY_DOCKER" == "true1" ]; then
    DNS_DOCKER=$(getConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_NAME)
    TDH_REGISTRY_DOCKER_NAME=$(getConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_NAME)
    TDH_REGISTRY_DOCKER_PASS=$(getConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_PASS)

    # --- TESTING REGISTRY ---
    messagePrint "Verify Docker Registry Access" "$TDH_REGISTRY_DOCKER_NAME"
    docker login -u $TDH_REGISTRY_DOCKER_USER -p $TDH_REGISTRY_DOCKER_PASS > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Docker login does not work"
      echo "       => docker login $TDH_REGISTRY_DOCKER_NAME -u $TDH_REGISTRY_DOCKER_USER -p $TDH_REGISTRY_DOCKER_PASS"; exit
    fi

    messagePrint "- Relocate images to" "$TDH_REGISTRY_DOCKER_NAME/$TDH_REGISTRY_DOCKER_USER/build-service"
    kbld relocate -f /tmp/build-service/images.lock --lock-output /tmp/build-service/images-relocated.lock \
         --repository $TDH_REGISTRY_DOCKER_NAME/$TDH_REGISTRY_DOCKER_USER/build-service > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Image Relocation faied"
      echo "       => kbld relocate -f /tmp/build-service/images.lock --lock-output /tmp/build-service/images-relocated.lock \\"
      echo "          --repository $TDH_REGISTRY_DOCKER_NAME/$TDH_REGISTRY_DOCKER_USER/build-service"; exit
    fi
  fi

  #########################################################################################################################
  ################################# TANZU BUILD SERVICE (TBS) - ON HARBOR REGISTRY ########################################
  #########################################################################################################################

  if [ "$TDH_SERVICE_REGISTRY_HARBOR" == "true" ]; then 
    DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
    TDH_HARBOR_REGISTRY_DNS_HARBOR=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR)
    TDH_HARBOR_REGISTRY_ADMIN_PASSWORD=$(getConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD)
    messagePrint "Verify Harbor Registry Access" "$TDH_HARBOR_REGISTRY_DNS_HARBOR"
    docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Docker login does not work 1"
      echo "       => docker login $TDH_HARBOR_REGISTRY_DNS_HARBOR -u admin -p $TDH_HARBOR_REGISTRY_ADMIN_PASSWORD"; exit
    fi

    messagePrint "- Relocate the images" "$TDH_REGISTRY_VMWARE_NAME/library/build-service"
    kbld relocate -f /tmp/build-service/images.lock --lock-output /tmp/build-service/images-relocated.lock \
         --repository $DNS_HARBOR/library/build-service > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: Image Relocation faied"
      echo "       => kbld relocate -f /tmp/build-service/images.lock --lock-output /tmp/build-service/images-relocated.lock \\"
      echo "          --repository $DNS_HARBOR/library/build-service"; exit
    fi

    # --- DEPLOY KAPP ---
    stt=$(kapp inspect -a tanzu-build-service 2>/dev/null | tail -1)
    if [ "$stt" != "Succeeded" ]; then
      messageTitle "Installing Tanzu Build Service"
      messagePrint "- Install Build Service (kapp)" "`date`"
      echo "-----------------------------------------------------------------------------------------------------------"
      echo " WARNING: This process may take up to 2h for the deployment, Please run kapp manually to get more info"
      echo "          => kapp inspect -a tanzu-build-service --column Namespace,name,reconcile_state | grep -v ok"
      echo "-----------------------------------------------------------------------------------------------------------"
        ytt -f /tmp/build-service/values.yaml \
            -f /tmp/build-service/manifests/ \
            -f $TDHPATH/certificates/ca.pem \
            -v docker_repository="$DNS_HARBOR/library/build-service" \
            -v docker_username="admin" \
            -v docker_password="$TDH_HARBOR_REGISTRY_ADMIN_PASSWORD" 2>/dev/null \
            | kbld -f /tmp/build-service/images-relocated.lock -f- 2>/dev/null \
            | kapp deploy -a tanzu-build-service -f- -y > /dev/null 2>&1; ret=$?
  
      cnt=$(kapp inspect -a tanzu-build-service --json | jq -r '.Tables[].Rows[].reconcile_state' | grep -vc "ok")
      while [ $cnt -ne 0 ]; do
        cnt=$(kapp inspect -a tanzu-build-service --json | jq -r '.Tables[].Rows[].reconcile_state' | grep -vc "ok")
        sleep 360
      done
      messagePrint "- Install Build Service completed" "`date`"
    else
      messageTitle "Verify Tanzu Build Service"
      messagePrint "- Veriify Build Service (kapp)" "tanzu-build-service"
    fi
  fi

  # --- DOWNLOAD KP UTILITY ---
  if [ ! -f /usr/local/bin/kp ]; then 
    downloadFromPivnet build-service $TDH_SERVICE_BUILD_SERVICE_VERSION $TDH_SERVICE_BUILD_SERVICE_KP "kp-darwin"

    echo "INFO: Please copy the file /tmp/kp-darwin-$TDH_SERVICE_BUILD_SERVICE_KP /usr/local/bin"
    echo "      => sudo mv /tmp/kp-darwin-$TDH_SERVICE_BUILD_SERVICE_KP /usr/local/bin"
    echo "      => sudo chmod a+x /usr/local/bin/kp"
  fi

  kp clusterbuilder list > /dev/null 2>&1
  if [ $? -ne 0 ]; then 
    # --- DOWNLOAD BUILD_SERVICE DEPENDANCIES ---
    downloadFromPivnet tbs-dependencies $TDH_SERVICE_BUILD_SERVICE_DEPENDANCIES $TDH_SERVICE_BUILD_SERVICE_DEPENDANCIES "descriptor-$TDH_SERVICE_BUILD_SERVICE_DEPENDANCIES.yaml"

    messagePrint "- Import Dependency Descriptors"
    kp import -f /tmp/descriptor-$TDH_SERVICE_BUILD_SERVICE_DEPENDANCIES.yaml > /dev/null 2>&1
  else
    messagePrint "- Verify Dependency Descriptors"
    echo "-----------------------------------------------------------------------------------------------------------"
    kp clusterbuilder list | sed '$d'
    echo "-----------------------------------------------------------------------------------------------------------"
  fi

  uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE          "true"
  uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE_VERSION  $TDH_SERVICE_BUILD_SERVICE_VERSION
  uodateConfigMap tanzu-demo-hub TDH_REGISTRY_VMWARE_NAME           $TDH_REGISTRY_VMWARE_NAME
  uodateConfigMap tanzu-demo-hub TDH_REGISTRY_VMWARE_USER           $TDH_REGISTRY_VMWARE_USER
  uodateConfigMap tanzu-demo-hub TDH_REGISTRY_VMWARE_PASS           $TDH_REGISTRY_VMWARE_PASS
  uodateConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_NAME           $TDH_REGISTRY_DOCKER_NAME
  uodateConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_USER           $TDH_REGISTRY_DOCKER_USER
  uodateConfigMap tanzu-demo-hub TDH_REGISTRY_DOCKER_PASS           $TDH_REGISTRY_DOCKER_PASS
  uodateConfigMap tanzu-demo-hub TDH_GITHUB_USER                    $TDH_GITHUB_USER
  uodateConfigMap tanzu-demo-hub TDH_GITHUB_PASS                    $TDH_GITHUB_PASS
}

downloadFromPivnet() {
  PRODUCT_SLUG=$1
  SLUG_VER="$2"
  PROD_VER="$3"
  PROD_STR="$4"

  for pid in $(pivnetAPI $PCF_PIVNET_TOKEN GET products/$PRODUCT_SLUG/releases 2> /dev/null | \
             jq -r ".releases[] | select(.version | scan(\"^$SLUG_VER\")).id"); do
    fid=$(pivnetAPI $PCF_PIVNET_TOKEN GET products/$PRODUCT_SLUG/releases/$pid 2>/dev/null | \
          jq -r ".product_files[] | select(.name | scan(\"^$PROD_STR\")) | select(.file_type == \"Software\") | select(.file_version | scan(\"$PROD_VER\")).id")

    if [ "${fid}" != "" ]; then
      # --- ACCEPT EULA ---
      pivnetAPI $PCF_PIVNET_TOKEN POST products/$PRODUCT_SLUG/releases/$pid/eula_acceptance > /dev/null 2>&1
      break
    else
      echo "ERROR: Unable to find $PRODUCT_SLUG $2 on Pivnet"
      exit 1
    fi
  done

  if [ "$fid" == "" ]; then 
    echo "ERROR: Can not find version: $SLUG_VER from ($PRODUCT_SLUG) on PIVNET"
    exit
  fi

  file=$(pivnetAPI $PCF_PIVNET_TOKEN GET products/$PRODUCT_SLUG/releases/$pid 2>/dev/null | \
       jq -r ".product_files[] | select(.id | tostring | scan(\"^$fid$\")).aws_object_key" | \
       awk -F'/' '{ print $NF }')

  if [ ! -f /tmp/$file ]; then
    messageTitle "Download ($PRODUCT_SLUG) from PIVNET"
    messagePrint " - Product Name:" "$PRODUCT_SLUG"
    messagePrint " - Product Slug:" "$SLUG_VER"
    messagePrint " - Product FileID:" "$fid"
    messagePrint " - Download Location:" "/tmp/$file"
    pivnetAPIdownload $PCF_PIVNET_TOKEN $PRODUCT_SLUG $pid $fid $file
  else
    messageTitle "Verify PIVNET Download ($PRODUCT_SLUG)"
    messagePrint " - Product Name:" "$PRODUCT_SLUG"
    messagePrint " - Product Slug:" "$SLUG_VER"
    messagePrint " - Product FileID:" "$fid"
    messagePrint " - Download Location:" "/tmp/$file"
  fi
}

getRootCA() {
  SECRET=$1
  LETSENSCRIPT_INTERMEDIATE_CERT=""
  LETSENSCRIPT_ROOT_CERT=""
  THD_TLS_CERTIFICATE=tdh-cert.pem
  THD_TLS_KEY=tdh-key.pem

  # --- CREATE CERTIFICATES DIRECTORY ---
  [ ! -d $TDHPATH/certificates ] && mkdir $TDHPATH/certificates

  if [ ! -f $TDHPATH/certificates/ca.pem -o $TDHPATH/certificates/$THD_TLS_CERTIFICATE -o $TDHPATH/certificates/$THD_TLS_KEY ]; then 
    messageTitle "Get RootCA for TLS Certificate ($SECRET)"
    kubectl get secret $SECRET -o json | jq -r '.data."tls.crt"' | base64 -d > $TDHPATH/certificates/$THD_TLS_CERTIFICATE
    kubectl get secret $SECRET -o json | jq -r '.data."tls.key"' | base64 -d > $TDHPATH/certificates/$THD_TLS_KEY

    if [ ! -f $TDHPATH/certificates/$THD_TLS_CERTIFICATE ]; then 
      echo "ERROR: Failed to get tdh-cert.pem"
      echo "       => kubectl get secret $SECRET -o json | jq -r '.data."tls.crt"' | base64 -d > $TDHPATH/certificates/$THD_TLS_CERTIFICATE"
      exit
    fi

    if [ ! -f $TDHPATH/certificates/$THD_TLS_KEY ]; then 
      echo "ERROR: Failed to get $THD_TLS_KEY"
      echo "       => kubectl get secret $SECRET -o json | jq -r '.data."tls.key"' | base64 -d > $TDHPATH/certificates/$THD_TLS_KEY"
      exit
    fi

    txt=$(kubectl get secret $SECRET -o json | jq -r '.data."tls.crt"' | base64 -d | openssl x509 -noout -issuer) 
    ca_country=$(echo $txt | awk -F'/' '{ print $2 }' | awk -F'=' '{ print $2 }') 
    ca_issuer=$(echo $txt | awk -F'/' '{ print $3 }' | awk -F'=' '{ print $2 }') 
    ca_cn=$(echo $txt | awk -F'/' '{ print $4 }' | awk -F'=' '{ print $2 }') 
  
    messagePrint " - TDH Certificate Issuer:"  "$ca_issuer"
    messagePrint " - TDH Certificate File:"    "$THD_TLS_CERTIFICATE"
    messagePrint " - TDH Certificate CN:"      "$ca_cn"
  
    if [ "$ca_issuer" == "Let's Encrypt" ]; then 
      if [ "$ca_cn" == "R3" ]; then 
        LETSENSCRIPT_INTERMEDIATE_CERT=$ca_cn
        LETSENSCRIPT_INTERMEDIATE_CERT_FILE=lets-encrypt-r3.pem

        url=https://letsencrypt.org/certs/lets-encrypt-r3.pem
        curl $url 2>/dev/null > $TDHPATH/certificates/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE
        if [ $? -eq 0 ]; then
          messagePrint " - Let's Encrypt Intermediate Certificate:"      "certificates/$LETSENSCRIPT_INTERMEDIATE_CERT"
          messagePrint " - Let's Encrypt Intermediate Certificate Url:"  "$url"
          messagePrint " - Let's Encrypt Intermediate Certificate File:" "certificates/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE"
        else
          echo "ERROR: Failed to optiain the root certidicate ertificates/$LETSENSCRIPT_INTERMEDIATE_CERT"
          echo "       => curl $url > $TDHPATH/certificates/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE"
          exit
        fi
      fi

      if [ "$ca_cn" == "E1" ]; then 
        LETSENSCRIPT_INTERMEDIATE_CERT=$ca_cn
        LETSENSCRIPT_INTERMEDIATE_CERT_FILE=lets-encrypt-e1.pem

        url=https://letsencrypt.org/certs/lets-encrypt-e1.pem
        curl $url 2>/dev/null > $TDHPATH/certificates/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE
        if [ $? -eq 0 ]; then
          messagePrint " - Let's Encrypt Intermediate Certificate:"      "certificates/$LETSENSCRIPT_INTERMEDIATE_CERT"
          messagePrint " - Let's Encrypt Intermediate Certificate Url:"  "$url"
          messagePrint " - Let's Encrypt Intermediate Certificate File:" "certificates/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE"
        else
          echo "ERROR: Failed to optiain the root certidicate ertificates/$LETSENSCRIPT_INTERMEDIATE_CERT"
          echo "       => curl $url > $TDHPATH/certificates/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE"
          exit
        fi
      fi

      if [ "$LETSENSCRIPT_INTERMEDIATE_CERT_FILE" != "" ]; then
        txt=$(openssl x509 -noout -in certificates/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE -issuer)
        ca_root_country=$(echo $txt | awk -F'/' '{ print $2 }' | awk -F'=' '{ print $2 }')
        ca_root_issuer=$(echo $txt | awk -F'/' '{ print $3 }' | awk -F'=' '{ print $2 }')
        ca_root_cn=$(echo $txt | awk -F'/' '{ print $4 }' | awk -F'=' '{ print $2 }')

        if [ "$ca_root_cn" == "ISRG Root X1" ]; then
          LETSENSCRIPT_ROOT_CERT=$ca_cn
          LETSENSCRIPT_ROOT_CERT_FILE=isrgrootx1.pem
  
          url=https://letsencrypt.org/certs/isrgrootx1.pem
          curl $url 2>/dev/null > $TDHPATH/certificates/$LETSENSCRIPT_ROOT_CERT_FILE
          messagePrint " - Let's Encrypt Root Certificate:"      "certificates/$LETSENSCRIPT_ROOT_CERT"
          messagePrint " - Let's Encrypt Root Certificate Url:"  "$url"
          messagePrint " - Let's Encrypt Root Certificate File:" "certificates/$LETSENSCRIPT_ROOT_CERT_FILE"
        fi

        cat $TDHPATH/certificates/$LETSENSCRIPT_INTERMEDIATE_CERT_FILE $TDHPATH/certificates/$LETSENSCRIPT_ROOT_CERT_FILE > $TDHPATH/certificates/ca.pem
      fi
    else
      echo "ERROR: Certificate Issuer unknown, Please place it manually to certificates/ca.pem"
      exit
    fi
  fi

  # --- VERIFY ROOTCA WITH CERTIFICATE ---
  stt=$(openssl verify -CAfile certificates/ca.pem certificates/$THD_TLS_CERTIFICATE)
  mo1=$(openssl x509 -noout -modulus -in certificates/$THD_TLS_CERTIFICATE | openssl md5)
  mo2=$(openssl rsa -noout -modulus -in certificates/$THD_TLS_KEY | openssl md5) 
  if [ "$mo1" == "$mo2" ]; then vry=ok; else vry=failed; fi

  messageTitle "Verify RootCA with the Certificate ($SECRET)"
  messagePrint " - Verify Cert ($THD_TLS_CERTIFICATE) with RootCA (ca.pem)"    "$stt"
  messagePrint " - Verify Cert ($THD_TLS_CERTIFICATE) with CertKey ($THD_TLS_KEY)"  "$vry"
}

InstallContour() {
  if [ "$TDH_SERVICE_INGRESS_CONTOUR" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_ENABLED        "false"
    return
  fi

  NAMESPACE=ingress-contour

  kubectl get ns $NAMESPACE > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    # --- INSTALL CONTOUR INGRESS ---
    messageTitle "Install Ingress Contour"
    kubectl create ns $NAMESPACE > /dev/null 2>&1
    helm install ingress bitnami/contour -n $NAMESPACE --version 3.3.1 > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to install bitnami/contour"
      echo "       => helm install ingress bitnami/contour -n $NAMESPACE --version 3.3.1"
      exit
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      cnt=$(kubectl get pods -n ingress-contour | sed 1d | grep -vc Running)
      if [ $cnt -eq 0 ]; then cnt=1; fi
      sleep 5
    done
  fi

  ver=$(helm -n $NAMESPACE ls -o json | jq -r '.[].app_version')
  crt=$(helm -n $NAMESPACE ls -o json | jq -r '.[].chart')
  stt=$(helm -n $NAMESPACE ls -o json | jq -r '.[].status')
  dat=$(helm -n $NAMESPACE ls -o json | jq -r '.[].updated')

messageTitle "Verify Ingress Contour"
  messagePrint " - Ingress Contour Namespace:"         "$NAMESPACE"
  messagePrint " - Ingress Contour Helm Chart:"        "$crt"
  messagePrint " - Ingress Contour Version:"           "$ver"
  messagePrint " - Ingress Contour Status:"            "$stt"
  messagePrint " - Ingress Contour Installed/Updated:" "$dat"

  # --- VERIFY DNS DOMAIN ---
  verifyHostedZone ${TDH_ENVNAME}.$AWS_HOSTED_DNS_DOMAIN

  # --- SET DNS RECORD ---
  if [ "$TDH_INFRASTRUCTURE" == "minikube" ]; then 
    ipa=$(kubectl describe svc ingress-contour-envoy --namespace ingress-contour | grep Ingress | awk '{print $3}')
    setDNSrecord "$ipa" "${TDH_ENVNAME}" "${AWS_HOSTED_DNS_DOMAIN}" "*.apps-contour" "A"
  else
    ipa=$(kubectl describe svc ingress-contour-envoy --namespace ingress-contour | grep Ingress | awk '{print $3}')
    setDNSalias "$ipa" "${TDH_ENVNAME}" "${AWS_HOSTED_DNS_DOMAIN}" "*.apps-contour" "A"
  fi

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_ENABLED        "true"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_IP          "$ipa"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_LB_DOMAIN      "apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_NAMESPACE      "$NAMESPACE"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_CHART_NAME     "$crt"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_CHART_VERSION  "$ver"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_CHART_STATUS   "$stt"
  uodateConfigMap tanzu-demo-hub TDH_INGRESS_CONTOUR_CHART_UPDATE   "$dat"
}

createClusterIssuer() {
  #https://cert-manager.io/docs/tutorials/acme/dns-validation/
  NAMESPACE=cert-manager

  if [ "${TDH_HARBOR_STAGING_TLS_CERT}" == "true" ]; then
    LETSENSCRIPT_SERVER="https://acme-staging-v02.api.letsencrypt.org/directory"
  else
    LETSENSCRIPT_SERVER="https://acme-v02.api.letsencrypt.org/directory"
  fi

  kubectl get clusterissuer $TDH_TLS_ISSUER_NAME -n kube-system > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    AWS_HOSTED_ZONE=$(aws route53 list-hosted-zones-by-name --dns-name ${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN} | \
                      jq -r ".HostedZones[] | select(.Name | scan(\"^${zone}.\")).Id")
    AWS_HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \
                        --query "HostedZones[?starts_with(to_string(Name), '${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}.')]" | \
                        jq -r '.[].Id' | awk -F '/' '{ print $NF }')

    messageTitle "Create LetsEnscript ClusterIssuer"
    messagePrint " - LetsEnscript Issuer Name"         "$TDH_TLS_ISSUER_NAME"
    messagePrint " - LetsEnscript Solver"              "route53"
    messagePrint " - LetsEnscript Solver ZoneID"       "$AWS_HOSTED_ZONE_ID"
    messagePrint " - LetsEnscript DNSZone"             "*.apps-contour.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"
    messagePrint " - LetsEnscript DNSZone"             "*.apps-nginx.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}"

    echo "apiVersion: cert-manager.io/v1alpha2"                                    >  $TDH_TLS_ISSUER_NAME
    echo "kind: ClusterIssuer"                                                     >> $TDH_TLS_ISSUER_NAME
    echo "metadata:"                                                               >> $TDH_TLS_ISSUER_NAME
    echo "  name: $TDH_TLS_ISSUER_NAME"                                            >> $TDH_TLS_ISSUER_NAME
    echo "spec:"                                                                   >> $TDH_TLS_ISSUER_NAME
    echo "  acme:"                                                                 >> $TDH_TLS_ISSUER_NAME
    echo "    email: sdubois@vmware.com"                                           >> $TDH_TLS_ISSUER_NAME
    echo "    privateKeySecretRef:"                                                >> $TDH_TLS_ISSUER_NAME
    echo "      name: $TDH_TLS_ISSUER_NAME"                                        >> $TDH_TLS_ISSUER_NAME
    echo "    server: $LETSENSCRIPT_SERVER"                                        >> $TDH_TLS_ISSUER_NAME
    echo "    solvers:"                                                            >> $TDH_TLS_ISSUER_NAME
    echo "    - selector:"                                                         >> $TDH_TLS_ISSUER_NAME
    echo "        dnsZones:"                                                       >> $TDH_TLS_ISSUER_NAME
    echo "          - \"$HARBOR_HOSTNAME\""                                        >> $TDH_TLS_ISSUER_NAME
    echo "          - \"$NOTARY_HOSTNAME\""                                        >> $TDH_TLS_ISSUER_NAME
    echo "          - \"*.apps-contour.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}\""  >> $TDH_TLS_ISSUER_NAME
    echo "          - \"*.apps-nginx.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}\""    >> $TDH_TLS_ISSUER_NAME
    echo "      dns01:"                                                            >> $TDH_TLS_ISSUER_NAME
    echo "        route53:"                                                        >> $TDH_TLS_ISSUER_NAME
    echo "          region: $AWS_REGION"                                           >> $TDH_TLS_ISSUER_NAME
    echo "          accessKeyID: $AWS_CERT_ACCESS_KEY"                             >> $TDH_TLS_ISSUER_NAME
    echo "          secretAccessKeySecretRef:"                                     >> $TDH_TLS_ISSUER_NAME
    echo "            name: route53-credentials-secret"                            >> $TDH_TLS_ISSUER_NAME
    echo "            key: aws-credentials"                                        >> $TDH_TLS_ISSUER_NAME
    echo "          # you can also assume a role with these credentials"           >> $TDH_TLS_ISSUER_NAME
    echo "          hostedZoneID: $AWS_HOSTED_ZONE_ID"                             >> $TDH_TLS_ISSUER_NAME

    # --- CREATE/RECREATE AWS ROUTE53 SECRET ---
    kubectl -n cert-manager delete secret route53-credentials-secret > /dev/null 2>&1
    kubectl -n default delete secret route53-credentials-secret > /dev/null 2>&1
    kubectl -n cert-manager create secret generic route53-credentials-secret \
            --from-literal=aws-credentials="$AWS_CERT_SECRET_KEY" > /dev/null 2>&1

    # --- RECREATE ISSUER ---
    kubectl -n $NAMESPACE delete --all clusterissuer,certificate,order,challenge > /dev/null 2>&1
    sleep 60
    kubectl -n $NAMESPACE apply -f $TDH_TLS_ISSUER_NAME
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to deploy letsenscript issuer"
      echo "       => kubectl -n $NAMESPACE apply -f $TDH_TLS_ISSUER_NAME"
      exit
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      # --- VERIFY ISSUER ---
      issuer_reason=$(kubectl get clusterissuer $TDH_TLS_ISSUER_NAME -n kube-system -o json 2>/dev/null | \
                    jq -r '.status.conditions[].reason' 2>/dev/null)
      issuer_status=$(kubectl get clusterissuer $TDH_TLS_ISSUER_NAME -n kube-system -o json 2>/dev/null | \
                    jq -r '.status.conditions[].status' 2>/dev/null)

      if [ "$issuer_reason" == "ACMEAccountRegistered" -a "$issuer_status" == "True" ]; then cnt=1; fi
      sleep 5
    done

    messagePrint " - LetsEnscript Request Reason"      "$issuer_reason"
    messagePrint " - LetsEnscript Request Status"      "$issuer_status"
  else
    issuer_reason=$(kubectl get clusterissuer letsencrypt-staging -n kube-system -o json 2>/dev/null | \
                  jq -r '.status.conditions[].reason')
    issuer_status=$(kubectl get clusterissuer letsencrypt-staging -n kube-system -o json 2>/dev/null | \
                  jq -r '.status.conditions[].status')
    issuer_date=$(kubectl get clusterissuer letsencrypt-staging -n kube-system -o json 2>/dev/null | \
                  jq -r '.status.conditions[].lastTransitionTime')

    messageTitle "Verify LetsEnscript ClusterIssuer"
    messagePrint " - LetsEnscript Issuer Name"         "$TDH_TLS_ISSUER_NAME"
    messagePrint " - LetsEnscript DNSZone"             "$HARBOR_HOSTNAME"
    messagePrint " - LetsEnscript DNSZone"             "$NOTARY_HOSTNAME"
    messagePrint " - LetsEnscript DNSZone"             "$SACHA_HOSTNAME"
    messagePrint " - LetsEnscript Request Requested"   "$issuer_date"
    messagePrint " - LetsEnscript Request Reason"      "$issuer_reason"
    messagePrint " - LetsEnscript Request Status"      "$issuer_status"
  fi

  # --- CREATE CERTIFICATE ---
  stt_rdy=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | \
            jq -r '.status.conditions[] | select(.type == "Ready").status')
  kubectl get secret $TDH_TLS_SECRET > /dev/null 2>&1; stt_sec=$?
  kubectl get certificate $TDH_TLS_CERT > /dev/null 2>&1; stt_crt=$?
  if [ "${stt_rdy}" == "Frue" -o $stt_sec -ne 0 -o $stt_crt -ne 0 ]; then
    messageTitle "Create LetsEnscript Certificate"
    CERTIFICATE_CONFIG=/tmp/certificate.yaml
    echo "apiVersion: cert-manager.io/v1"                       >  $CERTIFICATE_CONFIG
    echo "kind: Certificate"                                    >> $CERTIFICATE_CONFIG
    echo "metadata:"                                            >> $CERTIFICATE_CONFIG
    echo "  name: $TDH_TLS_CERT"                                >> $CERTIFICATE_CONFIG
    echo "  namespace: default"                                 >> $CERTIFICATE_CONFIG
    echo "spec:"                                                >> $CERTIFICATE_CONFIG
    echo "  secretName: $TDH_TLS_SECRET"                        >> $CERTIFICATE_CONFIG
    echo "  issuerRef:"                                         >> $CERTIFICATE_CONFIG
    echo "    name: letsencrypt-staging"                        >> $CERTIFICATE_CONFIG
    echo "    kind: ClusterIssuer"                              >> $CERTIFICATE_CONFIG
    echo "  dnsNames:"                                          >> $CERTIFICATE_CONFIG
    #echo "  - \"$HARBOR_HOSTNAME\""                             >> $CERTIFICATE_CONFIG
    #echo "  - \"$NOTARY_HOSTNAME\""                             >> $CERTIFICATE_CONFIG
    echo "  - '*.apps-contour.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}'"     >> $CERTIFICATE_CONFIG
    echo "  - '*.apps-nginx.${TDH_ENVNAME}.${AWS_HOSTED_DNS_DOMAIN}'"       >> $CERTIFICATE_CONFIG

    kubectl create -f $CERTIFICATE_CONFIG > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to deploy letsenscript certificate"
      echo "       => kubectl create -f $CERTIFICATE_CONFIG"
      exit
    fi

    sleep 30

    cnt=0
    while [ $cnt -eq 0 ]; do
      stt=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | \
            jq -r '.status.conditions[] | select(.type == "Ready").status')
      if [ "$stt" == "True" ]; then cnt=1; fi
      sleep 5
    done

    if [ "$stt" == "True" ]; then
      tim=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | jq -r '.status.conditions[].lastTransitionTime')
      msg=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | jq -r '.status.conditions[].message')
      tim=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | jq -r '.status.conditions[].lastTransitionTime')
      messagePrint " - LetsEnscript Certificate Name"         "$TDH_TLS_CERT"
      messagePrint " - LetsEnscript Certificate Issued"       "$tim"
      messagePrint " - LetsEnscript Certificate Message"      "$msg"
      messagePrint " - LetsEnscript Certificate Status"       "$stt"
      messagePrint " - LetsEnscript Certificate Secret"       "${TDH_TLS_SECRET}"
    else
      messagePrint " - LetsEnscript Certificate Status"       "$stt"
      echo "------------------------------------------------------------------------------------------------------------------------"
      kubectl describe certificate $TDH_TLS_CERT
      echo "------------------------------------------------------------------------------------------------------------------------"
      echo "=> kubectl describe clusterissuer letsencrypt-staging"
      echo "=> kubectl describe order"
      echo "=> kubectl describe challenge"
      echo "=> kubectl describe certificate"
      exit
    fi
  else
    stt=$(kubectl get certificate $TDH_TLS_CERT -o json 2>/dev/null | jq -r '.status.conditions[].status')
    if [ "$stt" != "True" ]; then
      messagePrint " - LetsEnscript Certificate Status"       "$stt"
      echo "------------------------------------------------------------------------------------------------------------------------"
      kubectl describe certificate $TDH_TLS_CERT
      echo "------------------------------------------------------------------------------------------------------------------------"
      echo "=> kubectl describe clusterissuer letsencrypt-staging"
      echo "=> kubectl describe order"
      echo "=> kubectl describe challenge"
      echo "=> kubectl describe certificate"
      exit
    fi
  fi

  messageTitle "Verify LetsEnscript Certificate"
  messagePrint " - LetsEnscript Certificate Name"       "$TDH_TLS_CERT"
  messagePrint " - LetsEnscript Certificate Secret"     "$TDH_TLS_SECRET"

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_CERTIFICATE_NAME   "$TDH_TLS_CERT"
  uodateConfigMap tanzu-demo-hub TDH_CERTIFICATE_SECRET "$TDH_TLS_SECRET"

  echo "------------------------------------------------------------------------------------------------------------------------"
  kubectl get secret tanzu-demo-hub-tls -o json | jq -r '.data."tls.crt"' | base64 -d | \
          openssl x509 -inform pem -noout -text | grep $AWS_HOSTED_DNS_DOMAIN
  echo "------------------------------------------------------------------------------------------------------------------------"
}

InstallCertManager() {
  NAMESPACE=cert-manager

  kubectl get ns $NAMESPACE > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    # --- INSTALL CONTOUR INGRESS ---
    messageTitle "Install Cert Manager"

    kubectl create ns $NAMESPACE > /dev/null 2>&1
    helm repo add jetstack https://charts.jetstack.io -n $NAMESPACE > /dev/null 2>&1
    #helm install cert-manager jetstack/cert-manager --namespace cert-manager \
    #   --version v1.0.2 --set installCRDs=true > /dev/null 2>&1

    helm install cert-manager jetstack/cert-manager -n cert-manager --version v1.1.0 --set installCRDs=true > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to install jetstack/cert-manager "
      echo "       => helm install cert-manager jetstack/cert-manager --namespace cert-manager \\"
      echo "          --version v1.1.0 --set installCRDs=true"
      exit
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      cnt=$(kubectl get pods -n $NAMESPACE | sed 1d | grep -vc Running)
      if [ $cnt -eq 0 ]; then cnt=1; fi

      sleep 5
    done
  fi

  ver=$(helm -n cert-manager ls -o json | jq -r '.[].app_version')
  crt=$(helm -n cert-manager ls -o json | jq -r '.[].chart')
  stt=$(helm -n cert-manager ls -o json | jq -r '.[].status')
  dat=$(helm -n cert-manager ls -o json | jq -r '.[].updated')

  messageTitle "Verify Cert Manager"
  messagePrint " - Cert Manager Namespace:"         "$NAMESPACE"
  messagePrint " - Cert Manager Helm Chart:"        "$crt"
  messagePrint " - Cert Manager Version:"           "$ver"
  messagePrint " - Cert Manager Status:"            "$stt"
  messagePrint " - Cert Manager Installed/Updated:" "$dat"

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_ENABLED        "true"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_NAMESPACE      "$NAMESPACE"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_CHART_NAME     "$crt"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_CHART_VERSION  "$ver"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_CHART_STATUS   "$stt"
  uodateConfigMap tanzu-demo-hub TDH_CERTMANAGER_CHART_UPDATE   "$dat"
}

InstallHarborRegistry() {
  if [ "$TDH_SERVICE_BUILD_SERVICE" != "true" ]; then
    uodateConfigMap tanzu-demo-hub TDH_SERVICE_BUILD_SERVICE        "false"
    return
  fi

  NAMESPACE=registry-harbor
  HARBOR_TLS_SECRET=$TDH_TLS_SECRET
  NOTARY_TLS_SECRET=$TDH_TLS_SECRET
  HARBOR_HOSTNAME="harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
  NOTARY_HOSTNAME="notary.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"

  kubectl get ns $NAMESPACE > /dev/null 2>&1
  if [ $? -ne 0 ]; then
    messageTitle "Install Harbor Reistry"

    # --- VERIFY SERVICE CONFIGURATION ---
    checkKubernetesServices harbor

    messagePrint " - Harbor Registry/Notary Server"        "harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
    messagePrint "  "                                      "notary.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"
    messagePrint " - Harbor Admin Password"                $(maskPassword "$TDH_HARBOR_ADMIN_PASSWORD")
    messagePrint " - Harbor TLS Staging Cert"              "$TDH_HARBOR_STAGING_TLS_CERT"

    # --- INSTALL CONTOUR INGRESS ---
    kubectl create ns $NAMESPACE > /dev/null 2>&1
    helm uninstall harbor -n $NAMESPACE > /dev/null 2>&1

    # --- COPY SECRET TO NAMESPACE ---
    kubectl get secret tanzu-demo-hub-tls --namespace=default  -oyaml | grep -v '^\s*namespace:\s' | \
    kubectl apply --namespace=registry-harbor -f -

    HARBOR_VALUES=/tmp/harbor_values.yaml
    echo "harborAdminPassword: $TDH_HARBOR_ADMIN_PASSWORD"                                                                      >  $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "service:"                                                                                                             >> $HARBOR_VALUES
    echo "  type: ClusterIP"                                                                                                    >> $HARBOR_VALUES
    echo "  tls:"                                                                                                               >> $HARBOR_VALUES
    echo "    enabled: true"                                                                                                    >> $HARBOR_VALUES
    echo "    existingSecret: $HARBOR_TLS_SECRET"                                                                               >> $HARBOR_VALUES
    echo "    notaryExistingSecret: $NOTARY_TLS_SECRET"                                                                         >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "ingress:"                                                                                                             >> $HARBOR_VALUES
    echo "  enabled: true"                                                                                                      >> $HARBOR_VALUES
    echo "  hosts:"                                                                                                             >> $HARBOR_VALUES
    echo "    core: $HARBOR_HOSTNAME"                                                                                           >> $HARBOR_VALUES
    echo "    notary: $NOTARY_HOSTNAME"                                                                                         >> $HARBOR_VALUES
    echo "  annotations:"                                                                                                       >> $HARBOR_VALUES
    #echo "    cert-manager.io/cluster-issuer: letsencrypt-staging  # use letsencrypt-staging cluster issuer for TLS certs"      >> $HARBOR_VALUES
    #echo "    kubernetes.io/tls-acme: \"true\"                     # using ACME certificates for TLS"                           >> $HARBOR_VALUES
    echo "    ingress.kubernetes.io/force-ssl-redirect: \"true\"   # force https, even if http is requested"                    >> $HARBOR_VALUES
    echo "    kubernetes.io/ingress.class: contour                 # using Contour for ingress"                                 >> $HARBOR_VALUES

    echo "externalURL: https://harbor.apps-contour.$TDH_ENVNAME.$AWS_HOSTED_DNS_DOMAIN"                                         >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "portal:"                                                                                                              >> $HARBOR_VALUES
    echo "  tls:"                                                                                                               >> $HARBOR_VALUES
    echo "    existingSecret: $HARBOR_TLS_SECRET"                                                                               >> $HARBOR_VALUES
    echo ""                                                                                                                     >> $HARBOR_VALUES
    echo "persistence:"                                                                                                         >> $HARBOR_VALUES
    echo "  enabled: true"                                                                                                      >> $HARBOR_VALUES
    echo "  resourcePolicy: 'keep'"                                                                                             >> $HARBOR_VALUES
    echo "  persistentVolumeClaim:"                                                                                             >> $HARBOR_VALUES
    echo "    registry:"                                                                                                        >> $HARBOR_VALUES
    echo "      accessMode: ReadWriteOnce"                                                                                      >> $HARBOR_VALUES
    echo "      size: 50Gi"                                                                                                     >> $HARBOR_VALUES

    helm install harbor bitnami/harbor -f $HARBOR_VALUES --version 9.2.2 -n $NAMESPACE > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failed to install bitnami/harbor"
      echo "       => helm install harbor bitnami/harbor -f $HARBOR_VALUES --version 9.2.2 -n $NAMESPACE"
      exit
    fi

    cnt=0
    while [ $cnt -eq 0 ]; do
      cnt=$(kubectl get pods -n registry-harbor | sed 1d | grep -vc Running)
      if [ $cnt -eq 0 ]; then cnt=1; fi
      sleep 5
    done

    # --- WAIT FOR HARBOR TO COME ONLINE ---
    messagePrint " - Verify Harbor Reistry Access" "$HARBOR_HOSTNAME"
    ret=1; cnt=0
    while [ $ret -ne 0 -a $cnt -le 5 ]; do
      docker login $HARBOR_HOSTNAME -u admin -p $TDH_HARBOR_ADMIN_PASSWORD > /dev/null 2>&1; ret=$?
      sleep 60
      let cnt=cnt+1
    done

    if [ $ret -ne 0 ]; then
      echo "ERROR: failed to login to registry"
      echo "       => docker login $HARBOR_HOSTNAME -u admin -p $TDH_HARBOR_ADMIN_PASSWORD"
      exit
    fi
  fi

  ver=$(helm -n $NAMESPACE ls -o json | jq -r '.[].app_version')
  crt=$(helm -n $NAMESPACE ls -o json | jq -r '.[].chart')
  stt=$(helm -n $NAMESPACE ls -o json | jq -r '.[].status')
  dat=$(helm -n $NAMESPACE ls -o json | jq -r '.[].updated')

  messageTitle "Verify Harbor Reistry"
  messagePrint " - Harbor Reistry Namespace:"         "$NAMESPACE"
  messagePrint " - Harbor Reistry Helm Chart:"        "$crt"
  messagePrint " - Harbor Reistry Version:"           "$ver"
  messagePrint " - Harbor Reistry Status:"            "$stt"
  messagePrint " - Harbor Reistry Installed/Updated:" "$dat"

  # --- STORE SETTINGS IN CONFIGMAP ---
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ENABLED        "true"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_ADMIN_PASSWORD "$TDH_HARBOR_ADMIN_PASSWORD"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_HARBOR     "$HARBOR_HOSTNAME"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_DNS_NOTARY     "$NOTARY_HOSTNAME"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_CHART_NAME     "$crt"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_CHART_VERSION  "$ver"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_CHART_STATUS   "$stt"
  uodateConfigMap tanzu-demo-hub TDH_HARBOR_REGISTRY_CHART_UPDATE   "$dat"
}

tdh_verifyTKGcluster() {
  messageTitle "TKG Cluster Information"
  messagePrint " - Cluster Name"           "$TDH_CLUSTER_NAME"
  messagePrint " - TKG Management Cluster" "$TDH_MANAGEMENT_CLUSTER"
  messagePrint " - Provisioner Name"       "$TDH_PROVISONER_NAME"
  messagePrint " - Account Name"           "$TDH_MISSION_CONTROL_ACCOUNT_NAME"
  echo 

  echo -n "Do you want detailed infos of the TKG cluster setup setup ($TDH_CLUSTER_NAME) ? (y/n): "; read cluster_info
  answer_provided="n"
  while [ "${answer_provided}" == "n" ]; do
    if [ "${cluster_info}" == "y" -o "${cluster_info}" == "Y" ]; then break; fi
    if [ "${cluster_info}" == "n" -o "${cluster_info}" == "N" ]; then break; fi
    echo -n "Do you want detailed infos of the TKG cluster setup setup ($TDH_CLUSTER_NAME) ? (y/n): "; read cluster_info
  done
  echo 

  if [ "${cluster_info}" == "y" -o "${cluster_info}" == "Y" ]; then
    prtHead "Show TDK Cluster infos ($TDH_CLUSTER_NAME)"
    execCmd "tmc cluster list $TDH_CLUSTER_NAME -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISONER_NAME"
    execCmd "tmc cluster get $TDH_CLUSTER_NAME -m $TDH_MANAGEMENT_CLUSTER -p $TDH_PROVISONER_NAME"
  fi
}

cleanAWSenv() {
  # --- DELETE HOSTED ZONE ---
  domain="$ENV_NAME.$AWS_HOSTED_DNS_DOMAIN"
  ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name ${domain} | \
            jq -r ".HostedZones[] | select(.Name | scan(\"^$domain.\")).Id")

  if [ "${ZONE_ID}" != "" ]; then
    messagePrint " - Deleting DNS Hosted Zone:" "$ZONE_ID"
    route53deleteHostedZone $ZONE_ID
  fi

  # --- DELETE INSTANCES ---
  SECGRP=/tmp/$$_aws_routing_table.json
  aws --region $AWS_LOCATION ec2 describe-instances > $SECGRP
  tot=$(grep -c "InstanceId" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    gid=$(jq -r ".Reservations[$i].Instances[].InstanceId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".Reservations[$i].Instances[].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}|p-bosh")
    if [ $cnt -gt 0 ]; then
      cnt=$(jq -r ".Reservations[$i].Instances[].State.Name" 2>/dev/null $SECGRP | egrep -c "terminated")
      if [ $cnt -eq 0 ]; then
        messagePrint " - Terminate Instance:" "$gid"
        aws --region $AWS_LOCATION ec2 terminate-instances --instance-ids $gid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to Terminage Instance: $gid"
          echo "aws --region $AWS_LOCATION ec2 terminate-instances  --instance-ids $gid"
          exit 1
        fi

        sleep 30
      fi
    fi

    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  # --- IAM DELETE ROLES ---
  TMPAWS=/tmp/$$_aws.json
  aws --region $AWS_LOCATION iam list-instance-profiles > $TMPAWS
  tot=$(grep -c "InstanceProfileId" $TMPAWS); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    arn=$(jq -r ".InstanceProfiles[$i].InstanceProfileId" 2>/dev/null $TMPAWS)
    nam=$(jq -r ".InstanceProfiles[$i].InstanceProfileName" 2>/dev/null $TMPAWS)

    cnt=$(echo $nam | egrep -c "^${ENV_NAME}_")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Delete IAM Instance Profile:" "$nam"
      for rol in $(jq -r ".InstanceProfiles[$i].Roles[].RoleName" 2>/dev/null $TMPAWS); do
        aws --region $AWS_LOCATION iam remove-role-from-instance-profile \
           --instance-profile-name $nam --role-name $rol
      done

      aws --region $AWS_LOCATION iam delete-instance-profile --instance-profile-name $nam > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete IAM Instance Profile: $nam"
        echo "aws --region $AWS_LOCATION iam delete-instance-profile --instance-profile-name $nam"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- DELETE KMS ALIASES ---
  aws --region eu-central-1 kms list-aliases --query "Aliases[?contains(to_string(AliasName),'$ENV_NAME')]" > $TMPAWS
  tot=$(grep -c "PolicyName" $TMPAWS); i=0
  while [ $i -lt $tot ]; do
    tid=$(jq -r ".[$i].TargetKeyId" 2>/dev/null $TMPAWS)
    arn=$(jq -r ".[$i].AliasArn" 2>/dev/null $TMPAWS)
    nam=$(jq -r ".[$i].AliasName" 2>/dev/null $TMPAWS)

    messagePrint " - Delete KMS Alias:" "$nam"
  done

  # --- IAM DELETE ROLES ---
  TMPAWS=/tmp/$$_aws.json
  aws --region $AWS_LOCATION iam list-roles --query "Roles[?starts_with(to_string(RoleName), '$ENV_NAME')]" > $TMPAWS

  tot=$(grep -c "RoleName" $TMPAWS); i=0
  while [ $i -lt $tot ]; do
    rid=$(jq -r ".[$i].RoleId" 2>/dev/null $TMPAWS)
    arn=$(jq -r ".[$i].Arn" 2>/dev/null $TMPAWS)
    nam=$(jq -r ".[$i].RoleName" 2>/dev/null $TMPAWS)

    messagePrint " - Delete IAM Role:" "$nam"

    for tmp in $(aws --region $AWS_LOCATION  iam list-policies | jq -r '.Policies[].Arn' | grep $ENV_NAME); do
      usr=$(aws --region $AWS_LOCATION iam list-entities-for-policy --policy-arn $tmp | jq -r '.PolicyUsers[].UserName')
      grp=$(aws --region $AWS_LOCATION iam list-entities-for-policy --policy-arn $tmp | jq -r '.PolicyGroups[].GroupName')
      rol=$(aws --region $AWS_LOCATION iam list-entities-for-policy --policy-arn $tmp | jq -r '.PolicyRoles[].RoleName')
      if [ "${usr}" != "" ]; then
        aws --region $AWS_LOCATION iam detach-user-policy --policy-arn $tmp --user-name $usr
      fi

      if [ "${rol}" != "" ]; then
        aws --region $AWS_LOCATION iam detach-role-policy --policy-arn $tmp --role-name $rol
      fi

      if [ "${grp}" != "" ]; then
        aws --region $AWS_LOCATION iam detach-user-policy --policy-arn $tmp --group-name $usr
      fi

      aws iam delete-policy --policy-arn $tmp
    done

    for pol in $(aws --region $AWS_LOCATION iam list-attached-role-policies --role-name $nam | \
        jq -r ".AttachedPolicies[].PolicyArn"); do

      aws --region $AWS_LOCATION iam detach-role-policy --role-name $nam --policy-arn $pol
      aws --region $AWS_LOCATION iam delete-role-policy --role-name $nam --policy-name $pol
    done

    # --- DELETE ROLE POLICY ---
    for pol in $(aws --region $AWS_LOCATION iam list-role-policies --role-name $nam | \
                 jq -r '.PolicyNames[]'); do

      aws --region $AWS_LOCATION iam delete-role-policy --role-name $nam --policy-name $pol
    done

    aws --region $AWS_LOCATION iam delete-role --role-name $nam > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failled to delete IAM Role: $nam"
      echo "       aws --region $AWS_LOCATION iam delete-role --role-name $nam"
      exit 1
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $TMPAWS

  # --- IAM DELETE USERS ---
  for arn in $(aws --region $AWS_LOCATION iam list-users | jq -r '.Users[].UserName' 2>/dev/null); do
    cnt=$(echo $arn | egrep -c "^${ENV_NAME}_")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Cleaning up iam User:" "$arn"

      # --- DETACH POLICY ---
      for pol in $(aws --region $AWS_LOCATION iam list-attached-user-policies --user-name $arn | \
          jq -r ".AttachedPolicies[].PolicyArn"); do

        aws --region $AWS_LOCATION iam detach-user-policy --user-name $arn --policy-arn $pol
      done

      # --- DELETE POLICY ---
      for pol in $(aws --region $AWS_LOCATION iam list-attached-user-policies --user-name $arn | \
          jq -r ".AttachedPolicies[].PolicyName"); do

        aws --region $AWS_LOCATION iam delete-user-policy --user-name $arn --policy-name $pol
      done

      # --- DELETE ACCESS-KEYS ---
      for key in $(aws --region $AWS_LOCATION iam list-access-keys --user-name $arn | \
          jq -r ".AccessKeyMetadata[].AccessKeyId"); do

        aws --region $AWS_LOCATION iam delete-access-key --access-key-id $key --user-name $arn
      done

      aws --region $AWS_LOCATION iam delete-user --user-name $arn
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete User"
        echo "aws --region $AWS_LOCATION iam delete-user --user-name $arn"
        exit 1
      fi
    fi
  done

  # --- DELETE LOAD BALANCERS ---
  TMPAWS=/tmp/$$_aws.json
  for nam in $(aws --region $AWS_LOCATION elb describe-load-balancers | jq -r '.LoadBalancerDescriptions[].LoadBalancerName' 2>/dev/null); do
    aws --region $AWS_LOCATION elb describe-load-balancers --load-balancer-names $nam >/dev/null 2>&1
    if [ $? -eq 0 ]; then
      for tag in $(aws --region $AWS_LOCATION elb describe-tags --load-balancer-name $nam 2>/dev/null | \
        jq -r '.TagDescriptions[].Tags[].Value'); do

        # --- DELETE LOAD-BALANCERS WITH TAG ENV_NAME ---
        enm=$(aws --region $AWS_LOCATION elb describe-tags --load-balancer-name $nam | \
            jq -r '.TagDescriptions[].Tags[].Value' | grep -c "${ENV_NAME}")

        # --- CLEANUOP NGINX-INGRESS-LOADBALANCER ---
        cnt=$(echo $tag | egrep -c "nginx-ingress-controller")
        if [ ${cnt} -gt 0 -o ${enm} -gt 0 ]; then
          messagePrint " - Cleaning up elp Load-Balancers:" "$nam"
          aws --region $AWS_LOCATION elb delete-load-balancer --load-balancer-name $nam >/dev/null 2>&1
          if [ $? -ne 0 ]; then
            echo "ERROR: failled to delete load-balancer: $nam"
            echo "aws --region $AWS_LOCATION elb delete-load-balancer --load-balancer-name $nam"
            exit 1
          fi
        fi
      done
    fi
  done

  rm -f $TMPAWS

  # --- DELETE LOAD BALANCERS ---
  for arn in $(aws --region $AWS_LOCATION elbv2 describe-load-balancers | jq -r '.LoadBalancers[].LoadBalancerArn' 2>/dev/null); do
    nam=$(aws --region $AWS_LOCATION elbv2 describe-load-balancers --load-balancer-arns $arn | \
        jq -r '.LoadBalancers[].LoadBalancerName')

    cnt=$(echo $nam | egrep -c "^${ENV_NAME}-")
    if [ $cnt -gt 0 ]; then
      aws --region $AWS_LOCATION elbv2 delete-load-balancer --load-balancer-arn $arn
      messagePrint " - Cleaning up elpv2 LoadBalancers:" "$nam"
    fi
  done

  # --- DELETE TARGET GROUPS ---
  SECGRP=/tmp/$$_aws_tgp.json
  aws --region $AWS_LOCATION elbv2 describe-target-groups > $SECGRP
  tot=$(grep -c "TargetGroupArn" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    sid=$(jq -r ".TargetGroups[$i].TargetGroupArn" 2>/dev/null $SECGRP)
    snm=$(jq -r ".TargetGroups[$i].TargetGroupName" 2>/dev/null $SECGRP)

    cnt=$(echo "$snm" | egrep -c "^${ENV_NAME}-")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Delete Target Group:" "$snm"
      aws --region $AWS_LOCATION elbv2 delete-target-group --target-group-arn $sid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete Target group"
        echo "aws --region $AWS_LOCATION elbv2 delete-target-group --target-group-arn $sid"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  for key in $(aws --region $AWS_LOCATION ec2 describe-key-pairs | jq -r '.KeyPairs[].KeyName' 2>/dev/null); do
    cnt=$(echo $key | egrep -c "^${ENV_NAME}-")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Delete KeyPair:" "$key"
      aws --region $AWS_LOCATION ec2 delete-key-pair --key-name $key >/dev/null 2>&1
      if [ $? -ne 0 ]; then
        aws --region $AWS_LOCATION ec2 delete-key-pair --key-name $key
        exit 1
      fi
    fi
  done

  # --- DELETE VPC's ---
  TMPVCP=/tmp/$$_aws_vpc.json
  aws --region $AWS_LOCATION ec2 describe-vpcs > $TMPVCP
  tot=$(grep -c "VpcId" $TMPVCP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    vpc=$(jq -r ".Vpcs[$i].VpcId" 2>/dev/null $TMPVCP)
    cnt=$(jq -r ".Vpcs[$i].Tags[].Value" 2>/dev/null $TMPVCP | egrep -c "^${ENV_NAME}|pcfjump")
    if [ $cnt -eq 0 ]; then let i=i+1; continue; fi
    messagePrint " VPC Ressources:" "$vpc"

    # --- DELETE NAT GATEWAY ---
    for id in $(aws --region $AWS_LOCATION ec2 describe-nat-gateways --filter "Name=vpc-id,Values=$vpc" | \
                jq -r ".NatGateways[].NatGatewayId" 2>/dev/null $TMPVAL); do

      messagePrint "  - 1Delete NAT Gateway:" "$id"
      aws --region $AWS_LOCATION ec2 delete-nat-gateway --nat-gateway-id $id > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to NAT Gateway: $id"
        echo "aws --region $AWS_LOCATION ec2 delete-nat-gateway --nat-gateway-id $id"
        #exit 1
      fi
    done

    sleep 10

    # --- CLEANUP ---
    rm -f $TMPAWS

    # --- DELETE LOAD BALANCERS ---
    for nam in $(aws --region $AWS_LOCATION elb describe-load-balancers | \
         jq -r '.LoadBalancerDescriptions[].LoadBalancerName' 2>/dev/null); do
      tvpc=$(aws --region $AWS_LOCATION elb describe-load-balancers --load-balancer-name $nam | \
          jq -r '.LoadBalancerDescriptions[].VPCId')
      if [ "$tvpc" == "${vpc}" ]; then
        aws --region $AWS_LOCATION elb delete-load-balancer --load-balancer-name $nam
        messagePrint "  - Cleaning up elp LoadBalancers:" "$nam"
        sleep 30
      fi
    done

    # --- DELETE INTERNET GATEWAYS ---
    for id in $(aws --region $AWS_LOCATION ec2 describe-internet-gateways --filter "Name=attachment.vpc-id,Values=$vpc" | \
                jq -r ".InternetGateways[].InternetGatewayId" 2>/dev/null); do

      sleep 10
      messagePrint "  - Detach Internet Gateway:" "$id"
      aws --region $AWS_LOCATION ec2 detach-internet-gateway --internet-gateway-id $id --vpc-id $vpc > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to detach internet gateway: $id"
        echo "aws --region $AWS_LOCATION ec2 detach-internet-gateway --internet-gateway-id $id --vpc-id $vpc"
        exit 1
      fi

      sleep 10
      messagePrint "  - Delete Internet Gateway:" "$id"
      aws --region $AWS_LOCATION ec2 delete-internet-gateway --internet-gateway-id $id > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete internet gateway: $id"
        echo "aws --region $AWS_LOCATION ec2 delete-internet-gateway --internet-gateway-id $id"
        exit 1
      fi
    done

    # --- DELETE SUBNETS ---
    for id in $(aws --region $AWS_LOCATION ec2 describe-subnets --filter "Name=vpc-id,Values=$vpc" | \
                jq -r ".Subnets[].SubnetId" 2>/dev/null); do

      messagePrint "  - Delete Subnet:" "$id"
      aws --region $AWS_LOCATION ec2 delete-subnet --subnet-id $id > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to deleting subnet: $id"
        echo "aws --region $AWS_LOCATION ec2 delete-subnet --subnet-id $id"
        exit 1
      fi
    done

    SECGRP=/tmp/$$_aws_security_groups.json
    aws --region $AWS_LOCATION ec2 describe-security-groups --filter "Name=vpc-id,Values=$vpc" > $SECGRP
    tot=$(grep -c "GroupName" $SECGRP); i=0; let tot=tot-1
    while [ $i -le $tot ]; do
      gnm=$(jq -r ".SecurityGroups[$i].GroupName" 2>/dev/null $SECGRP)
      gid=$(jq -r ".SecurityGroups[$i].GroupId" 2>/dev/null $SECGRP)

      cnt=$(jq -r ".SecurityGroups[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
      if [ $cnt -gt 0 ]; then
        messagePrint "  - Delete Security Group:" "$gnm"
        aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          aws --region $AWS_LOCATION ec2 delete-security-group --group-name $gnm > /dev/null 2>&1
          if [ $? -ne 0 ]; then
            echo "ERROR: failled to delete security group"
            echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid"
            echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-name $gnm"
            exit 1
          fi
        fi
      fi
      let i=i+1
    done

   # --- DELETE ALL SCURITY GROUPS ATTACHED TO THIS VPC ---
    for gid in $(aws --region $AWS_LOCATION ec2 describe-security-groups | \
         jq -r '.SecurityGroups[].GroupId' 2>/dev/null); do
      sgnm=$(aws --region $AWS_LOCATION ec2 describe-security-groups --group-ids  $gid | \
          jq -r '.SecurityGroups[].GroupName')
      tvpc=$(aws --region $AWS_LOCATION ec2 describe-security-groups --group-ids  $gid | \
          jq -r '.SecurityGroups[].VpcId')
      if [ "$tvpc" == "${vpc}" -a "${sgnm}" != "default" ]; then
        messagePrint "  - Cleaning up SecurityGroup:" "$gid"
        aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to delete security group"
          echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid"
          exit 1
        fi
      fi

      sleep 30
    done

    # --- CLEANUP ---
    rm -f $SECGRP

    # --- DELETE ROUTING TABLES ---
    for id in $(aws --region $AWS_LOCATION ec2 describe-route-tables --filter "Name=vpc-id,Values=$vpc" | \
                jq -r ".RouteTables[].RouteTableId" 2>/dev/null); do

      messagePrint "  - Delete Routing Table:" "$id"
      aws --region $AWS_LOCATION ec2 delete-route-table --route-table-id $id> /dev/null 2>&1
      #if [ $? -ne 0 ]; then
      #  echo "ERROR: failled to delete routing table: $id"
      #  echo "aws --region $AWS_LOCATION ec2 delete-route-table --route-table-id $id"
      #  #exit 1
      #fi
    done

    # --- DELETE VPC ---
    messagePrint "  - Delete VPC:" "$vpc"
    aws --region $AWS_LOCATION ec2 delete-vpc --vpc-id $vpc > /dev/null 2>&1
    if [ $? -ne 0 ]; then
      echo "ERROR: failled to delete VPC: $vpc"
      echo "aws --region $AWS_LOCATION ec2 delete-vpc --vpc-id $vpc"
      exit 1
    fi

    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $TMPVCP

  # --- DELETE ADDRESSES ---
  SECGRP=/tmp/$$_aws_routing_table.json
  aws --region $AWS_LOCATION ec2 describe-addresses > $SECGRP
  tot=$(grep -c "PublicIp" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    pid=$(jq -r ".Addresses[$i].PublicIp" 2>/dev/null $SECGRP)
    aid=$(jq -r ".Addresses[$i].AllocationId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".Addresses[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Release PublicIP:" "$pid"
      aws --region $AWS_LOCATION ec2 release-address --allocation-id $aid > /dev/null 2>&1; re1=$?
      aws --region $AWS_LOCATION ec2 release-address --public-ip $pid > /dev/null 2>&1; re2=$?
      if [ $re1 -ne 0 -a $re2 -ne 0 ]; then
#=> Delete NatGateway first (awspks-nat) 1660 llllllll
        echo "ERROR: 1failed to release IP: $pid"
        echo "aws --region $AWS_LOCATION ec2 release-address --allocation-id $aid"
        echo "aws --region $AWS_LOCATION ec2 release-address --public-ip $pid"
        exit 1
      fi

      sleep 20
    fi

    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $TMPAWS

  SECGRP=/tmp/$$_aws_routing_table.json
  aws --region $AWS_LOCATION ec2 describe-route-tables > $SECGRP
  tot=$(grep -c "RouteTableId" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    gid=$(jq -r ".RouteTables[$i].RouteTableId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".RouteTables[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Delete Routing Table:" "$gid"
      aws --region $AWS_LOCATION ec2 delete-route-table --group-id $gid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete Routing Table: $gid"
        echo "aws --region $AWS_LOCATION ec2 delete-route-table --route-table-id $gid"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  SECGRP=/tmp/$$_aws_nat_gateways.json
  aws --region $AWS_LOCATION ec2 describe-nat-gateways > $SECGRP
  tot=$(grep -c "GroupName" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    gid=$(jq -r ".NatGateways[$i].NatGatewayId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".NatGateways[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " - 2Delete NAT Gateway:" "$gid"
      aws --region $AWS_LOCATION ec2 delete-nat-gateway --group-id $gid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        aws --region $AWS_LOCATION ec2 delete-nat-gateway --nat-gateway-id $gid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to delete NAT Gateway: $gid"
          echo "aws --region $AWS_LOCATION ec2 delete-nat-gateway --group-id $gid"
          exit 1
        fi
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  SECGRP=/tmp/$$_aws_security_groups.json
  aws --region $AWS_LOCATION ec2 describe-security-groups > $SECGRP
  tot=$(grep -c "GroupName" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    gnm=$(jq -r ".SecurityGroups[$i].GroupName" 2>/dev/null $SECGRP)
    gid=$(jq -r ".SecurityGroups[$i].GroupId" 2>/dev/null $SECGRP)
#gugu

    cnt=$(jq -r ".SecurityGroups[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Delete Sevcurity Group:" "$gnm"
      aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        aws --region $AWS_LOCATION ec2 delete-security-group --group-name $gnm > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to delete security group"
          echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-id $gid"
          echo "aws --region $AWS_LOCATION ec2 delete-security-group --group-name $gnm"
          exit 1
        fi
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  SECGRP=/tmp/$$_aws_subnets.json
  aws --region $AWS_LOCATION ec2 describe-subnets > $SECGRP
  tot=$(grep -c "SubnetId" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    sid=$(jq -r ".Subnets[$i].SubnetId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".Subnets[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Delete Subnet:" "$sid"
      aws --region $AWS_LOCATION ec2 delete-subnet --subnet-id $sid > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete security group"
        echo "aws --region $AWS_LOCATION ec2 delete-subnet --subnet-id $sid"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  # --- DELETE ADDRESSES ---
  SECGRP=/tmp/$$_aws_routing_table.json
  aws --region $AWS_LOCATION ec2 describe-addresses > $SECGRP
  tot=$(grep -c "PublicIp" $SECGRP); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    pid=$(jq -r ".Addresses[$i].PublicIp" 2>/dev/null $SECGRP)
    aid=$(jq -r ".Addresses[$i].AllocationId" 2>/dev/null $SECGRP)

    cnt=$(jq -r ".Addresses[$i].Tags[].Value" 2>/dev/null $SECGRP | egrep -c "^${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Release PublicIP:" "$pid"
      aws --region $AWS_LOCATION ec2 release-address --allocation-id $aid > /dev/null 2>&1; re1=$?
      aws --region $AWS_LOCATION ec2 release-address --public-ip $pid > /dev/null 2>&1; re2=$?
      if [ $re1 -ne 0 -a $re2 -ne 0 ]; then
#=> Delete NatGateway first (awspks-nat)
        echo "ERROR: 2failed to release IP: $pid"
        echo "aws --region $AWS_LOCATION ec2 release-address --allocation-id $aid"
        echo "aws --region $AWS_LOCATION ec2 release-address --public-ip $pid"
        exit 1
      fi

      sleep 20
    fi

    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $TMPAWS

  # --- IAM DELETE POLICIES ---
  TMPAWS=/tmp/$$_aws.json
  aws --region $AWS_LOCATION iam list-policies > $TMPAWS
  tot=$(grep -c "PolicyName" $TMPAWS); i=0; let tot=tot-1
  while [ $i -le $tot ]; do
    arn=$(jq -r ".Policies[$i].Arn" 2>/dev/null $TMPAWS)
    nam=$(jq -r ".Policies[$i].PolicyName" 2>/dev/null $TMPAWS)

    cnt=$(echo $nam | egrep -c "^${ENV_NAME}_")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Delete IAM Policy:" "$nam"
      rol=$(aws iam list-entities-for-policy --policy-arn $arn | jq -r '.PolicyRoles[].RoleName')
      if [ "$rol" != "" ]; then
        aws --region $AWS_LOCATION iam detach-role-policy --role-name $rol --policy-arn $arn > /dev/null 2>&1
      fi

      aws --region $AWS_LOCATION iam delete-policy --policy-arn $arn > /dev/null 2>&1
      if [ $? -ne 0 ]; then
        echo "ERROR: failled to delete IAM Policy: $nam"
        echo "aws --region $AWS_LOCATION iam delete-policy --policy-arn $arn"
        exit 1
      fi
    fi
    let i=i+1
  done

  # --- CLEANUP ---
  rm -f $SECGRP

  for alias in $(aws --region $AWS_LOCATION kms list-aliases | jq -r '.Aliases[].AliasName'); do
    cnt=$(echo $alias | egrep -c "alias/${ENV_NAME}")
    if [ $cnt -gt 0 ]; then
      messagePrint " - Delete KMS Alias:" "$alias"
      aws --region $AWS_LOCATION kms delete-alias --alias-name $alias
    fi
  done

  # --- DELETE S3 BUCKETS ---
  for bucket in $(aws s3api --region=$AWS_LOCATION list-buckets | jq -r '.Buckets[].Name' | \
                egrep "^${ENV_NAME}-"); do
    messagePrint " - Delete S3 Bucket:" "$bucket"
    aws s3api --region=$AWS_LOCATION delete-bucket --bucket $bucket > /dev/null 2>&1
    #if [ $? -ne 0 ]; then
    #  echo "ERROR: failled to delete S3 Bucket: $nam"
    #  echo "aws s3api --region $AWS_LOCATION delete-bucket --bucket $bucket"
    #  exit 1
    #fi
  done

  # --- DELETE EBS VOLUMES
  TMPAWS=/tmp/$$_vol_aws.json
  aws --region $AWS_LOCATION ec2 describe-volumes > $TMPAWS
  i=0; tot=$(jq -r ".Volumes[$i].VolumeId" 2>/dev/null $TMPAWS | egrep -c "^vol-")

  while [ $i -lt $tot ]; do
    stt=$(jq -r ".Volumes[$i].State" 2>/dev/null $TMPAWS)
    vid=$(jq -r ".Volumes[$i].VolumeId" 2>/dev/null $TMPAWS)
    cnt=$(jq -r ".Volumes[$i].Tags[].Value" $TMPAWS 2>/dev/null| egrep -c "p-bosh|bosh-init")

    if [ $cnt -gt 0 ]; then
      if [ "$stt" == "available" ]; then
        messagePrint " - Delete Volume:" "$vid"
        aws --region $AWS_LOCATION ec2 delete-volume --volume-id $vid > /dev/null 2>&1
        if [ $? -ne 0 ]; then
          echo "ERROR: failled to delete volume: $vid"
          echo "aws --region $AWS_LOCATION ec2 delete-volume --volume-id $vid"
          exit 1
        fi
      else
        messagePrint " - Detaching Volume:" "$vid"
        #aws ec2 --region $AWS_LOCATION detach-volume --volume-id $vid > /dev/null 2>&1
        #aws ec2 --region $AWS_LOCATION ec2 delete-volume --volume-id $vid > /dev/null 2>&1
      fi
    fi

    let i=i+1
  done
  rm -f $SECGRP
}

